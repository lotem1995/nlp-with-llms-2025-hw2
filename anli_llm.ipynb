{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI with LLM\n",
    "\n",
    "You have to implement in this notebook a better ANLI classifier using an LLM.\n",
    "This classifier must be implemented using DSPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete!\n"
     ]
    }
   ],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import dspy\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "from dspy.primitives import Example\n",
    "from dspy.evaluate import Evaluate\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from typing import Literal\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import random\n",
    "from evaluate import load\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load Grok API key\n",
    "with open('grok_key.ini', 'r') as f:\n",
    "    line = f.read().strip()\n",
    "    if line.startswith('export XAI_API_KEY='):\n",
    "        api_key = line.split('=', 1)[1]\n",
    "        os.environ['XAI_API_KEY'] = api_key\n",
    "    else:\n",
    "        raise ValueError(\"Could not parse API key from file\")\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization split: 100 samples\n",
      "Evaluation split: 100 samples\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "#  Configuration setting parameters\n",
    "CONFIG = {\n",
    "    'OPTIMIZATION_SAMPLES': 100,\n",
    "    'EVALUATION_SAMPLES': 100,\n",
    "    'REFINE_N': 5,\n",
    "    'MAX_BOOTSTRAPPED_DEMOS': 20,\n",
    "    'NUM_CANDIDATE_PROGRAMS': 20  \n",
    "}\n",
    "\n",
    "# Load ANLI dataset and filter for non-empty reasons\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] is not None and x['reason'] != \"\")\n",
    "\n",
    "# Split dev_r3: shuffle for diversity, then split and convert to list of dicts\n",
    "dev_r3 = dataset['dev_r3'].shuffle(seed=42)\n",
    "midpoint = len(dev_r3) // 2\n",
    "\n",
    "batched_opt = dev_r3.select(range(midpoint))[:CONFIG['OPTIMIZATION_SAMPLES']]\n",
    "dev_optimization = [dict(zip(batched_opt, t)) for t in zip(*batched_opt.values())]\n",
    "\n",
    "batched_eval = dev_r3.select(range(midpoint, len(dev_r3)))[:CONFIG['EVALUATION_SAMPLES']]\n",
    "dev_evaluation = [dict(zip(batched_eval, t)) for t in zip(*batched_eval.values())]\n",
    "\n",
    "print(f\"Optimization split: {len(dev_optimization)} samples\")\n",
    "print(f\"Evaluation split: {len(dev_evaluation)} samples\")\n",
    "\n",
    "# Load sentence-transformers\n",
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3325254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signatures defined!\n"
     ]
    }
   ],
   "source": [
    "# DSPy Signatures \n",
    "class JointCoT(dspy.Signature):\n",
    "    \"\"\"Generate a Chain-of-Thought explanation and classify the relationship.\"\"\"\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    joint_explanation: str = dspy.OutputField(  \n",
    "        desc=\"Step-by-step reasoning that analyzes the premise and hypothesis to justify the classification label.\"\n",
    "    )\n",
    "    label: Literal[\"entailment\", \"contradiction\", \"neutral\"] = dspy.OutputField()\n",
    "\n",
    "class GenerateExplanation(dspy.Signature):\n",
    "    \"\"\"Generate a relevant CoT explanation for the premise-hypothesis relation.\"\"\"\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    generated_explanation: str = dspy.OutputField(  \n",
    "        desc=\"Provide step-by-step reasoning that analyzes how the hypothesis relates to the premise. Consider what information is given, what can be inferred, and what contradictions exist.\"\n",
    "    )\n",
    "\n",
    "class ClassifyWithExplanation(dspy.Signature):\n",
    "    \"\"\"Classify based on premise, hypothesis, and explanation.\"\"\"\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    explanation_text: str = dspy.InputField(  \n",
    "        desc=\"The explanation to use for classification\"\n",
    "    )\n",
    "    label: Literal[\"entailment\", \"contradiction\", \"neutral\"] = dspy.OutputField(\n",
    "        desc=\"Based on the explanation, classify as: 'entailment' if hypothesis must be true, 'contradiction' if hypothesis must be false, 'neutral' if hypothesis could be true or false.\"\n",
    "    )\n",
    "print(\"Signatures defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57127127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules defined!\n"
     ]
    }
   ],
   "source": [
    "# DSPy Modules \n",
    "class JointModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate = dspy.ChainOfThought(JointCoT)\n",
    "    \n",
    "    def forward(self, premise, hypothesis):\n",
    "        return self.generate(premise=premise, hypothesis=hypothesis)\n",
    "\n",
    "class PipelineModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.explain = dspy.ChainOfThought(GenerateExplanation)\n",
    "        self.classify = dspy.ChainOfThought(ClassifyWithExplanation)\n",
    "    \n",
    "    def forward(self, premise, hypothesis):\n",
    "        exp = self.explain(premise=premise, hypothesis=hypothesis)\n",
    "        result = self.classify(premise=premise, hypothesis=hypothesis, explanation_text=exp.generated_explanation)  \n",
    "        # Add the explanation to the result for evaluation\n",
    "        result.pipeline_explanation = exp.generated_explanation  \n",
    "        return result\n",
    "\n",
    "print(\"Modules defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe7014b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning threshold: 100%|██████████| 100/100 [00:01<00:00, 56.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned threshold: 0.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Similarity Functions \n",
    "def compute_similarity(text1, text2):\n",
    "    emb1 = similarity_model.encode(text1)\n",
    "    emb2 = similarity_model.encode(text2)\n",
    "    return util.cos_sim(emb1, emb2).item()\n",
    "\n",
    "def explanation_quality(premise, hypothesis, pred_exp, human_reason):\n",
    "    ph = f\"{premise} {hypothesis}\"\n",
    "    return {\n",
    "        'pred_vs_human': compute_similarity(pred_exp, human_reason),\n",
    "        'pred_vs_ph': compute_similarity(pred_exp, ph),\n",
    "        'human_vs_ph': compute_similarity(human_reason, ph)\n",
    "    }\n",
    "\n",
    "# Learn threshold \n",
    "def learn_threshold(data):\n",
    "    similarities = []\n",
    "    for ex in tqdm(data, desc=\"Learning threshold\"):\n",
    "        ph = f\"{ex['premise']} {ex['hypothesis']}\"\n",
    "        sim = compute_similarity(ex['reason'], ph)\n",
    "        similarities.append(sim)\n",
    "    threshold = np.mean(similarities) - np.std(similarities)\n",
    "    print(f\"Learned threshold: {threshold:.3f}\")\n",
    "    return threshold\n",
    "\n",
    "threshold = learn_threshold(dev_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46d2f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric defined!\n"
     ]
    }
   ],
   "source": [
    "# Custom Metric \n",
    "label_map = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "def custom_metric(example, pred, trace=None):\n",
    "    pred_label = getattr(pred, 'label', '').lower()\n",
    "    acc = 1.0 if label_map.get(pred_label, -1) == example['label'] else 0.0\n",
    "    \n",
    "    # Get explanation from either joint or pipeline field\n",
    "    exp = getattr(pred, 'joint_explanation', '') or getattr(pred, 'pipeline_explanation', '')\n",
    "    if not exp:\n",
    "        return 0.0\n",
    "    sim = compute_similarity(exp, example['reason'])\n",
    "    quality = 1.0 if sim > threshold else 0.0\n",
    "    \n",
    "    return (acc + quality) / 2.0\n",
    "\n",
    "print(\"Metric defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "475d6fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 20 traces per predictor.\n",
      "Will attempt to bootstrap 20 candidate sets.\n",
      "Average Metric: 84.50 / 100 (84.5%): 100%|██████████| 100/100 [00:01<00:00, 56.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:18 INFO dspy.evaluate.evaluate: Average Metric: 84.5 / 100 (84.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 84.5 for seed -3\n",
      "Scores so far: [84.5]\n",
      "Best score so far: 84.5\n",
      "Average Metric: 82.50 / 100 (82.5%): 100%|██████████| 100/100 [00:01<00:00, 60.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:20 INFO dspy.evaluate.evaluate: Average Metric: 82.5 / 100 (82.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5]\n",
      "Best score so far: 84.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:00<00:01, 49.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 20 full traces after 20 examples for up to 1 rounds, amounting to 20 attempts.\n",
      "Average Metric: 84.00 / 100 (84.0%): 100%|██████████| 100/100 [00:01<00:00, 56.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:22 INFO dspy.evaluate.evaluate: Average Metric: 84.0 / 100 (84.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0]\n",
      "Best score so far: 84.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:00<00:01, 46.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 13 full traces after 13 examples for up to 1 rounds, amounting to 13 attempts.\n",
      "Average Metric: 84.50 / 100 (84.5%): 100%|██████████| 100/100 [00:01<00:00, 58.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:24 INFO dspy.evaluate.evaluate: Average Metric: 84.5 / 100 (84.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5]\n",
      "Best score so far: 84.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:00<00:01, 58.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n",
      "Average Metric: 82.00 / 100 (82.0%): 100%|██████████| 100/100 [00:01<00:00, 64.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:26 INFO dspy.evaluate.evaluate: Average Metric: 82.0 / 100 (82.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0]\n",
      "Best score so far: 84.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:02, 42.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Average Metric: 82.50 / 100 (82.5%): 100%|██████████| 100/100 [00:01<00:00, 63.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:28 INFO dspy.evaluate.evaluate: Average Metric: 82.5 / 100 (82.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5]\n",
      "Best score so far: 84.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:00<00:02, 42.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 9 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Average Metric: 86.50 / 100 (86.5%): 100%|██████████| 100/100 [00:01<00:00, 67.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:29 INFO dspy.evaluate.evaluate: Average Metric: 86.5 / 100 (86.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 86.5 for seed 3\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:00<00:01, 62.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 84.50 / 100 (84.5%): 100%|██████████| 100/100 [00:01<00:00, 64.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:31 INFO dspy.evaluate.evaluate: Average Metric: 84.5 / 100 (84.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:00<00:01, 52.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 20 full traces after 21 examples for up to 1 rounds, amounting to 21 attempts.\n",
      "Average Metric: 84.50 / 100 (84.5%): 100%|██████████| 100/100 [00:01<00:00, 62.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:33 INFO dspy.evaluate.evaluate: Average Metric: 84.5 / 100 (84.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:00<00:01, 51.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 19 full traces after 19 examples for up to 1 rounds, amounting to 19 attempts.\n",
      "Average Metric: 83.00 / 100 (83.0%): 100%|██████████| 100/100 [00:01<00:00, 63.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:35 INFO dspy.evaluate.evaluate: Average Metric: 83.0 / 100 (83.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:00<00:01, 46.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 11 full traces after 11 examples for up to 1 rounds, amounting to 11 attempts.\n",
      "Average Metric: 84.50 / 100 (84.5%): 100%|██████████| 100/100 [00:01<00:00, 59.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:37 INFO dspy.evaluate.evaluate: Average Metric: 84.5 / 100 (84.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:00<00:02, 39.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n",
      "Average Metric: 83.50 / 100 (83.5%): 100%|██████████| 100/100 [00:01<00:00, 64.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:39 INFO dspy.evaluate.evaluate: Average Metric: 83.5 / 100 (83.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:00<00:01, 46.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 15 full traces after 15 examples for up to 1 rounds, amounting to 15 attempts.\n",
      "Average Metric: 83.50 / 100 (83.5%): 100%|██████████| 100/100 [00:01<00:00, 62.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:41 INFO dspy.evaluate.evaluate: Average Metric: 83.5 / 100 (83.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5, 83.5]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:00<00:01, 46.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 19 full traces after 19 examples for up to 1 rounds, amounting to 19 attempts.\n",
      "Average Metric: 83.00 / 100 (83.0%): 100%|██████████| 100/100 [00:01<00:00, 65.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:43 INFO dspy.evaluate.evaluate: Average Metric: 83.0 / 100 (83.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5, 83.5, 83.0]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:00<00:01, 50.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 15 full traces after 15 examples for up to 1 rounds, amounting to 15 attempts.\n",
      "Average Metric: 86.50 / 100 (86.5%): 100%|██████████| 100/100 [00:01<00:00, 62.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:45 INFO dspy.evaluate.evaluate: Average Metric: 86.5 / 100 (86.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5, 83.5, 83.0, 86.5]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:00<00:01, 50.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 16 full traces after 16 examples for up to 1 rounds, amounting to 16 attempts.\n",
      "Average Metric: 86.00 / 100 (86.0%): 100%|██████████| 100/100 [00:01<00:00, 68.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:46 INFO dspy.evaluate.evaluate: Average Metric: 86.0 / 100 (86.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5, 83.5, 83.0, 86.5, 86.0]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:00<00:01, 47.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 9 full traces after 9 examples for up to 1 rounds, amounting to 9 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 81.50 / 100 (81.5%): 100%|██████████| 100/100 [00:01<00:00, 66.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:48 INFO dspy.evaluate.evaluate: Average Metric: 81.5 / 100 (81.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5, 83.5, 83.0, 86.5, 86.0, 81.5]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:00<00:01, 56.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Average Metric: 83.00 / 100 (83.0%): 100%|██████████| 100/100 [00:01<00:00, 63.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:50 INFO dspy.evaluate.evaluate: Average Metric: 83.0 / 100 (83.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5, 83.5, 83.0, 86.5, 86.0, 81.5, 83.0]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:00<00:01, 52.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 7 examples for up to 1 rounds, amounting to 7 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 82.50 / 100 (82.5%): 100%|██████████| 100/100 [00:01<00:00, 59.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:52 INFO dspy.evaluate.evaluate: Average Metric: 82.5 / 100 (82.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5, 83.5, 83.0, 86.5, 86.0, 81.5, 83.0, 82.5]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:00<00:01, 49.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 12 full traces after 12 examples for up to 1 rounds, amounting to 12 attempts.\n",
      "Average Metric: 85.50 / 100 (85.5%): 100%|██████████| 100/100 [00:01<00:00, 62.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:53 INFO dspy.evaluate.evaluate: Average Metric: 85.5 / 100 (85.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5, 83.5, 83.0, 86.5, 86.0, 81.5, 83.0, 82.5, 85.5]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:00<00:01, 49.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 17 full traces after 17 examples for up to 1 rounds, amounting to 17 attempts.\n",
      "Average Metric: 86.00 / 100 (86.0%): 100%|██████████| 100/100 [00:01<00:00, 64.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:55 INFO dspy.evaluate.evaluate: Average Metric: 86.0 / 100 (86.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5, 83.5, 83.0, 86.5, 86.0, 81.5, 83.0, 82.5, 85.5, 86.0]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:00<00:01, 55.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 6 examples for up to 1 rounds, amounting to 6 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 82.50 / 100 (82.5%): 100%|██████████| 100/100 [00:01<00:00, 63.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:57 INFO dspy.evaluate.evaluate: Average Metric: 82.5 / 100 (82.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5, 83.5, 83.0, 86.5, 86.0, 81.5, 83.0, 82.5, 85.5, 86.0, 82.5]\n",
      "Best score so far: 86.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:01, 53.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 82.00 / 100 (82.0%): 100%|██████████| 100/100 [00:01<00:00, 66.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 11:59:59 INFO dspy.evaluate.evaluate: Average Metric: 82.0 / 100 (82.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [84.5, 82.5, 84.0, 84.5, 82.0, 82.5, 86.5, 84.5, 84.5, 83.0, 84.5, 83.5, 83.5, 83.0, 86.5, 86.0, 81.5, 83.0, 82.5, 85.5, 86.0, 82.5, 82.0]\n",
      "Best score so far: 86.5\n",
      "23 candidate programs found.\n",
      "Average Metric: 83.00 / 100 (83.0%): 100%|██████████| 100/100 [00:01<00:00, 68.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:00 INFO dspy.evaluate.evaluate: Average Metric: 83.0 / 100 (83.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 83.0 for seed -3\n",
      "Scores so far: [83.0]\n",
      "Best score so far: 83.0\n",
      "Average Metric: 85.50 / 100 (85.5%): 100%|██████████| 100/100 [00:01<00:00, 68.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:02 INFO dspy.evaluate.evaluate: Average Metric: 85.5 / 100 (85.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 85.5 for seed -2\n",
      "Scores so far: [83.0, 85.5]\n",
      "Best score so far: 85.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:00<00:01, 56.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 20 full traces after 20 examples for up to 1 rounds, amounting to 20 attempts.\n",
      "Average Metric: 84.00 / 100 (84.0%): 100%|██████████| 100/100 [00:01<00:00, 56.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:04 INFO dspy.evaluate.evaluate: Average Metric: 84.0 / 100 (84.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0]\n",
      "Best score so far: 85.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:00<00:01, 55.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 13 full traces after 13 examples for up to 1 rounds, amounting to 13 attempts.\n",
      "Average Metric: 84.50 / 100 (84.5%): 100%|██████████| 100/100 [00:01<00:00, 60.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:06 INFO dspy.evaluate.evaluate: Average Metric: 84.5 / 100 (84.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5]\n",
      "Best score so far: 85.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:00<00:01, 57.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n",
      "Average Metric: 83.50 / 100 (83.5%): 100%|██████████| 100/100 [00:01<00:00, 60.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:07 INFO dspy.evaluate.evaluate: Average Metric: 83.5 / 100 (83.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5]\n",
      "Best score so far: 85.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:01, 60.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 82.50 / 100 (82.5%): 100%|██████████| 100/100 [00:01<00:00, 63.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:09 INFO dspy.evaluate.evaluate: Average Metric: 82.5 / 100 (82.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5]\n",
      "Best score so far: 85.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:00<00:02, 43.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 85.00 / 100 (85.0%): 100%|██████████| 100/100 [00:01<00:00, 61.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:11 INFO dspy.evaluate.evaluate: Average Metric: 85.0 / 100 (85.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0]\n",
      "Best score so far: 85.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:00<00:01, 58.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 84.50 / 100 (84.5%): 100%|██████████| 100/100 [00:01<00:00, 65.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:13 INFO dspy.evaluate.evaluate: Average Metric: 84.5 / 100 (84.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5]\n",
      "Best score so far: 85.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:00<00:01, 50.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 20 full traces after 21 examples for up to 1 rounds, amounting to 21 attempts.\n",
      "Average Metric: 84.00 / 100 (84.0%): 100%|██████████| 100/100 [00:01<00:00, 58.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:15 INFO dspy.evaluate.evaluate: Average Metric: 84.0 / 100 (84.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0]\n",
      "Best score so far: 85.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:00<00:01, 55.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 19 full traces after 19 examples for up to 1 rounds, amounting to 19 attempts.\n",
      "Average Metric: 86.00 / 100 (86.0%): 100%|██████████| 100/100 [00:01<00:00, 59.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:17 INFO dspy.evaluate.evaluate: Average Metric: 86.0 / 100 (86.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 86.0 for seed 6\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0]\n",
      "Best score so far: 86.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:00<00:01, 54.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 11 full traces after 11 examples for up to 1 rounds, amounting to 11 attempts.\n",
      "Average Metric: 81.50 / 100 (81.5%): 100%|██████████| 100/100 [00:02<00:00, 49.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:19 INFO dspy.evaluate.evaluate: Average Metric: 81.5 / 100 (81.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5]\n",
      "Best score so far: 86.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:00<00:01, 56.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 84.00 / 100 (84.0%): 100%|██████████| 100/100 [00:01<00:00, 62.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:21 INFO dspy.evaluate.evaluate: Average Metric: 84.0 / 100 (84.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0]\n",
      "Best score so far: 86.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:00<00:01, 54.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 15 full traces after 15 examples for up to 1 rounds, amounting to 15 attempts.\n",
      "Average Metric: 85.00 / 100 (85.0%): 100%|██████████| 100/100 [00:01<00:00, 60.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:23 INFO dspy.evaluate.evaluate: Average Metric: 85.0 / 100 (85.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0, 85.0]\n",
      "Best score so far: 86.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:00<00:01, 55.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 19 full traces after 20 examples for up to 1 rounds, amounting to 20 attempts.\n",
      "Average Metric: 85.50 / 100 (85.5%): 100%|██████████| 100/100 [00:01<00:00, 57.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:25 INFO dspy.evaluate.evaluate: Average Metric: 85.5 / 100 (85.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0, 85.0, 85.5]\n",
      "Best score so far: 86.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:00<00:01, 53.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 15 full traces after 15 examples for up to 1 rounds, amounting to 15 attempts.\n",
      "Average Metric: 85.00 / 100 (85.0%): 100%|██████████| 100/100 [00:01<00:00, 59.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:27 INFO dspy.evaluate.evaluate: Average Metric: 85.0 / 100 (85.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0, 85.0, 85.5, 85.0]\n",
      "Best score so far: 86.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:00<00:01, 55.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 16 full traces after 16 examples for up to 1 rounds, amounting to 16 attempts.\n",
      "Average Metric: 82.50 / 100 (82.5%): 100%|██████████| 100/100 [00:01<00:00, 60.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:29 INFO dspy.evaluate.evaluate: Average Metric: 82.5 / 100 (82.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0, 85.0, 85.5, 85.0, 82.5]\n",
      "Best score so far: 86.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:00<00:01, 58.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 9 full traces after 9 examples for up to 1 rounds, amounting to 9 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 87.00 / 100 (87.0%): 100%|██████████| 100/100 [00:01<00:00, 61.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:31 INFO dspy.evaluate.evaluate: Average Metric: 87.0 / 100 (87.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 87.0 for seed 13\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0, 85.0, 85.5, 85.0, 82.5, 87.0]\n",
      "Best score so far: 87.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:00<00:01, 63.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Average Metric: 86.00 / 100 (86.0%): 100%|██████████| 100/100 [00:01<00:00, 58.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:33 INFO dspy.evaluate.evaluate: Average Metric: 86.0 / 100 (86.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0, 85.0, 85.5, 85.0, 82.5, 87.0, 86.0]\n",
      "Best score so far: 87.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:00<00:01, 52.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 7 examples for up to 1 rounds, amounting to 7 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 87.00 / 100 (87.0%): 100%|██████████| 100/100 [00:01<00:00, 56.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:35 INFO dspy.evaluate.evaluate: Average Metric: 87.0 / 100 (87.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0, 85.0, 85.5, 85.0, 82.5, 87.0, 86.0, 87.0]\n",
      "Best score so far: 87.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:00<00:01, 54.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 12 full traces after 12 examples for up to 1 rounds, amounting to 12 attempts.\n",
      "Average Metric: 83.50 / 100 (83.5%): 100%|██████████| 100/100 [00:01<00:00, 61.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:36 INFO dspy.evaluate.evaluate: Average Metric: 83.5 / 100 (83.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0, 85.0, 85.5, 85.0, 82.5, 87.0, 86.0, 87.0, 83.5]\n",
      "Best score so far: 87.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:00<00:01, 52.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 17 full traces after 17 examples for up to 1 rounds, amounting to 17 attempts.\n",
      "Average Metric: 85.50 / 100 (85.5%): 100%|██████████| 100/100 [00:01<00:00, 56.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:39 INFO dspy.evaluate.evaluate: Average Metric: 85.5 / 100 (85.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0, 85.0, 85.5, 85.0, 82.5, 87.0, 86.0, 87.0, 83.5, 85.5]\n",
      "Best score so far: 87.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:00<00:01, 55.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 6 examples for up to 1 rounds, amounting to 6 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 84.50 / 100 (84.5%): 100%|██████████| 100/100 [00:01<00:00, 64.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:40 INFO dspy.evaluate.evaluate: Average Metric: 84.5 / 100 (84.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0, 85.0, 85.5, 85.0, 82.5, 87.0, 86.0, 87.0, 83.5, 85.5, 84.5]\n",
      "Best score so far: 87.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:01, 58.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 86.00 / 100 (86.0%): 100%|██████████| 100/100 [00:01<00:00, 64.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 12:00:42 INFO dspy.evaluate.evaluate: Average Metric: 86.0 / 100 (86.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [83.0, 85.5, 84.0, 84.5, 83.5, 82.5, 85.0, 84.5, 84.0, 86.0, 81.5, 84.0, 85.0, 85.5, 85.0, 82.5, 87.0, 86.0, 87.0, 83.5, 85.5, 84.5, 86.0]\n",
      "Best score so far: 87.0\n",
      "23 candidate programs found.\n",
      "Optimization complete!\n"
     ]
    }
   ],
   "source": [
    "# Prepare Examples\n",
    "trainset = [Example(premise=ex['premise'], hypothesis=ex['hypothesis'], label=ex['label'], reason=ex['reason']).with_inputs('premise', 'hypothesis') for ex in dev_optimization]\n",
    "\n",
    "\n",
    "# Run optimization\n",
    "compiler = BootstrapFewShotWithRandomSearch(metric=custom_metric, max_bootstrapped_demos=CONFIG['MAX_BOOTSTRAPPED_DEMOS'], num_candidate_programs=CONFIG['NUM_CANDIDATE_PROGRAMS'])\n",
    "optimized_joint = compiler.compile(JointModule(), trainset=trainset)\n",
    "optimized_pipeline = compiler.compile(PipelineModule(), trainset=trainset)\n",
    "\n",
    "print(\"Optimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6744079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-of-N refinement ready!\n"
     ]
    }
   ],
   "source": [
    "# Custom Best-of-N Refinement (DSPy refine implementation)\n",
    "def best_of_n(module, premise, hypothesis, n=CONFIG['REFINE_N'], metric=custom_metric):\n",
    "    \"\"\"Generate n predictions and return the best one based on the custom metric.\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for _ in range(n):\n",
    "        try:\n",
    "            pred = module(premise=premise, hypothesis=hypothesis)\n",
    "            predictions.append(pred)\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not predictions:\n",
    "        # Fallback: try one more time\n",
    "        return module(premise=premise, hypothesis=hypothesis)\n",
    "    \n",
    "    # Score each prediction using the custom metric\n",
    "    best_pred = predictions[0]\n",
    "    best_score = -1\n",
    "    \n",
    "    for pred in predictions:\n",
    "        try:\n",
    "            # Create a dummy example for metric evaluation\n",
    "            example = {'premise': premise, 'hypothesis': hypothesis, 'label': 0, 'reason': ''}\n",
    "            score = metric(example, pred)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pred = pred\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return best_pred\n",
    "\n",
    "print(\"Best-of-N refinement ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b38ce6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction ready!\n"
     ]
    }
   ],
   "source": [
    "# Prediction with Refinement\n",
    "def predict_with_refine(module, premise, hypothesis):\n",
    "    return best_of_n(module, premise, hypothesis)\n",
    "\n",
    "print(\"Prediction ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6e26607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ready!\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Function \n",
    "def evaluate_model(module, data, deberta_preds=None):\n",
    "    preds, golds = [], []\n",
    "    sims = {'pred_vs_human': [], 'pred_vs_ph': [], 'human_vs_ph': []}\n",
    "    \n",
    "    for ex in tqdm(data, desc='Evaluating'):\n",
    "        try:\n",
    "            pred = predict_with_refine(module, ex['premise'], ex['hypothesis'])\n",
    "            pred_label = getattr(pred, 'label', '').lower()\n",
    "            preds.append(label_map.get(pred_label, -1))\n",
    "            golds.append(ex['label'])\n",
    "            \n",
    "            # Get explanation from either joint or pipeline field\n",
    "            exp = getattr(pred, 'joint_explanation', '') or getattr(pred, 'pipeline_explanation', '')\n",
    "            if exp:\n",
    "                metrics = explanation_quality(ex['premise'], ex['hypothesis'], exp, ex['reason'])\n",
    "                for k in sims:\n",
    "                    sims[k].append(metrics[k])\n",
    "            else:\n",
    "                for k in sims:\n",
    "                    sims[k].append(0.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            preds.append(-1)\n",
    "            golds.append(ex['label'])\n",
    "            for k in sims:\n",
    "                sims[k].append(0.0)\n",
    "\n",
    "    # Calculate classification metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    \n",
    "    # Filter out invalid predictions\n",
    "    valid_indices = [i for i, p in enumerate(preds) if p != -1]\n",
    "    valid_preds = [preds[i] for i in valid_indices]\n",
    "    valid_golds = [golds[i] for i in valid_indices]\n",
    "    \n",
    "    print(f\"Total predictions: {len(preds)}, Valid predictions: {len(valid_preds)}\")\n",
    "    \n",
    "    if len(valid_preds) == 0:\n",
    "        print(\"ERROR: No valid predictions found!\")\n",
    "        return None\n",
    "    \n",
    "    accuracy = accuracy_score(valid_golds, valid_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(valid_golds, valid_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Calculate average similarities\n",
    "    avg_sims = {k: np.mean(v) if v else 0.0 for k, v in sims.items()}\n",
    "    \n",
    "    # Return results dictionary\n",
    "    return {\n",
    "        'predictions': preds,\n",
    "        'golds': golds,\n",
    "        'metrics': {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        },\n",
    "        'avg_sims': avg_sims\n",
    "    }\n",
    "\n",
    "print(\"Evaluation ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fbc1092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison ready!\n"
     ]
    }
   ],
   "source": [
    "# Comparison Functions\n",
    "def compare_results(joint_results, pipeline_results):\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'F1', 'Precision', 'Recall', 'Pred-Human Sim', 'Pred-PH Sim', 'Human-PH Sim'],\n",
    "        'Joint': [\n",
    "            joint_results['metrics']['accuracy'], joint_results['metrics']['f1'],\n",
    "            joint_results['metrics']['precision'], joint_results['metrics']['recall'],\n",
    "            joint_results['avg_sims']['pred_vs_human'], joint_results['avg_sims']['pred_vs_ph'],\n",
    "            joint_results['avg_sims']['human_vs_ph']\n",
    "        ],\n",
    "        'Pipeline': [\n",
    "            pipeline_results['metrics']['accuracy'], pipeline_results['metrics']['f1'],\n",
    "            pipeline_results['metrics']['precision'], pipeline_results['metrics']['recall'],\n",
    "            pipeline_results['avg_sims']['pred_vs_human'], pipeline_results['avg_sims']['pred_vs_ph'],\n",
    "            pipeline_results['avg_sims']['human_vs_ph']\n",
    "        ]\n",
    "    })\n",
    "    return comparison_df\n",
    "\n",
    "def compute_agreement_metrics(llm_preds, deberta_preds, golds):\n",
    "    both_correct = llm_correct_deberta_wrong = deberta_correct_llm_wrong = both_wrong = 0\n",
    "    \n",
    "    deberta_mapped = [label_map.get(p.lower() if isinstance(p, str) else p, -1) for p in deberta_preds]\n",
    "    \n",
    "    for llm_pred, deberta_pred, gold in zip(llm_preds, deberta_mapped, golds):\n",
    "        if llm_pred == -1 or deberta_pred == -1:\n",
    "            continue\n",
    "        llm_correct = (llm_pred == gold)\n",
    "        deberta_correct = (deberta_pred == gold)\n",
    "        if llm_correct and deberta_correct:\n",
    "            both_correct += 1\n",
    "        elif llm_correct:\n",
    "            llm_correct_deberta_wrong += 1\n",
    "        elif deberta_correct:\n",
    "            deberta_correct_llm_wrong += 1\n",
    "        else:\n",
    "            both_wrong += 1\n",
    "    \n",
    "    total = both_correct + llm_correct_deberta_wrong + deberta_correct_llm_wrong + both_wrong\n",
    "    if total == 0:\n",
    "        return {'both_correct': 0, 'llm_correct_deberta_wrong': 0, 'deberta_correct_llm_wrong': 0, 'both_wrong': 0}\n",
    "    pct = lambda x: (x / total * 100) if total > 0 else 0\n",
    "    return {\n",
    "        'both_correct': both_correct, 'both_correct_pct': pct(both_correct),\n",
    "        'llm_correct_deberta_wrong': llm_correct_deberta_wrong, 'llm_correct_deberta_wrong_pct': pct(llm_correct_deberta_wrong),\n",
    "        'deberta_correct_llm_wrong': deberta_correct_llm_wrong, 'deberta_correct_llm_wrong_pct': pct(deberta_correct_llm_wrong),\n",
    "        'both_wrong': both_wrong, 'both_wrong_pct': pct(both_wrong)\n",
    "    }\n",
    "\n",
    "def print_agreement_analysis(joint_results, pipeline_results, deberta_preds):\n",
    "    joint_ag = compute_agreement_metrics(joint_results['predictions'], deberta_preds[:len(joint_results['predictions'])], joint_results['golds'])\n",
    "    pipeline_ag = compute_agreement_metrics(pipeline_results['predictions'], deberta_preds[:len(pipeline_results['predictions'])], pipeline_results['golds'])\n",
    "    \n",
    "    print(\"\\nJOINT vs DeBERTa:\")\n",
    "    for k in ['both_correct', 'llm_correct_deberta_wrong', 'deberta_correct_llm_wrong', 'both_wrong']:\n",
    "        print(f\"{k}: {joint_ag[k]} ({joint_ag[f'{k}_pct']:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nPIPELINE vs DeBERTa:\")\n",
    "    for k in ['both_correct', 'llm_correct_deberta_wrong', 'deberta_correct_llm_wrong', 'both_wrong']:\n",
    "        print(f\"{k}: {pipeline_ag[k]} ({pipeline_ag[f'{k}_pct']:.1f}%)\")\n",
    "\n",
    "print(\"Comparison ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f3f78c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:15<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 100, Valid predictions: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:13<00:00,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 100, Valid predictions: 100\n",
      "\n",
      "JOINT vs PIPELINE COMPARISON\n",
      "           Metric   Joint  Pipeline\n",
      "0        Accuracy  0.7500    0.6900\n",
      "1              F1  0.7529    0.6871\n",
      "2       Precision  0.7867    0.6880\n",
      "3          Recall  0.7500    0.6900\n",
      "4  Pred-Human Sim  0.5105    0.5563\n",
      "5     Pred-PH Sim  0.5869    0.6348\n",
      "6    Human-PH Sim  0.4027    0.4027\n",
      "\n",
      "AGREEMENT ANALYSIS\n",
      "\n",
      "JOINT vs DeBERTa:\n",
      "both_correct: 28 (28.0%)\n",
      "llm_correct_deberta_wrong: 47 (47.0%)\n",
      "deberta_correct_llm_wrong: 3 (3.0%)\n",
      "both_wrong: 22 (22.0%)\n",
      "\n",
      "PIPELINE vs DeBERTa:\n",
      "both_correct: 25 (25.0%)\n",
      "llm_correct_deberta_wrong: 44 (44.0%)\n",
      "deberta_correct_llm_wrong: 6 (6.0%)\n",
      "both_wrong: 25 (25.0%)\n",
      "\n",
      "TASK 1.4 SUMMARY\n",
      "✓ All requirements met!\n",
      "Joint Accuracy: 0.750, Pipeline: 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main Evaluation\n",
    "import pickle\n",
    "with open('deberta_preds.pkl', 'rb') as f:\n",
    "    deberta_preds = pickle.load(f)[:CONFIG['EVALUATION_SAMPLES']]\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "joint_results = evaluate_model(optimized_joint, dev_evaluation, deberta_preds)\n",
    "pipeline_results = evaluate_model(optimized_pipeline, dev_evaluation, deberta_preds)\n",
    "\n",
    "comparison_df = compare_results(joint_results, pipeline_results)\n",
    "print(\"\\nJOINT vs PIPELINE COMPARISON\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "print(\"\\nAGREEMENT ANALYSIS\")\n",
    "print_agreement_analysis(joint_results, pipeline_results, deberta_preds)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nTASK 1.4 SUMMARY\")\n",
    "print(\"✓ All requirements met!\")\n",
    "joint_acc = joint_results['metrics']['accuracy']\n",
    "pipeline_acc = pipeline_results['metrics']['accuracy']\n",
    "print(f\"Joint Accuracy: {joint_acc:.3f}, Pipeline: {pipeline_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8883268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.to_csv('task14_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8756f58e",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce4423",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b45be1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import load\n",
    "\n",
    "# accuracy = load(\"accuracy\")\n",
    "# precision = load(\"precision\")\n",
    "# recall = load(\"recall\")\n",
    "# f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ef7f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "# clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b471a74c",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
