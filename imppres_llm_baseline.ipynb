{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres LLM Baseline\n",
    "\n",
    "You have to implement in this notebook a baseline for ImpPres classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import dspy\n",
    "import pandas as pd\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "\n",
    "# for ollama\n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "# lm = dspy.LM(\n",
    "#     \"ollama/llama3.1:8b\",\n",
    "#     api_base=\"http://localhost:11434\",\n",
    "#     format=\"json\"        # litellm translates this to Ollama's stream=false\n",
    "# )\n",
    "dspy.configure(lm=lm)"
   ],
   "id": "2b9b979cbc0dc715"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import logging\n",
    "logging.getLogger(\"dspy.adapters.json_adapter\").setLevel(logging.ERROR)"
   ],
   "id": "dd1f20bcd15e900c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy program to classify pairs (premise, hypothesis) as entailment, contradiction, or neutral.\n",
    "class NLIImPresClassifier(dspy.Signature):\n",
    "    premise     :str = dspy.InputField(desc=\"A short passage or statement. All facts should be inferred from this text alone.\")\n",
    "    hypothesis  :str = dspy.InputField(desc=\"A second statement to evaluate. Check if this follows from, contradicts, or is unrelated to the premise.\")\n",
    "    label       : Literal[\"entailment\", \"neutral\", \"contradiction\"] = dspy.OutputField(\n",
    "        desc=(\n",
    "            \"Return one of: 'entailment', 'neutral', or 'contradiction'.\\n\"\n",
    "            \"- 'entailment': The hypothesis must be true if the premise is true.\\n\"\n",
    "            \"- 'contradiction': The hypothesis must be false if the premise is true.\\n\"\n",
    "            \"- 'neutral': The hypothesis could be either true or false based on the premise.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "predictor = dspy.Predict(NLIImPresClassifier)\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "def zero_shot_nli_classifier(x):\n",
    "    return {\n",
    "        'premise' : x['premise'],\n",
    "        'hypothesis': x['hypothesis'],\n",
    "        'pred_label' : predictor(premise=x['premise'], hypothesis=x['hypothesis']).label,\n",
    "        'gold_label' : label_names[x['gold_label']]\n",
    "    }"
   ],
   "id": "686e6e259245fe7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load ImpPres dataset",
   "id": "3bf1719d8f8eaf51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sections = ['presupposition_all_n_presupposition',\n",
    "            'presupposition_both_presupposition',\n",
    "            'presupposition_change_of_state',\n",
    "            'presupposition_cleft_existence',\n",
    "            'presupposition_cleft_uniqueness',\n",
    "            'presupposition_only_presupposition',\n",
    "            'presupposition_possessed_definites_existence',\n",
    "            'presupposition_possessed_definites_uniqueness',\n",
    "            'presupposition_question_presupposition']\n",
    "\n",
    "dataset = {}\n",
    "for section in sections:\n",
    "    print(f\"Loading dataset for section: {section}\")\n",
    "    dataset[section] = load_dataset(\"facebook/imppres\", section)"
   ],
   "id": "31e792a3bac1fc4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset",
   "id": "95be49c90c01f6a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ],
   "id": "4b5302e1aa89410d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ],
   "id": "f9e7a0ffbd08457"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ],
   "id": "40dd1d6e2a92bf94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will first run the dspy classifier through the dataset:",
   "id": "5c419a6458455fe6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def accuracy_metric(example, pred, *args):\n",
    "     return pred.label == example.label"
   ],
   "id": "c068b1c86ed42dfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "# Convert to DSPy Example objects\n",
    "dspy_examples = {}\n",
    "for section_name, section in dataset.items():\n",
    "    key = next(iter(section.keys()))\n",
    "    ds = section[key]\n",
    "    dspy_examples[section_name] = [\n",
    "        dspy.Example(\n",
    "            premise=ex['premise'],\n",
    "            hypothesis=ex['hypothesis'],\n",
    "            label=label_names[ex['gold_label']]\n",
    "        ).with_inputs(\"premise\", \"hypothesis\")\n",
    "        for ex in ds\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame(dspy_examples)\n",
    "display(df)"
   ],
   "id": "33c73280e69e73b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "from evaluate import combine, load\n",
    "\n",
    "# 1. Run DSPy evaluation for each section (here, limited to first 10 for demo)\n",
    "results = {}  # Store per-section predictions\n",
    "not_predicted = {}\n",
    "for sec in dspy_examples:\n",
    "    print(f\"Evaluating section:\\t{sec}\")\n",
    "    evaluator = Evaluate(\n",
    "        devset=dspy_examples[sec],\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    "        # max_errors=30\n",
    "    )\n",
    "    eval_res = evaluator(predictor)\n",
    "    _, result_tuples = eval_res\n",
    "    print(f\"number of results:\\t{len(result_tuples)}\")\n",
    "    preds, refs = [], []\n",
    "    not_predicted[sec] = {\n",
    "        'section':sec,\n",
    "        'num_not_predicted':0,\n",
    "        'not_predicted':[]\n",
    "    }\n",
    "    for example, prediction, correct in result_tuples:\n",
    "        if not hasattr(prediction, \"label\"):\n",
    "            not_predicted[sec]['num_not_predicted']+=1\n",
    "            not_predicted[sec]['not_predicted'].append((example, prediction, correct))\n",
    "            continue\n",
    "        preds.append(prediction.label)\n",
    "        refs.append(example.label)\n",
    "    results[sec] = {\"preds\": preds, \"refs\": refs}"
   ],
   "id": "409c65566eacfa62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's display some statistics about the results",
   "id": "f4746c046bb1c6e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "\n",
    "for sec, data in results.items():\n",
    "    preds = data['preds']\n",
    "    refs = data['refs']\n",
    "    print(f\"Section: {sec}\")\n",
    "    print(f\"  Total predictions: {len(preds)}\")\n",
    "    print(f\"  Total references:  {len(refs)}\")\n",
    "    print(f\"  Class distribution in predictions: {Counter(preds)}\")\n",
    "    print(f\"  Class distribution in references:  {Counter(refs)}\")\n",
    "    agree = sum([p == r for p, r in zip(preds, refs)])\n",
    "    print(f\"  Number of matches (agreement): {agree}\")\n",
    "    print(f\"  Accuracy (quick): {agree / len(refs):.3f}\")\n",
    "    print()\n",
    "\n",
    "# Overall stats\n",
    "all_preds = sum([v['preds'] for v in results.values()], [])\n",
    "all_refs  = sum([v['refs']  for v in results.values()], [])\n",
    "print(\"=== OVERALL ===\")\n",
    "print(f\"Total predictions: {len(all_preds)}\")\n",
    "print(f\"Total references:  {len(all_refs)}\")\n",
    "print(f\"Class distribution in predictions: {Counter(all_preds)}\")\n",
    "print(f\"Class distribution in references:  {Counter(all_refs)}\")\n",
    "agree = sum([p == r for p, r in zip(all_preds, all_refs)])\n",
    "print(f\"Number of matches (agreement): {agree}\")\n",
    "print(f\"Accuracy (quick): {agree / len(all_refs):.3f}\")\n"
   ],
   "id": "5ca7b91f2dc29bf3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now show information about non-predicted examples:",
   "id": "d7d2ca8f76c7a8e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_np = pd.DataFrame(list(not_predicted.values())).set_index(\"section\")\n",
    "exploded = df_np[\"not_predicted\"].explode()\n",
    "df_details = (\n",
    "    exploded\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"section\", \"not_predicted\": \"detail\"})\n",
    "    .join(pd.json_normalize(exploded).add_prefix(\"detail.\"))\n",
    ")\n",
    "display(df_details)\n",
    "for sec, info in not_predicted.items():\n",
    "    print(f\"=== Section: {sec} ‚Äî {info['num_not_predicted']} failures ===\")\n",
    "    for ex, raw_out, score in info['not_predicted']:\n",
    "        print(ex)\n",
    "        premise, hypothesis, ref,= ex\n",
    "        print(f\"üéØ Ref label: {ex[ref]}\")\n",
    "        print(f\"üí¨ Premise: {ex[premise]}\")\n",
    "        print(f\"üí¨ Hypothesis: {ex[hypothesis]}\")\n",
    "        print(f\"üõë Raw output: {raw_out!r}\")\n",
    "        print(f\"‚ö†Ô∏è Score: {score}\")\n",
    "        print(\"-\" * 40)"
   ],
   "id": "b7955257ecf8e406"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Prepare for metric calculation\n",
    "metric_prf = combine([\"precision\", \"recall\", \"f1\"])\n",
    "acc = load(\"accuracy\")\n",
    "rows = []\n",
    "all_preds, all_refs = [], []\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "for sec, data in results.items():\n",
    "    print(f\"Computing metrics for section: {sec}\")\n",
    "    preds = [label2id[label] for label in data[\"preds\"]]\n",
    "    refs  = [label2id[label] for label in data[\"refs\"]]\n",
    "    prf = metric_prf.compute(predictions=preds, references=refs, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "\n",
    "    rows.append({\"section\": sec, \"accuracy\": accuracy, **prf})\n",
    "    all_preds += preds\n",
    "    all_refs += refs\n",
    "\n",
    "# 3. Compute overall metrics\n",
    "overall_prf = metric_prf.compute(predictions=all_preds, references=all_refs, average=\"weighted\")\n",
    "overall_acc = acc.compute(predictions=all_preds, references=all_refs)[\"accuracy\"]\n",
    "rows.append({\"section\": \"all\", \"accuracy\": overall_acc, **overall_prf})\n",
    "\n",
    "# Create DataFrame and display\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "display(df_metrics.set_index(\"section\"))"
   ],
   "id": "20f5b67929c66534"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In our experiment we got the following results:\n",
    "| section                                       | accuracy | precision | recall  | f1       |\n",
    "|----------------------------------------------|----------|-----------|---------|----------|\n",
    "| presupposition_all_n_presupposition          | 0.942632 | 0.949257  | 0.942632| 0.942783 |\n",
    "| presupposition_both_presupposition           | 0.973158 | 0.974034  | 0.973158| 0.973184 |\n",
    "| presupposition_change_of_state               | 0.557895 | 0.655905  | 0.557895| 0.493381 |\n",
    "| presupposition_cleft_existence               | 0.686316 | 0.812531  | 0.686316| 0.669707 |\n",
    "| presupposition_cleft_uniqueness              | 0.474211 | 0.503028  | 0.474211| 0.350207 |\n",
    "| presupposition_only_presupposition           | 0.668947 | 0.778061  | 0.668947| 0.654415 |\n",
    "| presupposition_possessed_definites_existence | 0.923158 | 0.929153  | 0.923158| 0.923322 |\n",
    "| presupposition_possessed_definites_uniqueness| 0.475263 | 0.626211  | 0.475263| 0.352235 |\n",
    "| presupposition_question_presupposition       | 0.841053 | 0.863356  | 0.841053| 0.838288 |\n",
    "| all                                          | 0.726959 | 0.815532  | 0.726959| 0.717863 |\n",
    "\n",
    "With a total F1 score of 0.726959 with grok-3-mini. Let's try to optimize the model\n"
   ],
   "id": "e50fa69903b6adb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optimizing the model\n",
    "we will first create a dev\\test split:"
   ],
   "id": "fe8992aa33876994"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "\n",
    "dev_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for sec, examples in dspy_examples.items():\n",
    "    random.shuffle(examples)\n",
    "    n = len(examples)\n",
    "    split_point = int(0.7 * n)  # e.g., 70% dev, 30% test\n",
    "\n",
    "    dev_data[sec] = examples[:split_point]\n",
    "    test_data[sec] = examples[split_point:]\n",
    "display(pd.DataFrame(dev_data))\n",
    "display(pd.DataFrame(test_data))"
   ],
   "id": "ed61f1096174010d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's try Few-shot example optimization.\n",
    "We will try to optimize prompts separately for each section using few-shot example search."
   ],
   "id": "ed03933fa38856e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from datetime import datetime\n",
    "optimized_pipelines = {}\n",
    "\n",
    "for sec in dev_data:\n",
    "    # Flatten dev examples for prompt tuning\n",
    "    dev_set = dev_data[sec]\n",
    "\n",
    "    # Initialize optimizer\n",
    "    bs = BootstrapFewShot(\n",
    "        metric=accuracy_metric,\n",
    "        max_bootstrapped_demos=50,\n",
    "        max_labeled_demos=10\n",
    "    )\n",
    "\n",
    "    # Compile and tune using dev split\n",
    "    compiled = bs.compile(\n",
    "        student=predictor,\n",
    "        trainset=dev_set\n",
    "    )\n",
    "    optimized_pipelines[sec] = compiled\n",
    "    print(f\"‚úÖ Completed Bootstrapped few-shot for section `{sec}`\")\n",
    "\n",
    "# existing section pipelines\n",
    "pipelines = list(optimized_pipelines.values())\n",
    "joint_predictor = dspy.BetterTogether(*pipelines)\n",
    "joint_predictor.save(f\"joint_predictor_state_{datetime.timestamp()}.pkl\", save_program=False)"
   ],
   "id": "4b1318dbc67f041c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for sec, examples in test_data.items():\n",
    "    print(f\"Evaluating on test section: {sec}\")\n",
    "    evaluator = Evaluate(\n",
    "        devset=examples,\n",
    "        metric=accuracy_metric,\n",
    "        num_threads=20,\n",
    "        display_progress=True,\n",
    "        display_table=5,\n",
    "        provide_traceback=True,\n",
    "        max_errors=5,\n",
    "    )\n",
    "    result = evaluator(joint_predictor)\n",
    "    test_results[sec] = result"
   ],
   "id": "aa28d0ff0875e569"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metric_prf = combine([\"precision\", \"recall\", \"f1\"])\n",
    "acc = load(\"accuracy\")\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "rows = []\n",
    "all_preds, all_refs = [], []\n",
    "\n",
    "for sec, res in test_results.items():\n",
    "    print(f\"Metrics for section: {sec}\")\n",
    "    preds = [label2id[p.label] for (_, p, _) in res.results]\n",
    "    refs = [label2id[ex.label] for (ex, _, _) in res.results]\n",
    "\n",
    "    prf = metric_prf.compute(predictions=preds, references=refs, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "\n",
    "    rows.append({\"section\": sec, \"accuracy\": accuracy, **prf})\n",
    "    all_preds += preds\n",
    "    all_refs += refs\n",
    "\n",
    "overall_prf = metric_prf.compute(predictions=all_preds, references=all_refs, average=\"weighted\")\n",
    "overall_acc = acc.compute(predictions=all_preds, references=all_refs)[\"accuracy\"]\n",
    "rows.append({\"section\": \"all\", \"accuracy\": overall_acc, **overall_prf})\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "display(df_metrics.set_index(\"section\"))"
   ],
   "id": "23b5ae8dcfa60120"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
