{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres LLM Baseline\n",
    "\n",
    "You have to implement in this notebook a baseline for ImpPres classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:33.626488Z",
     "start_time": "2025-07-23T14:28:33.620541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "from os import environ\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import dspy\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"dspy.adapters.json_adapter\").setLevel(logging.ERROR)\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=environ['XAI_API_KEY'])\n",
    "\n",
    "# for ollama\n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "# lm = dspy.LM(\n",
    "#     \"ollama/llama3.1:8b\",\n",
    "#     api_base=\"http://localhost:11434\",\n",
    "#     format=\"json\"        # litellm translates this to Ollama's stream=false\n",
    "# )\n",
    "dspy.configure(lm=lm)"
   ],
   "id": "2b9b979cbc0dc715",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:33.691945Z",
     "start_time": "2025-07-23T14:28:33.684655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy program to classify pairs (premise, hypothesis) as entailment, contradiction, or neutral.\n",
    "class NLIImPresClassifier(dspy.Signature):\n",
    "    premise     :str = dspy.InputField(desc=\"A short passage or statement. All facts should be inferred from this text alone.\")\n",
    "    hypothesis  :str = dspy.InputField(desc=\"A second statement to evaluate. Check if this follows from, contradicts, or is unrelated to the premise.\")\n",
    "    label       : Literal[\"entailment\", \"neutral\", \"contradiction\"] = dspy.OutputField(\n",
    "        desc=(\n",
    "            \"Return one of: 'entailment', 'neutral', or 'contradiction'.\\n\"\n",
    "            \"- 'entailment': The hypothesis must be true if the premise is true.\\n\"\n",
    "            \"- 'contradiction': The hypothesis must be false if the premise is true.\\n\"\n",
    "            \"- 'neutral': The hypothesis could be either true or false based on the premise.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "predictor = dspy.Predict(NLIImPresClassifier)\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "def zero_shot_nli_classifier(x):\n",
    "    return {\n",
    "        'premise' : x['premise'],\n",
    "        'hypothesis': x['hypothesis'],\n",
    "        'pred_label' : predictor(premise=x['premise'], hypothesis=x['hypothesis']).label,\n",
    "        'gold_label' : label_names[x['gold_label']]\n",
    "    }"
   ],
   "id": "686e6e259245fe7a",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load ImpPres dataset",
   "id": "3bf1719d8f8eaf51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:33.905301Z",
     "start_time": "2025-07-23T14:28:33.748105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from os.path import exists\n",
    "\n",
    "# Define sections\n",
    "sections = [\n",
    "    'presupposition_all_n_presupposition',\n",
    "    'presupposition_both_presupposition',\n",
    "    'presupposition_change_of_state',\n",
    "    'presupposition_cleft_existence',\n",
    "    'presupposition_cleft_uniqueness',\n",
    "    'presupposition_only_presupposition',\n",
    "    'presupposition_possessed_definites_existence',\n",
    "    'presupposition_possessed_definites_uniqueness',\n",
    "    'presupposition_question_presupposition'\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "if not exists('combined_imppres_presuppositions.parquet'):\n",
    "    # Load each section\n",
    "    for section in sections:\n",
    "        print(f\"Loading dataset for section: {section}\")\n",
    "        dataset[section] = load_dataset(\"facebook/imppres\", section)\n",
    "\n",
    "    # Convert to dataframes and combine\n",
    "    dataframes_list = []\n",
    "    for section, data in dataset.items():\n",
    "        df = data.to_pandas()\n",
    "        df['section'] = section\n",
    "        dataframes_list.append(df)\n",
    "\n",
    "    combined_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "else:\n",
    "    combined_df = pd.read_parquet('combined_imppres_presuppositions.parquet')\n",
    "    print(f\"Loaded combined_imppres_presuppositions.parquet\")\n",
    "\n",
    "# Convert back to datasets\n",
    "dataset = {}\n",
    "for section, group in combined_df.groupby(\"section\"):\n",
    "    dataset[section] = Dataset.from_pandas(group)"
   ],
   "id": "47958515fef57a82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_imppres_presuppositions.parquet\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:33.932949Z",
     "start_time": "2025-07-23T14:28:33.928808Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "4e4a87f5ab7d4a5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presupposition_all_n_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_both_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_change_of_state': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_cleft_existence': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_cleft_uniqueness': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_only_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_possessed_definites_existence': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_possessed_definites_uniqueness': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_question_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " })}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:33.999015Z",
     "start_time": "2025-07-23T14:28:33.992011Z"
    }
   },
   "cell_type": "code",
   "source": "display(combined_df)",
   "id": "a2d7536a65ae709e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 premise  \\\n",
       "0      All ten guys that proved to boast were divorcing.   \n",
       "1      All ten guys that proved to boast were divorcing.   \n",
       "2      All ten guys that proved to boast were divorcing.   \n",
       "3      All ten guys that proved to boast weren't divo...   \n",
       "4      All ten guys that proved to boast weren't divo...   \n",
       "...                                                  ...   \n",
       "17095  If the actors do conceal where that mall shock...   \n",
       "17096  The actors didn't conceal where that mall shoc...   \n",
       "17097  Did the actors conceal where that mall shocks ...   \n",
       "17098  The actors might have concealed where that mal...   \n",
       "17099  If the actors do conceal where that mall shock...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "0       There are exactly ten guys that proved to boast.      unembedded   \n",
       "1      There are exactly eleven guys that proved to b...      unembedded   \n",
       "2      There are exactly ten senators that proved to ...      unembedded   \n",
       "3       There are exactly ten guys that proved to boast.         negated   \n",
       "4      There are exactly eleven guys that proved to b...         negated   \n",
       "...                                                  ...             ...   \n",
       "17095                               Travel shocks Janet.     conditional   \n",
       "17096  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17097  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17098  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17099  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "0      Not_In_Example  Not_In_Example        positive           0   \n",
       "1      Not_In_Example  Not_In_Example         negated           2   \n",
       "2      Not_In_Example  Not_In_Example         neutral           1   \n",
       "3      Not_In_Example  Not_In_Example        positive           0   \n",
       "4      Not_In_Example  Not_In_Example         negated           2   \n",
       "...               ...             ...             ...         ...   \n",
       "17095  Not_In_Example  Not_In_Example         neutral           1   \n",
       "17096         negated      unembedded  Not_In_Example           2   \n",
       "17097   interrogative      unembedded  Not_In_Example           1   \n",
       "17098           modal      unembedded  Not_In_Example           1   \n",
       "17099     conditional      unembedded  Not_In_Example           1   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "0         all_n_presupposition     0e           0   \n",
       "1         all_n_presupposition     1c           0   \n",
       "2         all_n_presupposition     2n           0   \n",
       "3         all_n_presupposition     3e           0   \n",
       "4         all_n_presupposition     4c           0   \n",
       "...                        ...    ...         ...   \n",
       "17095  question_presupposition  1895n          99   \n",
       "17096  question_presupposition  1896c          99   \n",
       "17097  question_presupposition  1897n          99   \n",
       "17098  question_presupposition  1898n          99   \n",
       "17099  question_presupposition  1899n          99   \n",
       "\n",
       "                                      section  \n",
       "0         presupposition_all_n_presupposition  \n",
       "1         presupposition_all_n_presupposition  \n",
       "2         presupposition_all_n_presupposition  \n",
       "3         presupposition_all_n_presupposition  \n",
       "4         presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "17095  presupposition_question_presupposition  \n",
       "17096  presupposition_question_presupposition  \n",
       "17097  presupposition_question_presupposition  \n",
       "17098  presupposition_question_presupposition  \n",
       "17099  presupposition_question_presupposition  \n",
       "\n",
       "[17100 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>0e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten senators that proved to ...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>2n</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>3e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>4c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17095</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>Travel shocks Janet.</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1895n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17096</th>\n",
       "      <td>The actors didn't conceal where that mall shoc...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1896c</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17097</th>\n",
       "      <td>Did the actors conceal where that mall shocks ...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1897n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17098</th>\n",
       "      <td>The actors might have concealed where that mal...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>modal</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1898n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17099</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>conditional</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1899n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17100 rows √ó 11 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ],
   "id": "4b5302e1aa89410d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:01:38.662588Z",
     "start_time": "2025-07-23T15:01:34.529388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ],
   "id": "f9e7a0ffbd08457",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ],
   "id": "40dd1d6e2a92bf94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will first run the dspy classifier through the dataset:",
   "id": "5c419a6458455fe6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:38.252731Z",
     "start_time": "2025-07-23T14:28:38.250801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy_metric(example, pred, *args):\n",
    "     return pred.label == example.label"
   ],
   "id": "c068b1c86ed42dfe",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:39.267810Z",
     "start_time": "2025-07-23T14:28:38.310788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# Convert to DSPy Example objects\n",
    "dspy_examples = {}\n",
    "for section_name, section in dataset.items():\n",
    "    dspy_examples[section_name] = [\n",
    "        dspy.Example(\n",
    "            premise=ex['premise'],\n",
    "            hypothesis=ex['hypothesis'],\n",
    "            label=label_names[ex['gold_label']]\n",
    "        ).with_inputs(\"premise\", \"hypothesis\")\n",
    "        for ex in section\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame(dspy_examples)"
   ],
   "id": "33c73280e69e73b1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     presupposition_all_n_presupposition presupposition_both_presupposition  \\\n",
       "0           [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1           [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "2           [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "3           [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "4           [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "...                                  ...                                ...   \n",
       "1895        [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1896        [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1897        [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1898        [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1899        [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "\n",
       "     presupposition_change_of_state presupposition_cleft_existence  \\\n",
       "0      [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "1      [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "2      [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "3      [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "4      [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "...                             ...                            ...   \n",
       "1895   [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "1896   [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "1897   [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "1898   [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "1899   [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "\n",
       "     presupposition_cleft_uniqueness presupposition_only_presupposition  \\\n",
       "0       [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1       [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "2       [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "3       [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "4       [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "...                              ...                                ...   \n",
       "1895    [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1896    [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1897    [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1898    [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1899    [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "\n",
       "     presupposition_possessed_definites_existence  \\\n",
       "0                    [premise, hypothesis, label]   \n",
       "1                    [premise, hypothesis, label]   \n",
       "2                    [premise, hypothesis, label]   \n",
       "3                    [premise, hypothesis, label]   \n",
       "4                    [premise, hypothesis, label]   \n",
       "...                                           ...   \n",
       "1895                 [premise, hypothesis, label]   \n",
       "1896                 [premise, hypothesis, label]   \n",
       "1897                 [premise, hypothesis, label]   \n",
       "1898                 [premise, hypothesis, label]   \n",
       "1899                 [premise, hypothesis, label]   \n",
       "\n",
       "     presupposition_possessed_definites_uniqueness  \\\n",
       "0                     [premise, hypothesis, label]   \n",
       "1                     [premise, hypothesis, label]   \n",
       "2                     [premise, hypothesis, label]   \n",
       "3                     [premise, hypothesis, label]   \n",
       "4                     [premise, hypothesis, label]   \n",
       "...                                            ...   \n",
       "1895                  [premise, hypothesis, label]   \n",
       "1896                  [premise, hypothesis, label]   \n",
       "1897                  [premise, hypothesis, label]   \n",
       "1898                  [premise, hypothesis, label]   \n",
       "1899                  [premise, hypothesis, label]   \n",
       "\n",
       "     presupposition_question_presupposition  \n",
       "0              [premise, hypothesis, label]  \n",
       "1              [premise, hypothesis, label]  \n",
       "2              [premise, hypothesis, label]  \n",
       "3              [premise, hypothesis, label]  \n",
       "4              [premise, hypothesis, label]  \n",
       "...                                     ...  \n",
       "1895           [premise, hypothesis, label]  \n",
       "1896           [premise, hypothesis, label]  \n",
       "1897           [premise, hypothesis, label]  \n",
       "1898           [premise, hypothesis, label]  \n",
       "1899           [premise, hypothesis, label]  \n",
       "\n",
       "[1900 rows x 9 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>presupposition_all_n_presupposition</th>\n",
       "      <th>presupposition_both_presupposition</th>\n",
       "      <th>presupposition_change_of_state</th>\n",
       "      <th>presupposition_cleft_existence</th>\n",
       "      <th>presupposition_cleft_uniqueness</th>\n",
       "      <th>presupposition_only_presupposition</th>\n",
       "      <th>presupposition_possessed_definites_existence</th>\n",
       "      <th>presupposition_possessed_definites_uniqueness</th>\n",
       "      <th>presupposition_question_presupposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1900 rows √ó 9 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "from evaluate import combine, load\n",
    "\n",
    "# 1. Run DSPy evaluation for each section (here, limited to first 10 for demo)\n",
    "results = {}  # Store per-section predictions\n",
    "not_predicted = {}\n",
    "for sec in dspy_examples:\n",
    "    print(f\"Evaluating section:\\t{sec}\")\n",
    "    evaluator = Evaluate(\n",
    "        devset=dspy_examples[sec],\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    "        # max_errors=30\n",
    "    )\n",
    "    eval_res = evaluator(predictor)\n",
    "    _, result_tuples = eval_res\n",
    "    print(f\"number of results:\\t{len(result_tuples)}\")\n",
    "    preds, refs = [], []\n",
    "    not_predicted[sec] = {\n",
    "        'section':sec,\n",
    "        'num_not_predicted':0,\n",
    "        'not_predicted':[]\n",
    "    }\n",
    "    for example, prediction, correct in result_tuples:\n",
    "        if not hasattr(prediction, \"label\"):\n",
    "            not_predicted[sec]['num_not_predicted']+=1\n",
    "            not_predicted[sec]['not_predicted'].append((example, prediction, correct))\n",
    "            continue\n",
    "        preds.append(prediction.label)\n",
    "        refs.append(example.label)\n",
    "    results[sec] = {\"preds\": preds, \"refs\": refs}"
   ],
   "id": "409c65566eacfa62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's display some statistics about the results",
   "id": "f4746c046bb1c6e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "for sec, data in results.items():\n",
    "    preds = data['preds']\n",
    "    refs = data['refs']\n",
    "    print(f\"Section: {sec}\")\n",
    "    print(f\"  Total predictions: {len(preds)}\")\n",
    "    print(f\"  Total references:  {len(refs)}\")\n",
    "    print(f\"  Class distribution in predictions: {Counter(preds)}\")\n",
    "    print(f\"  Class distribution in references:  {Counter(refs)}\")\n",
    "    agree = sum([p == r for p, r in zip(preds, refs)])\n",
    "    print(f\"  Number of matches (agreement): {agree}\")\n",
    "    print(f\"  Accuracy (quick): {agree / len(refs):.3f}\")\n",
    "    print()\n",
    "\n",
    "# Overall stats\n",
    "all_preds = sum([v['preds'] for v in results.values()], [])\n",
    "all_refs  = sum([v['refs']  for v in results.values()], [])\n",
    "print(\"=== OVERALL ===\")\n",
    "print(f\"Total predictions: {len(all_preds)}\")\n",
    "print(f\"Total references:  {len(all_refs)}\")\n",
    "print(f\"Class distribution in predictions: {Counter(all_preds)}\")\n",
    "print(f\"Class distribution in references:  {Counter(all_refs)}\")\n",
    "agree = sum([p == r for p, r in zip(all_preds, all_refs)])\n",
    "print(f\"Number of matches (agreement): {agree}\")\n",
    "print(f\"Accuracy (quick): {agree / len(all_refs):.3f}\")\n"
   ],
   "id": "5ca7b91f2dc29bf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now show information about non-predicted examples:",
   "id": "d7d2ca8f76c7a8e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_np = pd.DataFrame(list(not_predicted.values())).set_index(\"section\")\n",
    "exploded = df_np[\"not_predicted\"].explode()\n",
    "df_details = (\n",
    "    exploded\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"section\", \"not_predicted\": \"detail\"})\n",
    "    .join(pd.json_normalize(exploded).add_prefix(\"detail.\"))\n",
    ")\n",
    "display(df_details)\n",
    "for sec, info in not_predicted.items():\n",
    "    print(f\"=== Section: {sec} ‚Äî {info['num_not_predicted']} failures ===\")\n",
    "    for ex, raw_out, score in info['not_predicted']:\n",
    "        print(ex)\n",
    "        premise, hypothesis, ref,= ex\n",
    "        print(f\"üéØ Ref label: {ex[ref]}\")\n",
    "        print(f\"üí¨ Premise: {ex[premise]}\")\n",
    "        print(f\"üí¨ Hypothesis: {ex[hypothesis]}\")\n",
    "        print(f\"üõë Raw output: {raw_out!r}\")\n",
    "        print(f\"‚ö†Ô∏è Score: {score}\")\n",
    "        print(\"-\" * 40)"
   ],
   "id": "b7955257ecf8e406",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Prepare for metric calculation\n",
    "metric_prf = combine([\"precision\", \"recall\", \"f1\"])\n",
    "acc = load(\"accuracy\")\n",
    "rows = []\n",
    "all_preds, all_refs = [], []\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "for sec, data in results.items():\n",
    "    print(f\"Computing metrics for section: {sec}\")\n",
    "    preds = [label2id[label] for label in data[\"preds\"]]\n",
    "    refs  = [label2id[label] for label in data[\"refs\"]]\n",
    "    prf = metric_prf.compute(predictions=preds, references=refs, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "\n",
    "    rows.append({\"section\": sec, \"accuracy\": accuracy, **prf})\n",
    "    all_preds += preds\n",
    "    all_refs += refs\n",
    "\n",
    "# 3. Compute overall metrics\n",
    "overall_prf = metric_prf.compute(predictions=all_preds, references=all_refs, average=\"weighted\")\n",
    "overall_acc = acc.compute(predictions=all_preds, references=all_refs)[\"accuracy\"]\n",
    "rows.append({\"section\": \"all\", \"accuracy\": overall_acc, **overall_prf})\n",
    "\n",
    "# Create DataFrame and display\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "display(df_metrics.set_index(\"section\"))"
   ],
   "id": "20f5b67929c66534",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In our experiment we got the following results:\n",
    "| section                                       | accuracy | precision | recall  | f1       |\n",
    "|----------------------------------------------|----------|-----------|---------|----------|\n",
    "| presupposition_all_n_presupposition          | 0.942632 | 0.949257  | 0.942632| 0.942783 |\n",
    "| presupposition_both_presupposition           | 0.973158 | 0.974034  | 0.973158| 0.973184 |\n",
    "| presupposition_change_of_state               | 0.557895 | 0.655905  | 0.557895| 0.493381 |\n",
    "| presupposition_cleft_existence               | 0.686316 | 0.812531  | 0.686316| 0.669707 |\n",
    "| presupposition_cleft_uniqueness              | 0.474211 | 0.503028  | 0.474211| 0.350207 |\n",
    "| presupposition_only_presupposition           | 0.668947 | 0.778061  | 0.668947| 0.654415 |\n",
    "| presupposition_possessed_definites_existence | 0.923158 | 0.929153  | 0.923158| 0.923322 |\n",
    "| presupposition_possessed_definites_uniqueness| 0.475263 | 0.626211  | 0.475263| 0.352235 |\n",
    "| presupposition_question_presupposition       | 0.841053 | 0.863356  | 0.841053| 0.838288 |\n",
    "| all                                          | 0.726959 | 0.815532  | 0.726959| 0.717863 |\n",
    "\n",
    "With a total F1 score of 0.726959 with grok-3-mini. Let's try to optimize the model\n"
   ],
   "id": "e50fa69903b6adb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optimizing the model\n",
    "We are going to try optimize the model in a couple ways.\n",
    "we will first create a dev\\test split:"
   ],
   "id": "fe8992aa33876994"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:29:16.295709Z",
     "start_time": "2025-07-23T14:29:16.278662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from numpy import random\n",
    "rng = random.default_rng(42)\n",
    "\n",
    "def stratified_split(df, frac=0.7):\n",
    "    idx_dev = []\n",
    "    for (sec, lab), g in df.groupby([\"section\",\"gold_label\"]):\n",
    "        n = int(len(g)*frac)\n",
    "        idx = rng.permutation(g.index)\n",
    "        idx_dev.extend(idx[:n])\n",
    "    dev = df.loc[idx_dev]\n",
    "    test = df.drop(idx_dev)\n",
    "    return dev, test\n",
    "\n",
    "dev_df, test_df = stratified_split(combined_df)"
   ],
   "id": "ed61f1096174010d",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:29:16.344133Z",
     "start_time": "2025-07-23T14:29:16.335523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "display(pd.DataFrame(dev_df))\n",
    "display(pd.DataFrame(test_df))"
   ],
   "id": "777c40424bea1b4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 premise  \\\n",
       "1795   All eight women that compel libraries to appre...   \n",
       "500    Have the six guests that had badgered a lot of...   \n",
       "1792   Do all eight women that compel libraries to ap...   \n",
       "174    The ten women that talk didn't implore Pamela ...   \n",
       "152    All four waiters that were boring Paul have te...   \n",
       "...                                                  ...   \n",
       "15956  Patricia wasn't concealing when Renee commande...   \n",
       "15324  Museums might know when Donald neglects to hid...   \n",
       "15961      Sonia does forget when those peppers blacken.   \n",
       "16328        Does Jill conceal why the girls do scratch?   \n",
       "16920  Marla might find out why these waitresses have...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "1795   There are exactly eight women that compel libr...           modal   \n",
       "500    There are exactly six guests that had badgered...   interrogative   \n",
       "1792   There are exactly eight women that compel libr...   interrogative   \n",
       "174               There are exactly ten women that talk.         negated   \n",
       "152    There are exactly four waiters that were borin...      unembedded   \n",
       "...                                                  ...             ...   \n",
       "15956  Patricia is concealing when Renee commanded th...  Not_In_Example   \n",
       "15324   Donald doesn't neglect to hide that wheelbarrow.           modal   \n",
       "15961                       Those peppers don't blacken.      unembedded   \n",
       "16328                           The girls don't scratch.   interrogative   \n",
       "16920               These waitresses haven't retaliated.           modal   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "1795   Not_In_Example  Not_In_Example        positive           0   \n",
       "500    Not_In_Example  Not_In_Example        positive           0   \n",
       "1792   Not_In_Example  Not_In_Example        positive           0   \n",
       "174    Not_In_Example  Not_In_Example        positive           0   \n",
       "152    Not_In_Example  Not_In_Example        positive           0   \n",
       "...               ...             ...             ...         ...   \n",
       "15956         negated      unembedded  Not_In_Example           2   \n",
       "15324  Not_In_Example  Not_In_Example         negated           2   \n",
       "15961  Not_In_Example  Not_In_Example         negated           2   \n",
       "16328  Not_In_Example  Not_In_Example         negated           2   \n",
       "16920  Not_In_Example  Not_In_Example         negated           2   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "1795      all_n_presupposition  1795e          94   \n",
       "500       all_n_presupposition   500e          26   \n",
       "1792      all_n_presupposition  1792e          94   \n",
       "174       all_n_presupposition   174e           9   \n",
       "152       all_n_presupposition   152e           8   \n",
       "...                        ...    ...         ...   \n",
       "15956  question_presupposition   756c          39   \n",
       "15324  question_presupposition   124c           6   \n",
       "15961  question_presupposition   761c          40   \n",
       "16328  question_presupposition  1128c          59   \n",
       "16920  question_presupposition  1720c          90   \n",
       "\n",
       "                                      section  \n",
       "1795      presupposition_all_n_presupposition  \n",
       "500       presupposition_all_n_presupposition  \n",
       "1792      presupposition_all_n_presupposition  \n",
       "174       presupposition_all_n_presupposition  \n",
       "152       presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "15956  presupposition_question_presupposition  \n",
       "15324  presupposition_question_presupposition  \n",
       "15961  presupposition_question_presupposition  \n",
       "16328  presupposition_question_presupposition  \n",
       "16920  presupposition_question_presupposition  \n",
       "\n",
       "[11970 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>All eight women that compel libraries to appre...</td>\n",
       "      <td>There are exactly eight women that compel libr...</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1795e</td>\n",
       "      <td>94</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Have the six guests that had badgered a lot of...</td>\n",
       "      <td>There are exactly six guests that had badgered...</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>500e</td>\n",
       "      <td>26</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>Do all eight women that compel libraries to ap...</td>\n",
       "      <td>There are exactly eight women that compel libr...</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1792e</td>\n",
       "      <td>94</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>The ten women that talk didn't implore Pamela ...</td>\n",
       "      <td>There are exactly ten women that talk.</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>174e</td>\n",
       "      <td>9</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>All four waiters that were boring Paul have te...</td>\n",
       "      <td>There are exactly four waiters that were borin...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>152e</td>\n",
       "      <td>8</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15956</th>\n",
       "      <td>Patricia wasn't concealing when Renee commande...</td>\n",
       "      <td>Patricia is concealing when Renee commanded th...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>756c</td>\n",
       "      <td>39</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15324</th>\n",
       "      <td>Museums might know when Donald neglects to hid...</td>\n",
       "      <td>Donald doesn't neglect to hide that wheelbarrow.</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>124c</td>\n",
       "      <td>6</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15961</th>\n",
       "      <td>Sonia does forget when those peppers blacken.</td>\n",
       "      <td>Those peppers don't blacken.</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>761c</td>\n",
       "      <td>40</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16328</th>\n",
       "      <td>Does Jill conceal why the girls do scratch?</td>\n",
       "      <td>The girls don't scratch.</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1128c</td>\n",
       "      <td>59</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16920</th>\n",
       "      <td>Marla might find out why these waitresses have...</td>\n",
       "      <td>These waitresses haven't retaliated.</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1720c</td>\n",
       "      <td>90</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11970 rows √ó 11 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "                                                 premise  \\\n",
       "3      All ten guys that proved to boast weren't divo...   \n",
       "4      All ten guys that proved to boast weren't divo...   \n",
       "10     All ten guys that proved to boast might have b...   \n",
       "14     If all ten guys that proved to boast were divo...   \n",
       "19     All ten reports that can bore some waiter are ...   \n",
       "...                                                  ...   \n",
       "17087  Did the actors conceal where that mall shocks ...   \n",
       "17090  The actors might have concealed where that mal...   \n",
       "17092  The actors might have concealed where that mal...   \n",
       "17093  If the actors do conceal where that mall shock...   \n",
       "17096  The actors didn't conceal where that mall shoc...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "3       There are exactly ten guys that proved to boast.         negated   \n",
       "4      There are exactly eleven guys that proved to b...         negated   \n",
       "10     There are exactly eleven guys that proved to b...           modal   \n",
       "14     There are exactly ten senators that proved to ...     conditional   \n",
       "19     There are exactly ten reports that can bore so...      unembedded   \n",
       "...                                                  ...             ...   \n",
       "17087                             That mall shocks Janet   interrogative   \n",
       "17090                             That mall shocks Janet           modal   \n",
       "17092                               Travel shocks Janet.           modal   \n",
       "17093                             That mall shocks Janet     conditional   \n",
       "17096  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "3      Not_In_Example  Not_In_Example        positive           0   \n",
       "4      Not_In_Example  Not_In_Example         negated           2   \n",
       "10     Not_In_Example  Not_In_Example         negated           2   \n",
       "14     Not_In_Example  Not_In_Example         neutral           1   \n",
       "19     Not_In_Example  Not_In_Example        positive           0   \n",
       "...               ...             ...             ...         ...   \n",
       "17087  Not_In_Example  Not_In_Example        positive           0   \n",
       "17090  Not_In_Example  Not_In_Example        positive           0   \n",
       "17092  Not_In_Example  Not_In_Example         neutral           1   \n",
       "17093  Not_In_Example  Not_In_Example        positive           0   \n",
       "17096         negated      unembedded  Not_In_Example           2   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "3         all_n_presupposition     3e           0   \n",
       "4         all_n_presupposition     4c           0   \n",
       "10        all_n_presupposition    10c           0   \n",
       "14        all_n_presupposition    14n           0   \n",
       "19        all_n_presupposition    19e           1   \n",
       "...                        ...    ...         ...   \n",
       "17087  question_presupposition  1887e          99   \n",
       "17090  question_presupposition  1890e          99   \n",
       "17092  question_presupposition  1892n          99   \n",
       "17093  question_presupposition  1893e          99   \n",
       "17096  question_presupposition  1896c          99   \n",
       "\n",
       "                                      section  \n",
       "3         presupposition_all_n_presupposition  \n",
       "4         presupposition_all_n_presupposition  \n",
       "10        presupposition_all_n_presupposition  \n",
       "14        presupposition_all_n_presupposition  \n",
       "19        presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "17087  presupposition_question_presupposition  \n",
       "17090  presupposition_question_presupposition  \n",
       "17092  presupposition_question_presupposition  \n",
       "17093  presupposition_question_presupposition  \n",
       "17096  presupposition_question_presupposition  \n",
       "\n",
       "[5130 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>3e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>4c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>All ten guys that proved to boast might have b...</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>10c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>If all ten guys that proved to boast were divo...</td>\n",
       "      <td>There are exactly ten senators that proved to ...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>14n</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>All ten reports that can bore some waiter are ...</td>\n",
       "      <td>There are exactly ten reports that can bore so...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>19e</td>\n",
       "      <td>1</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17087</th>\n",
       "      <td>Did the actors conceal where that mall shocks ...</td>\n",
       "      <td>That mall shocks Janet</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1887e</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17090</th>\n",
       "      <td>The actors might have concealed where that mal...</td>\n",
       "      <td>That mall shocks Janet</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1890e</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17092</th>\n",
       "      <td>The actors might have concealed where that mal...</td>\n",
       "      <td>Travel shocks Janet.</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1892n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17093</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>That mall shocks Janet</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1893e</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17096</th>\n",
       "      <td>The actors didn't conceal where that mall shoc...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1896c</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5130 rows √ó 11 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:29:16.470165Z",
     "start_time": "2025-07-23T14:29:16.395771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_examples(df):\n",
    "    return [dspy.Example(\n",
    "        premise=r.premise, hypothesis=r.hypothesis,\n",
    "        label=label_names[r.gold_label]\n",
    "    ).with_inputs(\"premise\",\"hypothesis\") for r in df.itertuples()]\n",
    "dev_ex  = to_examples(dev_df)\n",
    "test_ex = to_examples(test_df)"
   ],
   "id": "72daff261fe243f7",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(model):\n",
    "    results = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    "    )(model)\n",
    "    score,results = results\n",
    "    print(f\"Score:\\t{score}\")\n",
    "    test_pred = [label2id[out[1].label] for out in results]\n",
    "    return score, results, test_pred\n",
    "\n",
    "def compute_matrices(test_pred):\n",
    "    prf = metric_prf.compute(predictions=test_pred, references=y_true, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=test_pred, references=y_true)\n",
    "    return {**prf, **accuracy}"
   ],
   "id": "77aea4441ccc9fc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:46:48.019954Z",
     "start_time": "2025-07-23T14:46:43.589493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictor_test_predictions = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    ")(predictor)"
   ],
   "id": "aec84683f7362d93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2278.00 / 2977 (76.5%):  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 2976/5130 [00:02<00:00, 2598.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:08.073192Z",
     "start_time": "2025-07-23T15:15:08.064900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "score,predictor_test_predictions_results = predictor_test_predictions\n",
    "print(f\"Score: {score}\")\n",
    "predictor_test_pred = [label2id[out[1].label] for out in predictor_test_predictions_results]\n",
    "y_true = [label2id[ex.label]  for ex in test_ex]"
   ],
   "id": "37dfea0647d46d01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 72.48\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:14.557289Z",
     "start_time": "2025-07-23T15:15:10.494869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictor_prf = metric_prf.compute(predictions=predictor_test_pred, references=y_true, average=\"weighted\")\n",
    "predictor_accuracy = acc.compute(predictions=predictor_test_pred, references=y_true)\n",
    "predictor_combined = {**predictor_prf, **predictor_accuracy}"
   ],
   "id": "24acf6b7d33c6e07",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:16.965478Z",
     "start_time": "2025-07-23T15:15:16.961608Z"
    }
   },
   "cell_type": "code",
   "source": "display(pd.DataFrame(predictor_combined, index=[\"Original predictor\"]))",
   "id": "c81e5b6b99f149eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    precision    recall        f1  accuracy\n",
       "Original predictor   0.813633  0.724756  0.715039  0.724756"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original predictor</th>\n",
       "      <td>0.813633</td>\n",
       "      <td>0.724756</td>\n",
       "      <td>0.715039</td>\n",
       "      <td>0.724756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In our code we saw a F1 score of 0.715039 on the test.",
   "id": "367bc4a21287ce75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simply few-shot strategy over the entire dataset",
   "id": "77973717f459d0b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:31:37.521429Z",
     "start_time": "2025-07-23T14:29:16.499344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "bs = BootstrapFewShot(metric=accuracy_metric, max_bootstrapped_demos=20, max_labeled_demos=16)\n",
    "overall_optimized = bs.compile(student=predictor, trainset=dev_ex)"
   ],
   "id": "4b1318dbc67f041c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/11970 [02:15<19:36:13,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 20 full traces after 23 examples for up to 1 rounds, amounting to 23 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:46:20.633031Z",
     "start_time": "2025-07-23T14:35:48.167812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Evaluate\n",
    "overall_report = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    ")(overall_optimized)"
   ],
   "id": "4b6030edd5815279",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3914.00 / 5130 (76.3%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5130/5130 [10:32<00:00,  8.12it/s]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 17:46:20 INFO dspy.evaluate.evaluate: Average Metric: 3914 / 5130 (76.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:41.683635Z",
     "start_time": "2025-07-23T15:15:41.676403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2) Extract labels\n",
    "overall_score,overall_report_results = overall_report\n",
    "print(f\"Score: {overall_score}\")\n",
    "overall_test_pred = [label2id[out[1].label] for out in overall_report_results]"
   ],
   "id": "ff158e4ed4f19e69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 76.3\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:46.827090Z",
     "start_time": "2025-07-23T15:15:42.763710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "overall_prf = metric_prf.compute(predictions=overall_test_pred, references=y_true, average=\"weighted\")\n",
    "overall_accuracy = acc.compute(predictions=overall_test_pred, references=y_true)\n",
    "overall_combined = {**overall_prf, **overall_accuracy}"
   ],
   "id": "b49a734bbc008a1a",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:46.892950Z",
     "start_time": "2025-07-23T15:15:46.889042Z"
    }
   },
   "cell_type": "code",
   "source": "display(pd.DataFrame(overall_combined, index=[\"Overall_optimized predictor\"]))",
   "id": "b7499fb9b43a28e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             precision    recall        f1  accuracy\n",
       "Overall_optimized predictor   0.826545  0.762963  0.758567  0.762963"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall_optimized predictor</th>\n",
       "      <td>0.826545</td>\n",
       "      <td>0.762963</td>\n",
       "      <td>0.758567</td>\n",
       "      <td>0.762963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When testing the overall model, we saw F1 Score of 0.758567, an improvement of 6.087%!",
   "id": "de3196a2632c1f64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Adaptive few-shot strategy\n",
    "We will now try to optimize for each section and create a new model which will predicate by majority vote."
   ],
   "id": "ed03933fa38856e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:29:59.714330Z",
     "start_time": "2025-07-23T15:29:59.657766Z"
    }
   },
   "cell_type": "code",
   "source": "sec_dev_ex = {sec: to_examples(group) for sec, group in dev_df.groupby(\"section\")}",
   "id": "df6539e3ac2fee22",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimized_pipelines = {}\n",
    "\n",
    "for sec in sec_dev_ex:\n",
    "    print(f\"Optimizing for section: {sec}\")\n",
    "    # Flatten dev examples for prompt tuning\n",
    "    dev_set = sec_dev_ex[sec]\n",
    "\n",
    "    # Initialize optimizer\n",
    "    bs = BootstrapFewShot(\n",
    "        metric=accuracy_metric,\n",
    "        max_bootstrapped_demos=8,\n",
    "        max_labeled_demos=4\n",
    "    )\n",
    "\n",
    "    # Compile and tune using dev split\n",
    "    compiled = bs.compile(\n",
    "        student=predictor,\n",
    "        trainset=dev_set\n",
    "    )\n",
    "    optimized_pipelines[sec] = compiled"
   ],
   "id": "f1bfbe1173b8e658",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T16:05:04.051873Z",
     "start_time": "2025-07-23T16:05:04.050005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# existing section pipelines\n",
    "ensemble_tp = dspy.Ensemble(reduce_fn=dspy.majority)\n",
    "adaptive_optimized = ensemble_tp.compile(list(optimized_pipelines.values()))"
   ],
   "id": "ae6e76930f5c4558",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T17:42:31.542203Z",
     "start_time": "2025-07-23T16:05:04.112934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "adaptive_report = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    ")(adaptive_optimized)"
   ],
   "id": "6b0d08aed1459bb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3821.00 / 5130 (74.5%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5130/5130 [1:37:27<00:00,  1.14s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 20:42:31 INFO dspy.evaluate.evaluate: Average Metric: 3821 / 5130 (74.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T17:44:38.531365Z",
     "start_time": "2025-07-23T17:44:38.474610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "adaptive_score,adaptive_report_results = adaptive_report\n",
    "print(f\"Score: {adaptive_score}\")\n",
    "adaptive_test_pred = [label2id[out[1].label] for out in adaptive_report_results]"
   ],
   "id": "e0ca3387c359351f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 74.48\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T17:44:43.145493Z",
     "start_time": "2025-07-23T17:44:40.076006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "adaptive_prf = metric_prf.compute(predictions=adaptive_test_pred, references=y_true, average=\"weighted\")\n",
    "adaptive_accuracy = acc.compute(predictions=adaptive_test_pred, references=y_true)\n",
    "adaptive_combined = {**adaptive_prf, **adaptive_accuracy}"
   ],
   "id": "383d148a01fdfb84",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T17:44:43.177543Z",
     "start_time": "2025-07-23T17:44:43.173971Z"
    }
   },
   "cell_type": "code",
   "source": "display(pd.DataFrame(adaptive_combined, index=[\"Adaptive_optimized predictor\"]))",
   "id": "98550da084f9e8c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                              precision    recall        f1  accuracy\n",
       "Adaptive_optimized predictor   0.821135  0.744834  0.738229  0.744834"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adaptive_optimized predictor</th>\n",
       "      <td>0.821135</td>\n",
       "      <td>0.744834</td>\n",
       "      <td>0.738229</td>\n",
       "      <td>0.744834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We got:\n",
    "0.821135,0.744834,0.738229,0.744834\n",
    "this shows around 0.02 improvement."
   ],
   "id": "d3a03cf39db51171"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1e408e95c79e5bb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "bsrs = BootstrapFewShotWithRandomSearch(\n",
    "    metric=accuracy_metric,\n",
    "    max_bootstrapped_demos=20,\n",
    "    max_labeled_demos=9,\n",
    "    num_candidate_programs=5,\n",
    "    num_threads=50\n",
    ")\n",
    "opted_rs = bsrs.compile(student=predictor, trainset=dev_ex, valset=dev_ex)"
   ],
   "id": "f8288e547ee09a9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1c7a46f92ab6043c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "mipro = MIPROv2(metric=accuracy_metric, auto=\"light\")\n",
    "opted_mipro = mipro.compile(predictor, trainset=dev_ex)"
   ],
   "id": "8dffca042122d056"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mipro_report = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    ")(opted_mipro)"
   ],
   "id": "488952d6e2e2a5c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mipro_score,mipro_report_results = mipro_report\n",
    "print(f\"Score: {mipro_score}\")\n",
    "mipro_test_pred = [label2id[out[1].label] for out in mipro_report_results]\n",
    "mipro_combined = compute_matrices(mipro_test_pred)"
   ],
   "id": "a2a3b4dc66a11dc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "display(pd.DataFrame(mipro_combined, index=[\"mipro_combined predictor\"]))",
   "id": "7d13e13deb2e16de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bd022cac18fef56d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dspy.teleprompt import Ensemble\n",
    "ensemble = Ensemble(reduce_fn=dspy.majority)\n",
    "combined = ensemble.compile([opted_rs, opted_mipro])"
   ],
   "id": "edeb3f14d590a71b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "combined_score, combined_report, combined_test_pred = evaluate(combined)\n",
    "combined_combined = compute_matrices(combined_test_pred)\n",
    "display(pd.DataFrame(combined_combined, index=[\"combined predictor\"]))"
   ],
   "id": "b977c881515b38b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:42:16.497813075Z",
     "start_time": "2025-07-20T16:54:07.329451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metric_prf = combine([\"precision\", \"recall\", \"f1\"])\n",
    "acc = load(\"accuracy\")\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}"
   ],
   "id": "23b5ae8dcfa60120",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's examine the results:\n",
    "\n",
    "| Section                                          | Accuracy | Precision | Recall   | F1 Score |\n",
    "|--------------------------------------------------|----------|-----------|----------|----------|\n",
    "| presupposition_all_n_presupposition              | 0.991228 | 0.991400  | 0.991228 | 0.991237 |\n",
    "| presupposition_both_presupposition               | 0.984211 | 0.984519  | 0.984211 | 0.984191 |\n",
    "| presupposition_change_of_state                   | 0.557895 | 0.652114  | 0.557895 | 0.491531 |\n",
    "| presupposition_cleft_existence                   | 0.743860 | 0.835004  | 0.743860 | 0.736982 |\n",
    "| presupposition_cleft_uniqueness                  | 0.496491 | 0.769661  | 0.496491 | 0.385733 |\n",
    "| presupposition_only_presupposition               | 0.700000 | 0.813519  | 0.700000 | 0.685228 |\n",
    "| presupposition_possessed_definites_existence     | 0.964912 | 0.965735  | 0.964912 | 0.964956 |\n",
    "| presupposition_possessed_definites_uniqueness    | 0.463158 | 0.769068  | 0.463158 | 0.343951 |\n",
    "| presupposition_question_presupposition           | 0.863158 | 0.875327  | 0.863158 | 0.861277 |\n",
    "| all                                              | 0.751657 | 0.828935  | 0.751657 | 0.745117 |\n",
    "\n",
    "Total F1 score of 0.745, not that much of an improvement :(\n",
    "\n",
    "let's try to optimize it in another way.\n"
   ],
   "id": "36dc383dc3a22118"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prepare prediction variables for comparison\n",
    "\n",
    "Before comparing with DeBERTa, let's prepare all the prediction variables we need:\n"
   ],
   "id": "fd2cfb5e9f7637ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert numeric predictions back to string labels for comparison\n",
    "id2label = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "\n",
    "# Zero-shot predictions (from the original predictor)\n",
    "zs_preds = [id2label[pred] for pred in predictor_test_pred]\n",
    "\n",
    "# Bootstrap few-shot predictions (from overall_optimized)\n",
    "bs_preds = [id2label[pred] for pred in overall_test_pred]\n",
    "\n",
    "# Random search predictions (from opted_rs)\n",
    "rs_report = Evaluate(\n",
    "    devset=test_ex,\n",
    "    metric=accuracy_metric,\n",
    "    return_outputs=True,\n",
    "    num_threads=50,\n",
    "    display_progress=True,\n",
    "    display_table=False,\n",
    "    provide_traceback=False\n",
    ")(opted_rs)\n",
    "rs_score, rs_results = rs_report\n",
    "rs_preds = [id2label[label2id[out[1].label]] for out in rs_results]\n",
    "\n",
    "# MIPROv2 predictions (from opted_mipro)\n",
    "mi_preds = [id2label[pred] for pred in mipro_test_pred]\n",
    "\n",
    "# Ensemble predictions (from combined model)\n",
    "ens_preds = [id2label[pred] for pred in combined_test_pred]\n",
    "\n",
    "# Add gold_label_str column to test_df\n",
    "test_df = test_df.copy()\n",
    "test_df['gold_label_str'] = test_df['gold_label'].map(id2label)\n",
    "\n",
    "# Define hf_metrics function\n",
    "def hf_metrics(preds, refs):\n",
    "    \"\"\"Compute HuggingFace metrics for predictions and references\"\"\"\n",
    "    pred_ids = [label2id[p] for p in preds]\n",
    "    ref_ids = [label2id[r] for r in refs]\n",
    "    \n",
    "    prf = metric_prf.compute(predictions=pred_ids, references=ref_ids, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=pred_ids, references=ref_ids)\n",
    "    \n",
    "    return {**prf, **accuracy}\n",
    "\n",
    "print(\"All prediction variables prepared successfully!\")\n",
    "print(f\"zs_preds length: {len(zs_preds)}\")\n",
    "print(f\"bs_preds length: {len(bs_preds)}\")\n",
    "print(f\"rs_preds length: {len(rs_preds)}\")\n",
    "print(f\"mi_preds length: {len(mi_preds)}\")\n",
    "print(f\"ens_preds length: {len(ens_preds)}\")\n"
   ],
   "id": "9396b7e758643182"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Comparing with DeBERTa",
   "id": "c5fdcf07f41b5409"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from os.path import exists\n",
    "\n",
    "# Load DeBERTa predictions if available\n",
    "if exists(\"deberta_item_preds.parquet\"):\n",
    "    deb = pd.read_parquet(\"deberta_item_preds.parquet\")\n",
    "    print(\"Loaded DeBERTa predictions from deberta_item_preds.parquet\")\n",
    "else:\n",
    "    print(\"Warning: deberta_item_preds.parquet not found!\")\n",
    "    print(\"This file should be generated by running imppres_baseline.ipynb first.\")\n",
    "    print(\"Skipping DeBERTa comparison section...\")\n",
    "    \n",
    "    # Create a summary of our LLM models without DeBERTa comparison\n",
    "    def pack_metrics_simple(name, preds):\n",
    "        return {\"model\": name, **hf_metrics(preds, test_df.gold_label_str.tolist())}\n",
    "\n",
    "    summary = [\n",
    "        pack_metrics_simple(\"ZeroShot\", zs_preds),\n",
    "        pack_metrics_simple(\"BootstrapFS\", bs_preds),\n",
    "        pack_metrics_simple(\"RandSearch\", rs_preds),\n",
    "        pack_metrics_simple(\"MIPROv2\", mi_preds),\n",
    "        pack_metrics_simple(\"Ensemble(RS+MI)\", ens_preds),\n",
    "    ]\n",
    "    summary_df = pd.DataFrame(summary).set_index(\"model\").sort_values(\"f1\", ascending=False)\n",
    "    print(\"\\nLLM Model Performance Summary:\")\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Exit early if no DeBERTa predictions\n",
    "    deb = None\n",
    "\n",
    "# Only run DeBERTa comparison if predictions are available\n",
    "if deb is not None:\n",
    "    # Build df for the current LLM model (example: zero-shot)\n",
    "    llm_df = test_df[[\"UID\",\"section\",\"gold_label_str\"]].copy()\n",
    "    llm_df[\"llm_pred\"] = zs_preds  # or bs_preds / rs_preds / ...\n",
    "\n",
    "    # Join\n",
    "    merged = llm_df.merge(deb[[\"UID\",\"deberta_pred\"]], on=\"UID\", how=\"inner\")\n",
    "\n",
    "    # Agreement counts\n",
    "    def agreement_counts(df, gold_col=\"gold_label_str\", p1=\"llm_pred\", p2=\"deberta_pred\"):\n",
    "        g = df[gold_col].values\n",
    "        a = df[p1].values\n",
    "        b = df[p2].values\n",
    "        both_correct  = ((a==g) & (b==g)).sum()\n",
    "        correct1_only = ((a==g) & (b!=g)).sum()\n",
    "        correct2_only = ((b==g) & (a!=g)).sum()\n",
    "        both_wrong    = ((a!=g) & (b!=g)).sum()\n",
    "        return both_correct, correct1_only, correct2_only, both_wrong\n",
    "\n",
    "    both, c1, c2, wrong = agreement_counts(merged)\n",
    "    agree_table = pd.DataFrame(\n",
    "        [[both, c1, c2, wrong]],\n",
    "        columns=[\"Correct (both)\", \"Correct1 (LLM only)\", \"Correct2 (DeBERTa only)\", \"Incorrect (both)\"],\n",
    "        index=[\"ZeroShot_vs_DeBERTa\"]\n",
    "    )\n",
    "    display(agree_table)\n",
    "\n",
    "    # Per-section agreement\n",
    "    def per_section_agreement(df):\n",
    "        rows = []\n",
    "        for sec, g in df.groupby(\"section\"):\n",
    "            b, c1, c2, w = agreement_counts(g)\n",
    "            rows.append([sec, b, c1, c2, w])\n",
    "        return pd.DataFrame(rows, columns=[\"section\",\"Correct\",\"Correct1\",\"Correct2\",\"Incorrect\"]).set_index(\"section\")\n",
    "\n",
    "    display(per_section_agreement(merged))\n",
    "\n",
    "    #%%\n",
    "    def compare_to_deberta(name, preds):\n",
    "        tmp = test_df[[\"UID\",\"section\",\"gold_label_str\"]].copy()\n",
    "        tmp[\"llm_pred\"] = preds\n",
    "        mer = tmp.merge(deb[[\"UID\",\"deberta_pred\"]], on=\"UID\")\n",
    "        b,c1,c2,w = agreement_counts(mer)\n",
    "        return pd.Series({\"model\":name,\"Correct\":b,\"Correct1\":c1,\"Correct2\":c2,\"Incorrect\":w})\n",
    "\n",
    "    rows = []\n",
    "    rows.append(compare_to_deberta(\"ZeroShot\", zs_preds))\n",
    "    rows.append(compare_to_deberta(\"BootstrapFS\", bs_preds))\n",
    "    rows.append(compare_to_deberta(\"RandSearch\", rs_preds))\n",
    "    rows.append(compare_to_deberta(\"MIPROv2\", mi_preds))\n",
    "    rows.append(compare_to_deberta(\"Ensemble(RS+MI)\", ens_preds))\n",
    "\n",
    "    agree_all_df = pd.DataFrame(rows).set_index(\"model\")\n",
    "    display(agree_all_df)\n",
    "    #%%\n",
    "    def pack_metrics(name, preds):\n",
    "        return {\"model\": name, **hf_metrics(preds, test_df.gold_label_str.tolist())}\n",
    "\n",
    "    summary = [\n",
    "        pack_metrics(\"ZeroShot\", zs_preds),\n",
    "        pack_metrics(\"BootstrapFS\", bs_preds),\n",
    "        pack_metrics(\"RandSearch\", rs_preds),\n",
    "        pack_metrics(\"MIPROv2\", mi_preds),\n",
    "        pack_metrics(\"Ensemble(RS+MI)\", ens_preds),\n",
    "        pack_metrics(\"DeBERTa\", deb.loc[deb.UID.isin(test_df.UID),\"deberta_pred\"].tolist()),\n",
    "    ]\n",
    "    summary_df = pd.DataFrame(summary).set_index(\"model\").sort_values(\"f1\", ascending=False)\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print(\"DeBERTa comparison section skipped due to missing predictions file.\")"
   ],
   "id": "a25a08c81b0aec5e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
