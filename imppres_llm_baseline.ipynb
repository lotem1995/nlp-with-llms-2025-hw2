{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres LLM Baseline\n",
    "\n",
    "You have to implement in this notebook a baseline for ImpPres classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:33.626488Z",
     "start_time": "2025-07-23T14:28:33.620541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "from os import environ\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import dspy\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"dspy.adapters.json_adapter\").setLevel(logging.ERROR)\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=environ['XAI_API_KEY'])\n",
    "\n",
    "# for ollama\n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "# lm = dspy.LM(\n",
    "#     \"ollama/llama3.1:8b\",\n",
    "#     api_base=\"http://localhost:11434\",\n",
    "#     format=\"json\"        # litellm translates this to Ollama's stream=false\n",
    "# )\n",
    "dspy.configure(lm=lm)"
   ],
   "id": "2b9b979cbc0dc715",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:33.691945Z",
     "start_time": "2025-07-23T14:28:33.684655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy program to classify pairs (premise, hypothesis) as entailment, contradiction, or neutral.\n",
    "class NLIImPresClassifier(dspy.Signature):\n",
    "    premise     :str = dspy.InputField(desc=\"A short passage or statement. All facts should be inferred from this text alone.\")\n",
    "    hypothesis  :str = dspy.InputField(desc=\"A second statement to evaluate. Check if this follows from, contradicts, or is unrelated to the premise.\")\n",
    "    label       : Literal[\"entailment\", \"neutral\", \"contradiction\"] = dspy.OutputField(\n",
    "        desc=(\n",
    "            \"Return one of: 'entailment', 'neutral', or 'contradiction'.\\n\"\n",
    "            \"- 'entailment': The hypothesis must be true if the premise is true.\\n\"\n",
    "            \"- 'contradiction': The hypothesis must be false if the premise is true.\\n\"\n",
    "            \"- 'neutral': The hypothesis could be either true or false based on the premise.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "predictor = dspy.Predict(NLIImPresClassifier)\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "def zero_shot_nli_classifier(x):\n",
    "    return {\n",
    "        'premise' : x['premise'],\n",
    "        'hypothesis': x['hypothesis'],\n",
    "        'pred_label' : predictor(premise=x['premise'], hypothesis=x['hypothesis']).label,\n",
    "        'gold_label' : label_names[x['gold_label']]\n",
    "    }"
   ],
   "id": "686e6e259245fe7a",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load ImpPres dataset",
   "id": "3bf1719d8f8eaf51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:33.905301Z",
     "start_time": "2025-07-23T14:28:33.748105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from os.path import exists\n",
    "\n",
    "# Define sections\n",
    "sections = [\n",
    "    'presupposition_all_n_presupposition',\n",
    "    'presupposition_both_presupposition',\n",
    "    'presupposition_change_of_state',\n",
    "    'presupposition_cleft_existence',\n",
    "    'presupposition_cleft_uniqueness',\n",
    "    'presupposition_only_presupposition',\n",
    "    'presupposition_possessed_definites_existence',\n",
    "    'presupposition_possessed_definites_uniqueness',\n",
    "    'presupposition_question_presupposition'\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "if not exists('combined_imppres_presuppositions.parquet'):\n",
    "    # Load each section\n",
    "    for section in sections:\n",
    "        print(f\"Loading dataset for section: {section}\")\n",
    "        dataset[section] = load_dataset(\"facebook/imppres\", section)\n",
    "\n",
    "    # Convert to dataframes and combine\n",
    "    dataframes_list = []\n",
    "    for section, data in dataset.items():\n",
    "        df = data.to_pandas()\n",
    "        df['section'] = section\n",
    "        dataframes_list.append(df)\n",
    "\n",
    "    combined_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "else:\n",
    "    combined_df = pd.read_parquet('combined_imppres_presuppositions.parquet')\n",
    "    print(f\"Loaded combined_imppres_presuppositions.parquet\")\n",
    "\n",
    "# Convert back to datasets\n",
    "dataset = {}\n",
    "for section, group in combined_df.groupby(\"section\"):\n",
    "    dataset[section] = Dataset.from_pandas(group)"
   ],
   "id": "47958515fef57a82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_imppres_presuppositions.parquet\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:33.932949Z",
     "start_time": "2025-07-23T14:28:33.928808Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "4e4a87f5ab7d4a5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presupposition_all_n_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_both_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_change_of_state': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_cleft_existence': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_cleft_uniqueness': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_only_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_possessed_definites_existence': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_possessed_definites_uniqueness': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_question_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " })}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:33.999015Z",
     "start_time": "2025-07-23T14:28:33.992011Z"
    }
   },
   "cell_type": "code",
   "source": "display(combined_df)",
   "id": "a2d7536a65ae709e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 premise  \\\n",
       "0      All ten guys that proved to boast were divorcing.   \n",
       "1      All ten guys that proved to boast were divorcing.   \n",
       "2      All ten guys that proved to boast were divorcing.   \n",
       "3      All ten guys that proved to boast weren't divo...   \n",
       "4      All ten guys that proved to boast weren't divo...   \n",
       "...                                                  ...   \n",
       "17095  If the actors do conceal where that mall shock...   \n",
       "17096  The actors didn't conceal where that mall shoc...   \n",
       "17097  Did the actors conceal where that mall shocks ...   \n",
       "17098  The actors might have concealed where that mal...   \n",
       "17099  If the actors do conceal where that mall shock...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "0       There are exactly ten guys that proved to boast.      unembedded   \n",
       "1      There are exactly eleven guys that proved to b...      unembedded   \n",
       "2      There are exactly ten senators that proved to ...      unembedded   \n",
       "3       There are exactly ten guys that proved to boast.         negated   \n",
       "4      There are exactly eleven guys that proved to b...         negated   \n",
       "...                                                  ...             ...   \n",
       "17095                               Travel shocks Janet.     conditional   \n",
       "17096  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17097  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17098  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17099  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "0      Not_In_Example  Not_In_Example        positive           0   \n",
       "1      Not_In_Example  Not_In_Example         negated           2   \n",
       "2      Not_In_Example  Not_In_Example         neutral           1   \n",
       "3      Not_In_Example  Not_In_Example        positive           0   \n",
       "4      Not_In_Example  Not_In_Example         negated           2   \n",
       "...               ...             ...             ...         ...   \n",
       "17095  Not_In_Example  Not_In_Example         neutral           1   \n",
       "17096         negated      unembedded  Not_In_Example           2   \n",
       "17097   interrogative      unembedded  Not_In_Example           1   \n",
       "17098           modal      unembedded  Not_In_Example           1   \n",
       "17099     conditional      unembedded  Not_In_Example           1   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "0         all_n_presupposition     0e           0   \n",
       "1         all_n_presupposition     1c           0   \n",
       "2         all_n_presupposition     2n           0   \n",
       "3         all_n_presupposition     3e           0   \n",
       "4         all_n_presupposition     4c           0   \n",
       "...                        ...    ...         ...   \n",
       "17095  question_presupposition  1895n          99   \n",
       "17096  question_presupposition  1896c          99   \n",
       "17097  question_presupposition  1897n          99   \n",
       "17098  question_presupposition  1898n          99   \n",
       "17099  question_presupposition  1899n          99   \n",
       "\n",
       "                                      section  \n",
       "0         presupposition_all_n_presupposition  \n",
       "1         presupposition_all_n_presupposition  \n",
       "2         presupposition_all_n_presupposition  \n",
       "3         presupposition_all_n_presupposition  \n",
       "4         presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "17095  presupposition_question_presupposition  \n",
       "17096  presupposition_question_presupposition  \n",
       "17097  presupposition_question_presupposition  \n",
       "17098  presupposition_question_presupposition  \n",
       "17099  presupposition_question_presupposition  \n",
       "\n",
       "[17100 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>0e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten senators that proved to ...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>2n</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>3e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>4c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17095</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>Travel shocks Janet.</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1895n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17096</th>\n",
       "      <td>The actors didn't conceal where that mall shoc...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1896c</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17097</th>\n",
       "      <td>Did the actors conceal where that mall shocks ...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1897n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17098</th>\n",
       "      <td>The actors might have concealed where that mal...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>modal</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1898n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17099</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>conditional</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1899n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17100 rows × 11 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ],
   "id": "4b5302e1aa89410d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:01:38.662588Z",
     "start_time": "2025-07-23T15:01:34.529388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ],
   "id": "f9e7a0ffbd08457",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ],
   "id": "40dd1d6e2a92bf94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will first run the dspy classifier through the dataset:",
   "id": "5c419a6458455fe6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:38.252731Z",
     "start_time": "2025-07-23T14:28:38.250801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy_metric(example, pred, *args):\n",
    "     return pred.label == example.label"
   ],
   "id": "c068b1c86ed42dfe",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:39.267810Z",
     "start_time": "2025-07-23T14:28:38.310788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# Convert to DSPy Example objects\n",
    "dspy_examples = {}\n",
    "for section_name, section in dataset.items():\n",
    "    dspy_examples[section_name] = [\n",
    "        dspy.Example(\n",
    "            premise=ex['premise'],\n",
    "            hypothesis=ex['hypothesis'],\n",
    "            label=label_names[ex['gold_label']]\n",
    "        ).with_inputs(\"premise\", \"hypothesis\")\n",
    "        for ex in section\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame(dspy_examples)"
   ],
   "id": "33c73280e69e73b1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     presupposition_all_n_presupposition presupposition_both_presupposition  \\\n",
       "0           [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1           [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "2           [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "3           [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "4           [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "...                                  ...                                ...   \n",
       "1895        [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1896        [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1897        [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1898        [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1899        [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "\n",
       "     presupposition_change_of_state presupposition_cleft_existence  \\\n",
       "0      [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "1      [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "2      [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "3      [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "4      [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "...                             ...                            ...   \n",
       "1895   [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "1896   [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "1897   [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "1898   [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "1899   [premise, hypothesis, label]   [premise, hypothesis, label]   \n",
       "\n",
       "     presupposition_cleft_uniqueness presupposition_only_presupposition  \\\n",
       "0       [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1       [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "2       [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "3       [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "4       [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "...                              ...                                ...   \n",
       "1895    [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1896    [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1897    [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1898    [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "1899    [premise, hypothesis, label]       [premise, hypothesis, label]   \n",
       "\n",
       "     presupposition_possessed_definites_existence  \\\n",
       "0                    [premise, hypothesis, label]   \n",
       "1                    [premise, hypothesis, label]   \n",
       "2                    [premise, hypothesis, label]   \n",
       "3                    [premise, hypothesis, label]   \n",
       "4                    [premise, hypothesis, label]   \n",
       "...                                           ...   \n",
       "1895                 [premise, hypothesis, label]   \n",
       "1896                 [premise, hypothesis, label]   \n",
       "1897                 [premise, hypothesis, label]   \n",
       "1898                 [premise, hypothesis, label]   \n",
       "1899                 [premise, hypothesis, label]   \n",
       "\n",
       "     presupposition_possessed_definites_uniqueness  \\\n",
       "0                     [premise, hypothesis, label]   \n",
       "1                     [premise, hypothesis, label]   \n",
       "2                     [premise, hypothesis, label]   \n",
       "3                     [premise, hypothesis, label]   \n",
       "4                     [premise, hypothesis, label]   \n",
       "...                                            ...   \n",
       "1895                  [premise, hypothesis, label]   \n",
       "1896                  [premise, hypothesis, label]   \n",
       "1897                  [premise, hypothesis, label]   \n",
       "1898                  [premise, hypothesis, label]   \n",
       "1899                  [premise, hypothesis, label]   \n",
       "\n",
       "     presupposition_question_presupposition  \n",
       "0              [premise, hypothesis, label]  \n",
       "1              [premise, hypothesis, label]  \n",
       "2              [premise, hypothesis, label]  \n",
       "3              [premise, hypothesis, label]  \n",
       "4              [premise, hypothesis, label]  \n",
       "...                                     ...  \n",
       "1895           [premise, hypothesis, label]  \n",
       "1896           [premise, hypothesis, label]  \n",
       "1897           [premise, hypothesis, label]  \n",
       "1898           [premise, hypothesis, label]  \n",
       "1899           [premise, hypothesis, label]  \n",
       "\n",
       "[1900 rows x 9 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>presupposition_all_n_presupposition</th>\n",
       "      <th>presupposition_both_presupposition</th>\n",
       "      <th>presupposition_change_of_state</th>\n",
       "      <th>presupposition_cleft_existence</th>\n",
       "      <th>presupposition_cleft_uniqueness</th>\n",
       "      <th>presupposition_only_presupposition</th>\n",
       "      <th>presupposition_possessed_definites_existence</th>\n",
       "      <th>presupposition_possessed_definites_uniqueness</th>\n",
       "      <th>presupposition_question_presupposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "      <td>[premise, hypothesis, label]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1900 rows × 9 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "from evaluate import combine, load\n",
    "\n",
    "# 1. Run DSPy evaluation for each section (here, limited to first 10 for demo)\n",
    "results = {}  # Store per-section predictions\n",
    "not_predicted = {}\n",
    "for sec in dspy_examples:\n",
    "    print(f\"Evaluating section:\\t{sec}\")\n",
    "    evaluator = Evaluate(\n",
    "        devset=dspy_examples[sec],\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    "        # max_errors=30\n",
    "    )\n",
    "    eval_res = evaluator(predictor)\n",
    "    _, result_tuples = eval_res\n",
    "    print(f\"number of results:\\t{len(result_tuples)}\")\n",
    "    preds, refs = [], []\n",
    "    not_predicted[sec] = {\n",
    "        'section':sec,\n",
    "        'num_not_predicted':0,\n",
    "        'not_predicted':[]\n",
    "    }\n",
    "    for example, prediction, correct in result_tuples:\n",
    "        if not hasattr(prediction, \"label\"):\n",
    "            not_predicted[sec]['num_not_predicted']+=1\n",
    "            not_predicted[sec]['not_predicted'].append((example, prediction, correct))\n",
    "            continue\n",
    "        preds.append(prediction.label)\n",
    "        refs.append(example.label)\n",
    "    results[sec] = {\"preds\": preds, \"refs\": refs}"
   ],
   "id": "409c65566eacfa62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's display some statistics about the results",
   "id": "f4746c046bb1c6e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "for sec, data in results.items():\n",
    "    preds = data['preds']\n",
    "    refs = data['refs']\n",
    "    print(f\"Section: {sec}\")\n",
    "    print(f\"  Total predictions: {len(preds)}\")\n",
    "    print(f\"  Total references:  {len(refs)}\")\n",
    "    print(f\"  Class distribution in predictions: {Counter(preds)}\")\n",
    "    print(f\"  Class distribution in references:  {Counter(refs)}\")\n",
    "    agree = sum([p == r for p, r in zip(preds, refs)])\n",
    "    print(f\"  Number of matches (agreement): {agree}\")\n",
    "    print(f\"  Accuracy (quick): {agree / len(refs):.3f}\")\n",
    "    print()\n",
    "\n",
    "# Overall stats\n",
    "all_preds = sum([v['preds'] for v in results.values()], [])\n",
    "all_refs  = sum([v['refs']  for v in results.values()], [])\n",
    "print(\"=== OVERALL ===\")\n",
    "print(f\"Total predictions: {len(all_preds)}\")\n",
    "print(f\"Total references:  {len(all_refs)}\")\n",
    "print(f\"Class distribution in predictions: {Counter(all_preds)}\")\n",
    "print(f\"Class distribution in references:  {Counter(all_refs)}\")\n",
    "agree = sum([p == r for p, r in zip(all_preds, all_refs)])\n",
    "print(f\"Number of matches (agreement): {agree}\")\n",
    "print(f\"Accuracy (quick): {agree / len(all_refs):.3f}\")\n"
   ],
   "id": "5ca7b91f2dc29bf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now show information about non-predicted examples:",
   "id": "d7d2ca8f76c7a8e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_np = pd.DataFrame(list(not_predicted.values())).set_index(\"section\")\n",
    "exploded = df_np[\"not_predicted\"].explode()\n",
    "df_details = (\n",
    "    exploded\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"section\", \"not_predicted\": \"detail\"})\n",
    "    .join(pd.json_normalize(exploded).add_prefix(\"detail.\"))\n",
    ")\n",
    "display(df_details)\n",
    "for sec, info in not_predicted.items():\n",
    "    print(f\"=== Section: {sec} — {info['num_not_predicted']} failures ===\")\n",
    "    for ex, raw_out, score in info['not_predicted']:\n",
    "        print(ex)\n",
    "        premise, hypothesis, ref,= ex\n",
    "        print(f\"🎯 Ref label: {ex[ref]}\")\n",
    "        print(f\"💬 Premise: {ex[premise]}\")\n",
    "        print(f\"💬 Hypothesis: {ex[hypothesis]}\")\n",
    "        print(f\"🛑 Raw output: {raw_out!r}\")\n",
    "        print(f\"⚠️ Score: {score}\")\n",
    "        print(\"-\" * 40)"
   ],
   "id": "b7955257ecf8e406",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Prepare for metric calculation\n",
    "metric_prf = combine([\"precision\", \"recall\", \"f1\"])\n",
    "acc = load(\"accuracy\")\n",
    "rows = []\n",
    "all_preds, all_refs = [], []\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "for sec, data in results.items():\n",
    "    print(f\"Computing metrics for section: {sec}\")\n",
    "    preds = [label2id[label] for label in data[\"preds\"]]\n",
    "    refs  = [label2id[label] for label in data[\"refs\"]]\n",
    "    prf = metric_prf.compute(predictions=preds, references=refs, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "\n",
    "    rows.append({\"section\": sec, \"accuracy\": accuracy, **prf})\n",
    "    all_preds += preds\n",
    "    all_refs += refs\n",
    "\n",
    "# 3. Compute overall metrics\n",
    "overall_prf = metric_prf.compute(predictions=all_preds, references=all_refs, average=\"weighted\")\n",
    "overall_acc = acc.compute(predictions=all_preds, references=all_refs)[\"accuracy\"]\n",
    "rows.append({\"section\": \"all\", \"accuracy\": overall_acc, **overall_prf})\n",
    "\n",
    "# Create DataFrame and display\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "display(df_metrics.set_index(\"section\"))"
   ],
   "id": "20f5b67929c66534",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In our experiment we got the following results:\n",
    "| section                                       | accuracy | precision | recall  | f1       |\n",
    "|----------------------------------------------|----------|-----------|---------|----------|\n",
    "| presupposition_all_n_presupposition          | 0.942632 | 0.949257  | 0.942632| 0.942783 |\n",
    "| presupposition_both_presupposition           | 0.973158 | 0.974034  | 0.973158| 0.973184 |\n",
    "| presupposition_change_of_state               | 0.557895 | 0.655905  | 0.557895| 0.493381 |\n",
    "| presupposition_cleft_existence               | 0.686316 | 0.812531  | 0.686316| 0.669707 |\n",
    "| presupposition_cleft_uniqueness              | 0.474211 | 0.503028  | 0.474211| 0.350207 |\n",
    "| presupposition_only_presupposition           | 0.668947 | 0.778061  | 0.668947| 0.654415 |\n",
    "| presupposition_possessed_definites_existence | 0.923158 | 0.929153  | 0.923158| 0.923322 |\n",
    "| presupposition_possessed_definites_uniqueness| 0.475263 | 0.626211  | 0.475263| 0.352235 |\n",
    "| presupposition_question_presupposition       | 0.841053 | 0.863356  | 0.841053| 0.838288 |\n",
    "| all                                          | 0.726959 | 0.815532  | 0.726959| 0.717863 |\n",
    "\n",
    "With a total F1 score of 0.726959 with grok-3-mini. Let's try to optimize the model\n"
   ],
   "id": "e50fa69903b6adb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optimizing the model\n",
    "We are going to try optimize the model in a couple ways.\n",
    "we will first create a dev\\test split:"
   ],
   "id": "fe8992aa33876994"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:46:32.996614Z",
     "start_time": "2025-07-23T14:46:27.683135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# acc_metric   = evaluate.load(\"accuracy\")\n",
    "# f1_metric    = evaluate.load(\"f1\")\n",
    "# prec_metric  = evaluate.load(\"precision\")\n",
    "# rec_metric   = evaluate.load(\"recall\")"
   ],
   "id": "faa4ccfc4da7e840",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d5b76bd1f8142e59f8634582dd15fe3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:29:16.295709Z",
     "start_time": "2025-07-23T14:29:16.278662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from numpy import random\n",
    "rng = random.default_rng(42)\n",
    "\n",
    "def stratified_split(df, frac=0.7):\n",
    "    idx_dev = []\n",
    "    for (sec, lab), g in df.groupby([\"section\",\"gold_label\"]):\n",
    "        n = int(len(g)*frac)\n",
    "        idx = rng.permutation(g.index)\n",
    "        idx_dev.extend(idx[:n])\n",
    "    dev = df.loc[idx_dev]\n",
    "    test = df.drop(idx_dev)\n",
    "    return dev, test\n",
    "\n",
    "dev_df, test_df = stratified_split(combined_df)"
   ],
   "id": "ed61f1096174010d",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:29:16.344133Z",
     "start_time": "2025-07-23T14:29:16.335523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "display(pd.DataFrame(dev_df))\n",
    "display(pd.DataFrame(test_df))"
   ],
   "id": "777c40424bea1b4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 premise  \\\n",
       "1795   All eight women that compel libraries to appre...   \n",
       "500    Have the six guests that had badgered a lot of...   \n",
       "1792   Do all eight women that compel libraries to ap...   \n",
       "174    The ten women that talk didn't implore Pamela ...   \n",
       "152    All four waiters that were boring Paul have te...   \n",
       "...                                                  ...   \n",
       "15956  Patricia wasn't concealing when Renee commande...   \n",
       "15324  Museums might know when Donald neglects to hid...   \n",
       "15961      Sonia does forget when those peppers blacken.   \n",
       "16328        Does Jill conceal why the girls do scratch?   \n",
       "16920  Marla might find out why these waitresses have...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "1795   There are exactly eight women that compel libr...           modal   \n",
       "500    There are exactly six guests that had badgered...   interrogative   \n",
       "1792   There are exactly eight women that compel libr...   interrogative   \n",
       "174               There are exactly ten women that talk.         negated   \n",
       "152    There are exactly four waiters that were borin...      unembedded   \n",
       "...                                                  ...             ...   \n",
       "15956  Patricia is concealing when Renee commanded th...  Not_In_Example   \n",
       "15324   Donald doesn't neglect to hide that wheelbarrow.           modal   \n",
       "15961                       Those peppers don't blacken.      unembedded   \n",
       "16328                           The girls don't scratch.   interrogative   \n",
       "16920               These waitresses haven't retaliated.           modal   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "1795   Not_In_Example  Not_In_Example        positive           0   \n",
       "500    Not_In_Example  Not_In_Example        positive           0   \n",
       "1792   Not_In_Example  Not_In_Example        positive           0   \n",
       "174    Not_In_Example  Not_In_Example        positive           0   \n",
       "152    Not_In_Example  Not_In_Example        positive           0   \n",
       "...               ...             ...             ...         ...   \n",
       "15956         negated      unembedded  Not_In_Example           2   \n",
       "15324  Not_In_Example  Not_In_Example         negated           2   \n",
       "15961  Not_In_Example  Not_In_Example         negated           2   \n",
       "16328  Not_In_Example  Not_In_Example         negated           2   \n",
       "16920  Not_In_Example  Not_In_Example         negated           2   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "1795      all_n_presupposition  1795e          94   \n",
       "500       all_n_presupposition   500e          26   \n",
       "1792      all_n_presupposition  1792e          94   \n",
       "174       all_n_presupposition   174e           9   \n",
       "152       all_n_presupposition   152e           8   \n",
       "...                        ...    ...         ...   \n",
       "15956  question_presupposition   756c          39   \n",
       "15324  question_presupposition   124c           6   \n",
       "15961  question_presupposition   761c          40   \n",
       "16328  question_presupposition  1128c          59   \n",
       "16920  question_presupposition  1720c          90   \n",
       "\n",
       "                                      section  \n",
       "1795      presupposition_all_n_presupposition  \n",
       "500       presupposition_all_n_presupposition  \n",
       "1792      presupposition_all_n_presupposition  \n",
       "174       presupposition_all_n_presupposition  \n",
       "152       presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "15956  presupposition_question_presupposition  \n",
       "15324  presupposition_question_presupposition  \n",
       "15961  presupposition_question_presupposition  \n",
       "16328  presupposition_question_presupposition  \n",
       "16920  presupposition_question_presupposition  \n",
       "\n",
       "[11970 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>All eight women that compel libraries to appre...</td>\n",
       "      <td>There are exactly eight women that compel libr...</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1795e</td>\n",
       "      <td>94</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Have the six guests that had badgered a lot of...</td>\n",
       "      <td>There are exactly six guests that had badgered...</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>500e</td>\n",
       "      <td>26</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>Do all eight women that compel libraries to ap...</td>\n",
       "      <td>There are exactly eight women that compel libr...</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1792e</td>\n",
       "      <td>94</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>The ten women that talk didn't implore Pamela ...</td>\n",
       "      <td>There are exactly ten women that talk.</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>174e</td>\n",
       "      <td>9</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>All four waiters that were boring Paul have te...</td>\n",
       "      <td>There are exactly four waiters that were borin...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>152e</td>\n",
       "      <td>8</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15956</th>\n",
       "      <td>Patricia wasn't concealing when Renee commande...</td>\n",
       "      <td>Patricia is concealing when Renee commanded th...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>756c</td>\n",
       "      <td>39</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15324</th>\n",
       "      <td>Museums might know when Donald neglects to hid...</td>\n",
       "      <td>Donald doesn't neglect to hide that wheelbarrow.</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>124c</td>\n",
       "      <td>6</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15961</th>\n",
       "      <td>Sonia does forget when those peppers blacken.</td>\n",
       "      <td>Those peppers don't blacken.</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>761c</td>\n",
       "      <td>40</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16328</th>\n",
       "      <td>Does Jill conceal why the girls do scratch?</td>\n",
       "      <td>The girls don't scratch.</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1128c</td>\n",
       "      <td>59</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16920</th>\n",
       "      <td>Marla might find out why these waitresses have...</td>\n",
       "      <td>These waitresses haven't retaliated.</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1720c</td>\n",
       "      <td>90</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11970 rows × 11 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "                                                 premise  \\\n",
       "3      All ten guys that proved to boast weren't divo...   \n",
       "4      All ten guys that proved to boast weren't divo...   \n",
       "10     All ten guys that proved to boast might have b...   \n",
       "14     If all ten guys that proved to boast were divo...   \n",
       "19     All ten reports that can bore some waiter are ...   \n",
       "...                                                  ...   \n",
       "17087  Did the actors conceal where that mall shocks ...   \n",
       "17090  The actors might have concealed where that mal...   \n",
       "17092  The actors might have concealed where that mal...   \n",
       "17093  If the actors do conceal where that mall shock...   \n",
       "17096  The actors didn't conceal where that mall shoc...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "3       There are exactly ten guys that proved to boast.         negated   \n",
       "4      There are exactly eleven guys that proved to b...         negated   \n",
       "10     There are exactly eleven guys that proved to b...           modal   \n",
       "14     There are exactly ten senators that proved to ...     conditional   \n",
       "19     There are exactly ten reports that can bore so...      unembedded   \n",
       "...                                                  ...             ...   \n",
       "17087                             That mall shocks Janet   interrogative   \n",
       "17090                             That mall shocks Janet           modal   \n",
       "17092                               Travel shocks Janet.           modal   \n",
       "17093                             That mall shocks Janet     conditional   \n",
       "17096  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "3      Not_In_Example  Not_In_Example        positive           0   \n",
       "4      Not_In_Example  Not_In_Example         negated           2   \n",
       "10     Not_In_Example  Not_In_Example         negated           2   \n",
       "14     Not_In_Example  Not_In_Example         neutral           1   \n",
       "19     Not_In_Example  Not_In_Example        positive           0   \n",
       "...               ...             ...             ...         ...   \n",
       "17087  Not_In_Example  Not_In_Example        positive           0   \n",
       "17090  Not_In_Example  Not_In_Example        positive           0   \n",
       "17092  Not_In_Example  Not_In_Example         neutral           1   \n",
       "17093  Not_In_Example  Not_In_Example        positive           0   \n",
       "17096         negated      unembedded  Not_In_Example           2   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "3         all_n_presupposition     3e           0   \n",
       "4         all_n_presupposition     4c           0   \n",
       "10        all_n_presupposition    10c           0   \n",
       "14        all_n_presupposition    14n           0   \n",
       "19        all_n_presupposition    19e           1   \n",
       "...                        ...    ...         ...   \n",
       "17087  question_presupposition  1887e          99   \n",
       "17090  question_presupposition  1890e          99   \n",
       "17092  question_presupposition  1892n          99   \n",
       "17093  question_presupposition  1893e          99   \n",
       "17096  question_presupposition  1896c          99   \n",
       "\n",
       "                                      section  \n",
       "3         presupposition_all_n_presupposition  \n",
       "4         presupposition_all_n_presupposition  \n",
       "10        presupposition_all_n_presupposition  \n",
       "14        presupposition_all_n_presupposition  \n",
       "19        presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "17087  presupposition_question_presupposition  \n",
       "17090  presupposition_question_presupposition  \n",
       "17092  presupposition_question_presupposition  \n",
       "17093  presupposition_question_presupposition  \n",
       "17096  presupposition_question_presupposition  \n",
       "\n",
       "[5130 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>3e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>4c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>All ten guys that proved to boast might have b...</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>10c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>If all ten guys that proved to boast were divo...</td>\n",
       "      <td>There are exactly ten senators that proved to ...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>14n</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>All ten reports that can bore some waiter are ...</td>\n",
       "      <td>There are exactly ten reports that can bore so...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>19e</td>\n",
       "      <td>1</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17087</th>\n",
       "      <td>Did the actors conceal where that mall shocks ...</td>\n",
       "      <td>That mall shocks Janet</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1887e</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17090</th>\n",
       "      <td>The actors might have concealed where that mal...</td>\n",
       "      <td>That mall shocks Janet</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1890e</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17092</th>\n",
       "      <td>The actors might have concealed where that mal...</td>\n",
       "      <td>Travel shocks Janet.</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1892n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17093</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>That mall shocks Janet</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1893e</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17096</th>\n",
       "      <td>The actors didn't conceal where that mall shoc...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1896c</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5130 rows × 11 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:29:16.470165Z",
     "start_time": "2025-07-23T14:29:16.395771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_examples(df):\n",
    "    return [dspy.Example(\n",
    "        premise=r.premise, hypothesis=r.hypothesis,\n",
    "        label=label_names[r.gold_label]\n",
    "    ).with_inputs(\"premise\",\"hypothesis\") for r in df.itertuples()]\n",
    "dev_ex  = to_examples(dev_df)\n",
    "test_ex = to_examples(test_df)"
   ],
   "id": "72daff261fe243f7",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(model):\n",
    "    results = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    "    )(model)\n",
    "    score,results = results\n",
    "    print(f\"Score:\\t{score}\")\n",
    "    test_pred = [label2id[out[1].label] for out in results]\n",
    "    return score, results, test_pred\n",
    "def compute_matrices(test_pred):\n",
    "    prf = metric_prf.compute(predictions=test_pred, references=y_true, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=test_pred, references=y_true)\n",
    "    return {**prf, **accuracy}"
   ],
   "id": "77aea4441ccc9fc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:46:48.019954Z",
     "start_time": "2025-07-23T14:46:43.589493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictor_test_predictions = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    ")(predictor)"
   ],
   "id": "aec84683f7362d93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2278.00 / 2977 (76.5%):  58%|█████▊    | 2976/5130 [00:02<00:00, 2598.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:08.073192Z",
     "start_time": "2025-07-23T15:15:08.064900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "score,predictor_test_predictions_results = predictor_test_predictions\n",
    "print(f\"Score: {score}\")\n",
    "predictor_test_pred = [label2id[out[1].label] for out in predictor_test_predictions_results]\n",
    "y_true = [label2id[ex.label]  for ex in test_ex]"
   ],
   "id": "37dfea0647d46d01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 72.48\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:14.557289Z",
     "start_time": "2025-07-23T15:15:10.494869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictor_prf = metric_prf.compute(predictions=predictor_test_pred, references=y_true, average=\"weighted\")\n",
    "predictor_accuracy = acc.compute(predictions=predictor_test_pred, references=y_true)\n",
    "predictor_combined = {**predictor_prf, **predictor_accuracy}"
   ],
   "id": "24acf6b7d33c6e07",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:16.965478Z",
     "start_time": "2025-07-23T15:15:16.961608Z"
    }
   },
   "cell_type": "code",
   "source": "display(pd.DataFrame(predictor_combined, index=[\"Original predictor\"]))",
   "id": "c81e5b6b99f149eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    precision    recall        f1  accuracy\n",
       "Original predictor   0.813633  0.724756  0.715039  0.724756"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original predictor</th>\n",
       "      <td>0.813633</td>\n",
       "      <td>0.724756</td>\n",
       "      <td>0.715039</td>\n",
       "      <td>0.724756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In our code we saw a F1 score of 0.715039 on the test.",
   "id": "367bc4a21287ce75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simply few-shot strategy over the entire dataset",
   "id": "77973717f459d0b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:31:37.521429Z",
     "start_time": "2025-07-23T14:29:16.499344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "bs = BootstrapFewShot(metric=accuracy_metric, max_bootstrapped_demos=20, max_labeled_demos=16)\n",
    "optimized = bs.compile(student=predictor, trainset=dev_ex)"
   ],
   "id": "4b1318dbc67f041c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/11970 [02:15<19:36:13,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 20 full traces after 23 examples for up to 1 rounds, amounting to 23 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:32:20.710485Z",
     "start_time": "2025-07-23T14:32:20.708514Z"
    }
   },
   "cell_type": "code",
   "source": "overall_optimized = optimized",
   "id": "417b2502acac71af",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:46:20.633031Z",
     "start_time": "2025-07-23T14:35:48.167812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Evaluate\n",
    "overall_report = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    ")(overall_optimized)"
   ],
   "id": "4b6030edd5815279",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3914.00 / 5130 (76.3%): 100%|██████████| 5130/5130 [10:32<00:00,  8.12it/s]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 17:46:20 INFO dspy.evaluate.evaluate: Average Metric: 3914 / 5130 (76.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:41.683635Z",
     "start_time": "2025-07-23T15:15:41.676403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2) Extract labels\n",
    "overall_score,overall_report_results = overall_report\n",
    "print(f\"Score: {overall_score}\")\n",
    "overall_test_pred = [label2id[out[1].label] for out in overall_report_results]"
   ],
   "id": "ff158e4ed4f19e69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 76.3\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:46.827090Z",
     "start_time": "2025-07-23T15:15:42.763710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "overall_prf = metric_prf.compute(predictions=overall_test_pred, references=y_true, average=\"weighted\")\n",
    "overall_accuracy = acc.compute(predictions=overall_test_pred, references=y_true)\n",
    "overall_combined = {**overall_prf, **overall_accuracy}"
   ],
   "id": "b49a734bbc008a1a",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:15:46.892950Z",
     "start_time": "2025-07-23T15:15:46.889042Z"
    }
   },
   "cell_type": "code",
   "source": "display(pd.DataFrame(overall_combined, index=[\"Overall_optimized predictor\"]))",
   "id": "b7499fb9b43a28e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             precision    recall        f1  accuracy\n",
       "Overall_optimized predictor   0.826545  0.762963  0.758567  0.762963"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall_optimized predictor</th>\n",
       "      <td>0.826545</td>\n",
       "      <td>0.762963</td>\n",
       "      <td>0.758567</td>\n",
       "      <td>0.762963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When testing the overall model, we saw F1 Score of 0.758567, an improvement of 6.087%!",
   "id": "de3196a2632c1f64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Adaptive few-shot strategy\n",
    "We will now try to optimize for each section and create a new model which will predicate by majority vote."
   ],
   "id": "ed03933fa38856e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:29:59.714330Z",
     "start_time": "2025-07-23T15:29:59.657766Z"
    }
   },
   "cell_type": "code",
   "source": "sec_dev_ex = {sec: to_examples(group) for sec, group in dev_df.groupby(\"section\")}",
   "id": "df6539e3ac2fee22",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T16:05:04.005048Z",
     "start_time": "2025-07-23T15:30:02.849931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimized_pipelines = {}\n",
    "\n",
    "for sec in sec_dev_ex:\n",
    "    print(f\"Optimizing for section: {sec}\")\n",
    "    # Flatten dev examples for prompt tuning\n",
    "    dev_set = sec_dev_ex[sec]\n",
    "\n",
    "    # Initialize optimizer\n",
    "    bs = BootstrapFewShot(\n",
    "        metric=accuracy_metric,\n",
    "        max_bootstrapped_demos=8,\n",
    "        max_labeled_demos=4\n",
    "    )\n",
    "\n",
    "    # Compile and tune using dev split\n",
    "    compiled = bs.compile(\n",
    "        student=predictor,\n",
    "        trainset=dev_set\n",
    "    )\n",
    "    optimized_pipelines[sec] = compiled"
   ],
   "id": "f1bfbe1173b8e658",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for section: presupposition_all_n_presupposition\n",
      "Average Metric: 94.00 / 94 (100.0%):   1%|          | 94/11970 [54:37<115:01:59, 34.87s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1330 [00:59<2:43:43,  7.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n",
      "✅ Completed Bootstrapped few-shot for section `presupposition_all_n_presupposition`\n",
      "Optimizing for section: presupposition_both_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1330 [00:53<2:26:49,  6.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n",
      "✅ Completed Bootstrapped few-shot for section `presupposition_both_presupposition`\n",
      "Optimizing for section: presupposition_change_of_state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 27/1330 [02:21<1:54:10,  5.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 27 examples for up to 1 rounds, amounting to 27 attempts.\n",
      "✅ Completed Bootstrapped few-shot for section `presupposition_change_of_state`\n",
      "Optimizing for section: presupposition_cleft_existence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1330 [02:02<2:07:14,  5.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 21 examples for up to 1 rounds, amounting to 21 attempts.\n",
      "✅ Completed Bootstrapped few-shot for section `presupposition_cleft_existence`\n",
      "Optimizing for section: presupposition_cleft_uniqueness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 36/1330 [04:14<2:32:31,  7.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 36 examples for up to 1 rounds, amounting to 36 attempts.\n",
      "✅ Completed Bootstrapped few-shot for section `presupposition_cleft_uniqueness`\n",
      "Optimizing for section: presupposition_only_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/1330 [01:46<1:56:17,  5.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 20 examples for up to 1 rounds, amounting to 20 attempts.\n",
      "✅ Completed Bootstrapped few-shot for section `presupposition_only_presupposition`\n",
      "Optimizing for section: presupposition_possessed_definites_existence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1330 [00:37<1:42:59,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n",
      "✅ Completed Bootstrapped few-shot for section `presupposition_possessed_definites_existence`\n",
      "Optimizing for section: presupposition_possessed_definites_uniqueness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 183/1330 [19:21<2:01:20,  6.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 183 examples for up to 1 rounds, amounting to 183 attempts.\n",
      "✅ Completed Bootstrapped few-shot for section `presupposition_possessed_definites_uniqueness`\n",
      "Optimizing for section: presupposition_question_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1330 [01:23<2:10:47,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 14 examples for up to 1 rounds, amounting to 14 attempts.\n",
      "✅ Completed Bootstrapped few-shot for section `presupposition_question_presupposition`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T16:05:04.051873Z",
     "start_time": "2025-07-23T16:05:04.050005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# existing section pipelines\n",
    "ensemble_tp = dspy.Ensemble(reduce_fn=dspy.majority)\n",
    "adaptive_optimized = ensemble_tp.compile(list(optimized_pipelines.values()))\n",
    "#\n",
    "# # 3. Now save the compiled predictor to disk\n",
    "# fname = f\"joint_predictor_state_{datetime.now().timestamp():.0f}.pkl\"\n",
    "# joint_predictor.save(fname, save_program=False)\n",
    "# print(f\"✅ Saved joint predictor to {fname}\")"
   ],
   "id": "ae6e76930f5c4558",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T17:42:31.542203Z",
     "start_time": "2025-07-23T16:05:04.112934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "adaptive_report = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    ")(adaptive_optimized)"
   ],
   "id": "6b0d08aed1459bb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3821.00 / 5130 (74.5%): 100%|██████████| 5130/5130 [1:37:27<00:00,  1.14s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 20:42:31 INFO dspy.evaluate.evaluate: Average Metric: 3821 / 5130 (74.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T17:44:38.531365Z",
     "start_time": "2025-07-23T17:44:38.474610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "adaptive_score,adaptive_report_results = adaptive_report\n",
    "print(f\"Score: {adaptive_score}\")\n",
    "adaptive_test_pred = [label2id[out[1].label] for out in adaptive_report_results]"
   ],
   "id": "e0ca3387c359351f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 74.48\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T17:44:43.145493Z",
     "start_time": "2025-07-23T17:44:40.076006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "adaptive_prf = metric_prf.compute(predictions=adaptive_test_pred, references=y_true, average=\"weighted\")\n",
    "adaptive_accuracy = acc.compute(predictions=adaptive_test_pred, references=y_true)\n",
    "adaptive_combined = {**adaptive_prf, **adaptive_accuracy}"
   ],
   "id": "383d148a01fdfb84",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T17:44:43.177543Z",
     "start_time": "2025-07-23T17:44:43.173971Z"
    }
   },
   "cell_type": "code",
   "source": "display(pd.DataFrame(adaptive_combined, index=[\"Adaptive_optimized predictor\"]))",
   "id": "98550da084f9e8c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                              precision    recall        f1  accuracy\n",
       "Adaptive_optimized predictor   0.821135  0.744834  0.738229  0.744834"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adaptive_optimized predictor</th>\n",
       "      <td>0.821135</td>\n",
       "      <td>0.744834</td>\n",
       "      <td>0.738229</td>\n",
       "      <td>0.744834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We got:\n",
    "0.821135,0.744834,0.738229,0.744834\n",
    "this shows around 0.02 improvement."
   ],
   "id": "d3a03cf39db51171"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1e408e95c79e5bb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T19:05:13.620678Z",
     "start_time": "2025-07-23T17:52:44.906794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "bsrs = BootstrapFewShotWithRandomSearch(\n",
    "    metric=accuracy_metric,\n",
    "    max_bootstrapped_demos=20,\n",
    "    max_labeled_demos=9,\n",
    "    num_candidate_programs=5,\n",
    "    num_threads=50\n",
    ")\n",
    "opted_rs = bsrs.compile(student=predictor, trainset=dev_ex, valset=dev_ex)"
   ],
   "id": "f8288e547ee09a9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 20 traces per predictor.\n",
      "Will attempt to bootstrap 5 candidate sets.\n",
      "Average Metric: 2500.00 / 2985 (83.8%):  25%|██▍       | 2984/11970 [00:05<00:04, 1873.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7203.00 / 9896 (72.8%):  83%|████████▎ | 9895/11970 [00:08<00:01, 1622.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 242.00 / 255 (94.9%):   2%|▏         | 254/11970 [00:53<3:04:36,  1.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9258.00 / 11970 (77.3%): 100%|██████████| 11970/11970 [24:56<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 21:17:57 INFO dspy.evaluate.evaluate: Average Metric: 9258 / 11970 (77.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 77.34 for seed -2\n",
      "Scores so far: [72.81, 77.34]\n",
      "Best score so far: 77.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/11970 [00:00<01:55, 103.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 20 full traces after 20 examples for up to 1 rounds, amounting to 20 attempts.\n",
      "Average Metric: 9169.00 / 11970 (76.6%): 100%|██████████| 11970/11970 [24:24<00:00,  8.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 21:42:56 INFO dspy.evaluate.evaluate: Average Metric: 9169 / 11970 (76.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [72.81, 77.34, 76.6]\n",
      "Best score so far: 77.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/11970 [01:11<13:15:31,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 13 full traces after 18 examples for up to 1 rounds, amounting to 18 attempts.\n",
      "Average Metric: 7164.00 / 9521 (75.2%):  80%|███████▉  | 9520/11970 [19:38<1:32:35,  2.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  84%|████████▍ | 10047/11970 [20:19<02:00, 15.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:11 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If Irene's print that hadn't bothered Cindy does fall, it's okay.\", 'hypothesis': \"Irene's print that hadn't bothered Cindy does fall.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 495/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  84%|████████▍ | 10049/11970 [20:36<1:09:21,  2.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:11 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Did Mitchell's mountain that is hurting Marcus disgust Timothy?\", 'hypothesis': \"Mitchell's mountain that is hurting Marcus did disgust Timothy.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 495/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n",
      "2025/07/23 22:05:11 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If Theresa's river that hasn't hurt Adam did freeze, it's okay.\", 'hypothesis': \"Waitresses has exactly one river that hasn't hurt Adam.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 495/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  84%|████████▍ | 10050/11970 [20:36<59:44,  1.87s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:11 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Homer's snake that embarrassed some legislature did resemble the paintings.\", 'hypothesis': 'Joel has exactly one snake that embarrassed some legislature.', 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 495/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  84%|████████▍ | 10052/11970 [20:36<42:36,  1.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:11 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Does Wendy's cup that hasn't vanished disturb Nicole?\", 'hypothesis': \"Wendy's cup that hasn't vanished does disturb Nicole.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 495/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  84%|████████▍ | 10052/11970 [20:36<42:36,  1.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Sally's car that annoys Vincent might bore Tina.\", 'hypothesis': \"Sally's car that annoys Vincent does bore Tina.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 493/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  84%|████████▍ | 10054/11970 [20:36<32:53,  1.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Did Becca's horse that existed break the plane?\", 'hypothesis': \"Becca's horse that existed did break the plane.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 493/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  84%|████████▍ | 10054/11970 [20:36<32:53,  1.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Karla's snake that had upset Vanessa did bore Veronica.\", 'hypothesis': 'The customer has exactly one snake that had upset Vanessa.', 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 493/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  84%|████████▍ | 10055/11970 [20:37<32:52,  1.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Did Theresa's river that hasn't hurt Adam freeze?\", 'hypothesis': \"Waitresses has exactly one river that hasn't hurt Adam.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 494/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  84%|████████▍ | 10057/11970 [20:37<20:49,  1.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Does those dancers' closet that aggravates many women resemble those photographs?\", 'hypothesis': \"Those dancers' closet that aggravates many women does resemble those photographs.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 494/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  84%|████████▍ | 10057/11970 [20:37<20:49,  1.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Brian's popsicle that couldn't stun Karla might fall.\", 'hypothesis': \"Brian's popsicle that couldn't stun Karla does fall.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 493/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  84%|████████▍ | 10059/11970 [20:37<20:47,  1.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Does Elizabeth's couch that wasn't stunning Rhonda slip?\", 'hypothesis': \"Elizabeth's couch that wasn't stunning Rhonda does slip.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 494/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  85%|████████▌ | 10231/11970 [20:37<00:47, 36.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If Becca's horse that existed did break the plane, it's okay.\", 'hypothesis': \"Becca's horse that existed did break the plane.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 493/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  85%|████████▌ | 10233/11970 [20:37<00:47, 36.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Gerald's banana that worries Pamela doesn't blacken.\", 'hypothesis': 'Those women has exactly one banana that worries Pamela.', 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 493/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  86%|████████▋ | 10327/11970 [20:37<00:21, 76.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Did those waitresses' shoe that hadn't annoyed the schools disappear?\", 'hypothesis': \"Roger have exactly one shoe that hadn't annoyed the schools.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 491/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  89%|████████▉ | 10671/11970 [20:37<00:05, 225.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"John's chair that aggravates Victoria didn't aggravate the school.\", 'hypothesis': 'Kevin has exactly one chair that aggravates Victoria.', 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 489/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  89%|████████▉ | 10685/11970 [20:37<00:05, 225.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If David's box that wasn't disgusting Marie did look like those prints, it's okay.\", 'hypothesis': \"Sara has exactly one box that wasn't disgusting Marie.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 490/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  90%|█████████ | 10791/11970 [20:37<00:03, 310.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:12 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Donald's door that disgusts Sonia doesn't waste away.\", 'hypothesis': 'Katherine has exactly one door that disgusts Sonia.', 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 490/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  93%|█████████▎| 11131/11970 [20:37<00:01, 723.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:13 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Nina's commentary that embarrasses Sherry might have bored these senators.\", 'hypothesis': 'Sandra has exactly one commentary that embarrasses Sherry.', 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 486/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  93%|█████████▎| 11142/11970 [20:37<00:01, 723.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:13 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Gerald's banana that worries Pamela might blacken.\", 'hypothesis': \"Gerald's banana that worries Pamela does blacken.\", 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 486/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  93%|█████████▎| 11147/11970 [20:37<00:01, 723.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:13 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Sally's hat that has irritated Derek might have shrunk.\", 'hypothesis': 'Winston Churchill has exactly one hat that has irritated Derek.', 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 487/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  94%|█████████▍| 11228/11970 [20:38<00:01, 723.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:13 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"Connie's commentary that irritated Deborah does worry James.\", 'hypothesis': 'Todd has exactly one commentary that irritated Deborah.', 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 486/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  94%|█████████▍| 11299/11970 [20:38<00:00, 816.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:13 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If Victoria's muffin that was impressing Mark did hurt Brian, it's okay.\", 'hypothesis': 'Karla has exactly one muffin that was impressing Mark.', 'label': 'neutral'}) (input_keys={'hypothesis', 'premise'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is — Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 486/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7555.00 / 10048 (75.2%):  96%|█████████▋| 11549/11970 [20:38<00:45,  9.33it/s]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 22:05:13 WARNING dspy.utils.parallelizer: Execution cancelled due to errors or interruption.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Execution cancelled due to errors or interruption.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mException\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[120]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdspy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mteleprompt\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BootstrapFewShotWithRandomSearch\n\u001B[32m      3\u001B[39m bsrs = BootstrapFewShotWithRandomSearch(\n\u001B[32m      4\u001B[39m     metric=accuracy_metric,\n\u001B[32m      5\u001B[39m     max_bootstrapped_demos=\u001B[32m20\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m      8\u001B[39m     num_threads=\u001B[32m50\u001B[39m\n\u001B[32m      9\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m opted_rs = \u001B[43mbsrs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudent\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpredictor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdev_ex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdev_ex\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/teleprompt/random_search.py:119\u001B[39m, in \u001B[36mBootstrapFewShotWithRandomSearch.compile\u001B[39m\u001B[34m(self, student, teacher, trainset, valset, restrict, labeled_sample)\u001B[39m\n\u001B[32m    108\u001B[39m     program = optimizer.compile(student, teacher=teacher, trainset=trainset_copy)\n\u001B[32m    110\u001B[39m evaluate = Evaluate(\n\u001B[32m    111\u001B[39m     devset=\u001B[38;5;28mself\u001B[39m.valset,\n\u001B[32m    112\u001B[39m     metric=\u001B[38;5;28mself\u001B[39m.metric,\n\u001B[32m   (...)\u001B[39m\u001B[32m    116\u001B[39m     display_progress=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    117\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m119\u001B[39m score, subscores = \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprogram\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_all_scores\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m all_subscores.append(subscores)\n\u001B[32m    123\u001B[39m \u001B[38;5;66;03m############ Assertion-aware Optimization ############\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001B[39m, in \u001B[36mwith_callbacks.<locals>.sync_wrapper\u001B[39m\u001B[34m(instance, *args, **kwargs)\u001B[39m\n\u001B[32m    324\u001B[39m callbacks = _get_active_callbacks(instance)\n\u001B[32m    325\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m callbacks:\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    328\u001B[39m call_id = uuid.uuid4().hex\n\u001B[32m    330\u001B[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:171\u001B[39m, in \u001B[36mEvaluate.__call__\u001B[39m\u001B[34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs, callback_metadata)\u001B[39m\n\u001B[32m    167\u001B[39m         program._suggest_failures += dspy.settings.get(\u001B[33m\"\u001B[39m\u001B[33msuggest_failures\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    169\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m prediction, score\n\u001B[32m--> \u001B[39m\u001B[32m171\u001B[39m results = \u001B[43mexecutor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_item\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    172\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(devset) == \u001B[38;5;28mlen\u001B[39m(results)\n\u001B[32m    174\u001B[39m results = [((dspy.Prediction(), \u001B[38;5;28mself\u001B[39m.failure_score) \u001B[38;5;28;01mif\u001B[39;00m r \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m r) \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m results]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/parallelizer.py:48\u001B[39m, in \u001B[36mParallelExecutor.execute\u001B[39m\u001B[34m(self, function, data)\u001B[39m\n\u001B[32m     46\u001B[39m tqdm.tqdm._instances.clear()\n\u001B[32m     47\u001B[39m wrapped = \u001B[38;5;28mself\u001B[39m._wrap_function(function)\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_execute_parallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrapped\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/parallelizer.py:203\u001B[39m, in \u001B[36mParallelExecutor._execute_parallel\u001B[39m\u001B[34m(self, function, data)\u001B[39m\n\u001B[32m    201\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cancel_jobs.is_set():\n\u001B[32m    202\u001B[39m     logger.warning(\u001B[33m\"\u001B[39m\u001B[33mExecution cancelled due to errors or interruption.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m203\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mExecution cancelled due to errors or interruption.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    205\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "\u001B[31mException\u001B[39m: Execution cancelled due to errors or interruption."
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1c7a46f92ab6043c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "mipro = MIPROv2(metric=accuracy_metric, auto=\"light\")\n",
    "opted_mipro = mipro.compile(predictor, trainset=dev_ex)"
   ],
   "id": "8dffca042122d056"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mipro_report = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    ")(opted_mipro)"
   ],
   "id": "488952d6e2e2a5c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mipro_score,mipro_report_results = mipro_report\n",
    "print(f\"Score: {mipro_score}\")\n",
    "mipro_test_pred = [label2id[out[1].label] for out in mipro_report_results]"
   ],
   "id": "a2a3b4dc66a11dc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "mipro_combined = compute_matrices(mipro_test_pred)",
   "id": "8d547baef787ab52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "display(pd.DataFrame(mipro_combined, index=[\"mipro_combined predictor\"]))",
   "id": "7d13e13deb2e16de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e542ccfb5cbf53a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dspy.teleprompt import Ensemble\n",
    "ensemble = Ensemble(reduce_fn=dspy.majority)\n",
    "combined = ensemble.compile([opted_rs, opted_mipro])"
   ],
   "id": "edeb3f14d590a71b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "combined_score, combined_report, combined_test_pred = evaluate(combined)\n",
    "combined_combined = compute_matrices(combined_test_pred)\n",
    "display(pd.DataFrame(combined_combined, index=[\"combined predictor\"]))"
   ],
   "id": "b977c881515b38b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:42:16.497324268Z",
     "start_time": "2025-07-20T16:59:42.748481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "test_results = {}\n",
    "test_results_on_original_predicator = {}\n",
    "for sec, examples in test_data.items():\n",
    "    # print(f\"Evaluating on test section: {sec}\")\n",
    "    # evaluator = Evaluate(\n",
    "    #     devset=examples,\n",
    "    #     metric=accuracy_metric,\n",
    "    #     num_threads=20,\n",
    "    #     display_progress=True,\n",
    "    #     display_table=5,\n",
    "    #     provide_traceback=True,\n",
    "    #     max_errors=5,\n",
    "    # )\n",
    "    # result = evaluator(joint_predictor)\n",
    "    # test_results[sec] = result\n",
    "    print(f\"Evaluating on test section:\\t{sec}\")\n",
    "    evaluator = Evaluate(\n",
    "        devset=examples,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    "        # max_errors=30\n",
    "    )\n",
    "    eval_res = evaluator(joint_predictor)\n",
    "    eval_res_on_original_predicator = evaluator(predictor)\n",
    "    _, result_tuples = eval_res\n",
    "    _, result_tuples_on_original_predicator = eval_res_on_original_predicator\n",
    "    preds, refs = [], []\n",
    "    for example, prediction, correct in result_tuples:\n",
    "        if not hasattr(prediction, \"label\"):\n",
    "            continue\n",
    "        preds.append(prediction.label)\n",
    "        refs.append(example.label)\n",
    "    for example, prediction, correct in result_tuples_on_original_predicator:\n",
    "        if not hasattr(prediction, \"label\"):\n",
    "            continue\n",
    "        preds.append(prediction.label)\n",
    "        refs.append(example.label)\n",
    "    test_results[sec] = {\"preds\": preds, \"refs\": refs}\n",
    "    test_results_on_original_predicator[sec] = {\"preds\": preds, \"refs\": refs}"
   ],
   "id": "aa28d0ff0875e569",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test section:\tpresupposition_all_n_presupposition\n",
      "Average Metric: 565.00 / 570 (99.1%): 100%|██████████| 570/570 [00:00<00:00, 699.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 19:59:51 INFO dspy.evaluate.evaluate: Average Metric: 565 / 570 (99.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 540.00 / 570 (94.7%): 100%|██████████| 570/570 [00:00<00:00, 3489.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 19:59:52 INFO dspy.evaluate.evaluate: Average Metric: 540 / 570 (94.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test section:\tpresupposition_both_presupposition\n",
      "Average Metric: 561.00 / 570 (98.4%): 100%|██████████| 570/570 [00:05<00:00, 96.67it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:02 INFO dspy.evaluate.evaluate: Average Metric: 561 / 570 (98.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 557.00 / 570 (97.7%): 100%|██████████| 570/570 [00:00<00:00, 3310.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:02 INFO dspy.evaluate.evaluate: Average Metric: 557 / 570 (97.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test section:\tpresupposition_change_of_state\n",
      "Average Metric: 318.00 / 570 (55.8%): 100%|██████████| 570/570 [00:00<00:00, 838.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:11 INFO dspy.evaluate.evaluate: Average Metric: 318 / 570 (55.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 321.00 / 570 (56.3%): 100%|██████████| 570/570 [00:00<00:00, 3965.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:11 INFO dspy.evaluate.evaluate: Average Metric: 321 / 570 (56.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test section:\tpresupposition_cleft_existence\n",
      "Average Metric: 424.00 / 570 (74.4%): 100%|██████████| 570/570 [00:07<00:00, 72.57it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:21 INFO dspy.evaluate.evaluate: Average Metric: 424 / 570 (74.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 412.00 / 570 (72.3%): 100%|██████████| 570/570 [00:00<00:00, 3830.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:21 INFO dspy.evaluate.evaluate: Average Metric: 412 / 570 (72.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test section:\tpresupposition_cleft_uniqueness\n",
      "Average Metric: 283.00 / 570 (49.6%): 100%|██████████| 570/570 [00:03<00:00, 146.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:30 INFO dspy.evaluate.evaluate: Average Metric: 283 / 570 (49.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 283.00 / 570 (49.6%): 100%|██████████| 570/570 [00:00<00:00, 2733.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:31 INFO dspy.evaluate.evaluate: Average Metric: 283 / 570 (49.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test section:\tpresupposition_only_presupposition\n",
      "Average Metric: 399.00 / 570 (70.0%): 100%|██████████| 570/570 [00:03<00:00, 181.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:41 INFO dspy.evaluate.evaluate: Average Metric: 399 / 570 (70.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 396.00 / 570 (69.5%): 100%|██████████| 570/570 [00:00<00:00, 2636.49it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:42 INFO dspy.evaluate.evaluate: Average Metric: 396 / 570 (69.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test section:\tpresupposition_possessed_definites_existence\n",
      "Average Metric: 550.00 / 570 (96.5%): 100%|██████████| 570/570 [00:02<00:00, 202.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:51 INFO dspy.evaluate.evaluate: Average Metric: 550 / 570 (96.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 524.00 / 570 (91.9%): 100%|██████████| 570/570 [00:00<00:00, 3297.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:00:52 INFO dspy.evaluate.evaluate: Average Metric: 524 / 570 (91.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test section:\tpresupposition_possessed_definites_uniqueness\n",
      "Average Metric: 264.00 / 570 (46.3%): 100%|██████████| 570/570 [00:07<00:00, 75.69it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:01:02 INFO dspy.evaluate.evaluate: Average Metric: 264 / 570 (46.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 263.00 / 570 (46.1%): 100%|██████████| 570/570 [00:00<00:00, 2802.54it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:01:03 INFO dspy.evaluate.evaluate: Average Metric: 263 / 570 (46.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test section:\tpresupposition_question_presupposition\n",
      "Average Metric: 492.00 / 570 (86.3%): 100%|██████████| 570/570 [00:10<00:00, 54.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:01:14 INFO dspy.evaluate.evaluate: Average Metric: 492 / 570 (86.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 496.00 / 570 (87.0%): 100%|██████████| 570/570 [00:00<00:00, 3431.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 20:01:14 INFO dspy.evaluate.evaluate: Average Metric: 496 / 570 (87.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:42:16.497813075Z",
     "start_time": "2025-07-20T16:54:07.329451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metric_prf = combine([\"precision\", \"recall\", \"f1\"])\n",
    "acc = load(\"accuracy\")\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}"
   ],
   "id": "23b5ae8dcfa60120",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:42:16.498041204Z",
     "start_time": "2025-07-20T17:01:15.336525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rows = []\n",
    "all_preds, all_refs = [], []\n",
    "\n",
    "for sec, res in test_results.items():\n",
    "    print(f\"Calculating metrics for section:\\t{sec}\")\n",
    "    preds = [label2id[p] for p in res['preds']]\n",
    "    refs = [label2id[ex] for ex in res['refs']]\n",
    "\n",
    "    prf = metric_prf.compute(predictions=preds, references=refs, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "\n",
    "    rows.append({\"section\": sec, \"accuracy\": accuracy, **prf})\n",
    "    all_preds += preds\n",
    "    all_refs += refs\n",
    "\n",
    "overall_prf = metric_prf.compute(predictions=all_preds, references=all_refs, average=\"weighted\")\n",
    "overall_acc = acc.compute(predictions=all_preds, references=all_refs)[\"accuracy\"]\n",
    "rows.append({\"section\": \"all\", \"accuracy\": overall_acc, **overall_prf})\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "display(df_metrics.set_index(\"section\"))"
   ],
   "id": "60dd214c23649ddf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics for section:\tpresupposition_all_n_presupposition\n",
      "Calculating metrics for section:\tpresupposition_both_presupposition\n",
      "Calculating metrics for section:\tpresupposition_change_of_state\n",
      "Calculating metrics for section:\tpresupposition_cleft_existence\n",
      "Calculating metrics for section:\tpresupposition_cleft_uniqueness\n",
      "Calculating metrics for section:\tpresupposition_only_presupposition\n",
      "Calculating metrics for section:\tpresupposition_possessed_definites_existence\n",
      "Calculating metrics for section:\tpresupposition_possessed_definites_uniqueness\n",
      "Calculating metrics for section:\tpresupposition_question_presupposition\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                               accuracy  precision    recall  \\\n",
       "section                                                                        \n",
       "presupposition_all_n_presupposition            0.969298   0.971071  0.969298   \n",
       "presupposition_both_presupposition             0.980702   0.981264  0.980702   \n",
       "presupposition_change_of_state                 0.560526   0.668705  0.560526   \n",
       "presupposition_cleft_existence                 0.733333   0.832485  0.733333   \n",
       "presupposition_cleft_uniqueness                0.496491   0.769661  0.496491   \n",
       "presupposition_only_presupposition             0.697368   0.801856  0.697368   \n",
       "presupposition_possessed_definites_existence   0.942105   0.945813  0.942105   \n",
       "presupposition_possessed_definites_uniqueness  0.462281   0.763888  0.462281   \n",
       "presupposition_question_presupposition         0.866667   0.878098  0.866667   \n",
       "all                                            0.745419   0.826205  0.745419   \n",
       "\n",
       "                                                     f1  \n",
       "section                                                  \n",
       "presupposition_all_n_presupposition            0.969294  \n",
       "presupposition_both_presupposition             0.980716  \n",
       "presupposition_change_of_state                 0.495187  \n",
       "presupposition_cleft_existence                 0.725441  \n",
       "presupposition_cleft_uniqueness                0.385187  \n",
       "presupposition_only_presupposition             0.683522  \n",
       "presupposition_possessed_definites_existence   0.942186  \n",
       "presupposition_possessed_definites_uniqueness  0.342022  \n",
       "presupposition_question_presupposition         0.864703  \n",
       "all                                            0.738150  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>presupposition_all_n_presupposition</th>\n",
       "      <td>0.969298</td>\n",
       "      <td>0.971071</td>\n",
       "      <td>0.969298</td>\n",
       "      <td>0.969294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_both_presupposition</th>\n",
       "      <td>0.980702</td>\n",
       "      <td>0.981264</td>\n",
       "      <td>0.980702</td>\n",
       "      <td>0.980716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_change_of_state</th>\n",
       "      <td>0.560526</td>\n",
       "      <td>0.668705</td>\n",
       "      <td>0.560526</td>\n",
       "      <td>0.495187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_cleft_existence</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.832485</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.725441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_cleft_uniqueness</th>\n",
       "      <td>0.496491</td>\n",
       "      <td>0.769661</td>\n",
       "      <td>0.496491</td>\n",
       "      <td>0.385187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_only_presupposition</th>\n",
       "      <td>0.697368</td>\n",
       "      <td>0.801856</td>\n",
       "      <td>0.697368</td>\n",
       "      <td>0.683522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_possessed_definites_existence</th>\n",
       "      <td>0.942105</td>\n",
       "      <td>0.945813</td>\n",
       "      <td>0.942105</td>\n",
       "      <td>0.942186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_possessed_definites_uniqueness</th>\n",
       "      <td>0.462281</td>\n",
       "      <td>0.763888</td>\n",
       "      <td>0.462281</td>\n",
       "      <td>0.342022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_question_presupposition</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.878098</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.864703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.745419</td>\n",
       "      <td>0.826205</td>\n",
       "      <td>0.745419</td>\n",
       "      <td>0.738150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:42:16.499134497Z",
     "start_time": "2025-07-20T17:01:15.691456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rows = []\n",
    "all_preds, all_refs = [], []\n",
    "\n",
    "for sec, res in test_results_on_original_predicator.items():\n",
    "    print(f\"Calculating metrics for section:\\t{sec}\")\n",
    "    preds = [label2id[p] for p in res['preds']]\n",
    "    refs = [label2id[ex] for ex in res['refs']]\n",
    "\n",
    "    prf = metric_prf.compute(predictions=preds, references=refs, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "\n",
    "    rows.append({\"section\": sec, \"accuracy\": accuracy, **prf})\n",
    "    all_preds += preds\n",
    "    all_refs += refs\n",
    "\n",
    "overall_prf = metric_prf.compute(predictions=all_preds, references=all_refs, average=\"weighted\")\n",
    "overall_acc = acc.compute(predictions=all_preds, references=all_refs)[\"accuracy\"]\n",
    "rows.append({\"section\": \"all\", \"accuracy\": overall_acc, **overall_prf})\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "display(df_metrics.set_index(\"section\"))"
   ],
   "id": "23cbc17317f6d82d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics for section:\tpresupposition_all_n_presupposition\n",
      "Calculating metrics for section:\tpresupposition_both_presupposition\n",
      "Calculating metrics for section:\tpresupposition_change_of_state\n",
      "Calculating metrics for section:\tpresupposition_cleft_existence\n",
      "Calculating metrics for section:\tpresupposition_cleft_uniqueness\n",
      "Calculating metrics for section:\tpresupposition_only_presupposition\n",
      "Calculating metrics for section:\tpresupposition_possessed_definites_existence\n",
      "Calculating metrics for section:\tpresupposition_possessed_definites_uniqueness\n",
      "Calculating metrics for section:\tpresupposition_question_presupposition\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                               accuracy  precision    recall  \\\n",
       "section                                                                        \n",
       "presupposition_all_n_presupposition            0.969298   0.971071  0.969298   \n",
       "presupposition_both_presupposition             0.980702   0.981264  0.980702   \n",
       "presupposition_change_of_state                 0.560526   0.668705  0.560526   \n",
       "presupposition_cleft_existence                 0.733333   0.832485  0.733333   \n",
       "presupposition_cleft_uniqueness                0.496491   0.769661  0.496491   \n",
       "presupposition_only_presupposition             0.697368   0.801856  0.697368   \n",
       "presupposition_possessed_definites_existence   0.942105   0.945813  0.942105   \n",
       "presupposition_possessed_definites_uniqueness  0.462281   0.763888  0.462281   \n",
       "presupposition_question_presupposition         0.866667   0.878098  0.866667   \n",
       "all                                            0.745419   0.826205  0.745419   \n",
       "\n",
       "                                                     f1  \n",
       "section                                                  \n",
       "presupposition_all_n_presupposition            0.969294  \n",
       "presupposition_both_presupposition             0.980716  \n",
       "presupposition_change_of_state                 0.495187  \n",
       "presupposition_cleft_existence                 0.725441  \n",
       "presupposition_cleft_uniqueness                0.385187  \n",
       "presupposition_only_presupposition             0.683522  \n",
       "presupposition_possessed_definites_existence   0.942186  \n",
       "presupposition_possessed_definites_uniqueness  0.342022  \n",
       "presupposition_question_presupposition         0.864703  \n",
       "all                                            0.738150  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>presupposition_all_n_presupposition</th>\n",
       "      <td>0.969298</td>\n",
       "      <td>0.971071</td>\n",
       "      <td>0.969298</td>\n",
       "      <td>0.969294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_both_presupposition</th>\n",
       "      <td>0.980702</td>\n",
       "      <td>0.981264</td>\n",
       "      <td>0.980702</td>\n",
       "      <td>0.980716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_change_of_state</th>\n",
       "      <td>0.560526</td>\n",
       "      <td>0.668705</td>\n",
       "      <td>0.560526</td>\n",
       "      <td>0.495187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_cleft_existence</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.832485</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.725441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_cleft_uniqueness</th>\n",
       "      <td>0.496491</td>\n",
       "      <td>0.769661</td>\n",
       "      <td>0.496491</td>\n",
       "      <td>0.385187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_only_presupposition</th>\n",
       "      <td>0.697368</td>\n",
       "      <td>0.801856</td>\n",
       "      <td>0.697368</td>\n",
       "      <td>0.683522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_possessed_definites_existence</th>\n",
       "      <td>0.942105</td>\n",
       "      <td>0.945813</td>\n",
       "      <td>0.942105</td>\n",
       "      <td>0.942186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_possessed_definites_uniqueness</th>\n",
       "      <td>0.462281</td>\n",
       "      <td>0.763888</td>\n",
       "      <td>0.462281</td>\n",
       "      <td>0.342022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_question_presupposition</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.878098</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.864703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.745419</td>\n",
       "      <td>0.826205</td>\n",
       "      <td>0.745419</td>\n",
       "      <td>0.738150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's examine the results:\n",
    "\n",
    "| Section                                          | Accuracy | Precision | Recall   | F1 Score |\n",
    "|--------------------------------------------------|----------|-----------|----------|----------|\n",
    "| presupposition_all_n_presupposition              | 0.991228 | 0.991400  | 0.991228 | 0.991237 |\n",
    "| presupposition_both_presupposition               | 0.984211 | 0.984519  | 0.984211 | 0.984191 |\n",
    "| presupposition_change_of_state                   | 0.557895 | 0.652114  | 0.557895 | 0.491531 |\n",
    "| presupposition_cleft_existence                   | 0.743860 | 0.835004  | 0.743860 | 0.736982 |\n",
    "| presupposition_cleft_uniqueness                  | 0.496491 | 0.769661  | 0.496491 | 0.385733 |\n",
    "| presupposition_only_presupposition               | 0.700000 | 0.813519  | 0.700000 | 0.685228 |\n",
    "| presupposition_possessed_definites_existence     | 0.964912 | 0.965735  | 0.964912 | 0.964956 |\n",
    "| presupposition_possessed_definites_uniqueness    | 0.463158 | 0.769068  | 0.463158 | 0.343951 |\n",
    "| presupposition_question_presupposition           | 0.863158 | 0.875327  | 0.863158 | 0.861277 |\n",
    "| all                                              | 0.751657 | 0.828935  | 0.751657 | 0.745117 |\n",
    "\n",
    "Total F1 score of 0.745, not that much of an improvement :(\n",
    "\n",
    "let's try to optimize it in another way."
   ],
   "id": "fd9ccd828452caba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
