{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres LLM Baseline with Chain of Thought\n",
    "\n",
    "This notebook implements a baseline for ImpPres classification using an LLM through DSPy, including Chain of Thought reasoning.\n",
    "\n",
    "The implementation follows these steps:\n",
    "\n",
    "1. Basic Setup:\n",
    "   - Configure DSPy environment\n",
    "   - Define NLI Classifier Signature\n",
    "\n",
    "2. Dataset Loading:\n",
    "   - Load ImpPres presupposition sections\n",
    "   - Prepare data for evaluation\n",
    "\n",
    "3. Zero-shot Baseline:\n",
    "   - Implement basic NLI classifier\n",
    "   - Evaluate on all sections\n",
    "\n",
    "4. Optimization Strategies:\n",
    "   - Bootstrap Few-Shot learning\n",
    "   - MIPROv2 optimization\n",
    "   - Ensemble methods\n",
    "\n",
    "5. Chain of Thought Enhancement:\n",
    "   - Implement CoT-based classifier\n",
    "   - Compare with basic classifier\n",
    "\n",
    "6. Model Comparison:\n",
    "   - Compare with DeBERTa baseline\n",
    "   - Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9b979cbc0dc715",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:12.765252Z",
     "start_time": "2025-07-31T20:35:10.455958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "from os import environ\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import dspy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "logging.getLogger(\"dspy.adapters.json_adapter\").setLevel(logging.ERROR)\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=environ['XAI_API_KEY'])\n",
    "\n",
    "# for ollama\n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "# lm = dspy.LM(\n",
    "#     \"ollama/llama3.1:8b\",\n",
    "#     api_base=\"http://localhost:11434\",\n",
    "#     format=\"json\"        # litellm translates this to Ollama's stream=false\n",
    "# )\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686e6e259245fe7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:12.865734Z",
     "start_time": "2025-07-31T20:35:12.861941Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy program to classify pairs (premise, hypothesis) as entailment, contradiction, or neutral.\n",
    "class NLIImPresClassifier(dspy.Signature):\n",
    "    \"\"\"A DSPy signature for Natural Language Inference classification.\n",
    "    \n",
    "    This classifier takes a premise and hypothesis as input and determines their \n",
    "    logical relationship: entailment, neutral, or contradiction.\n",
    "    \"\"\"\n",
    "    premise     : str = dspy.InputField(desc=\"A short passage or statement. All facts should be inferred from this text alone.\")\n",
    "    hypothesis  : str = dspy.InputField(desc=\"A second statement to evaluate. Check if this follows from, contradicts, or is unrelated to the premise.\")\n",
    "    label       : Literal[\"entailment\", \"neutral\", \"contradiction\"] = dspy.OutputField(\n",
    "        desc=(\n",
    "            \"Return one of: 'entailment', 'neutral', or 'contradiction'.\\n\"\n",
    "            \"- 'entailment': The hypothesis must be true if the premise is true.\\n\"\n",
    "            \"- 'contradiction': The hypothesis must be false if the premise is true.\\n\"\n",
    "            \"- 'neutral': The hypothesis could be either true or false based on the premise.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create a basic predictor using the signature\n",
    "predictor = dspy.Predict(NLIImPresClassifier)\n",
    "\n",
    "# Define label names for mapping between numeric and string labels\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "def zero_shot_nli_classifier(x):\n",
    "    \"\"\"Apply zero-shot NLI classification to a single example.\n",
    "    \n",
    "    Args:\n",
    "        x: Dictionary containing 'premise', 'hypothesis', and 'gold_label'\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with the input fields plus predictions\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'premise': x['premise'],\n",
    "        'hypothesis': x['hypothesis'],\n",
    "        'pred_label': predictor(premise=x['premise'], hypothesis=x['hypothesis']).label,\n",
    "        'gold_label': label_names[x['gold_label']]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1719d8f8eaf51",
   "metadata": {},
   "source": [
    "## Load ImpPres dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47958515fef57a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:13.221042Z",
     "start_time": "2025-07-31T20:35:12.909475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_imppres_presuppositions.parquet\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from os.path import exists\n",
    "\n",
    "# Define sections\n",
    "sections = [\n",
    "    'presupposition_all_n_presupposition',\n",
    "    'presupposition_both_presupposition',\n",
    "    'presupposition_change_of_state',\n",
    "    'presupposition_cleft_existence',\n",
    "    'presupposition_cleft_uniqueness',\n",
    "    'presupposition_only_presupposition',\n",
    "    'presupposition_possessed_definites_existence',\n",
    "    'presupposition_possessed_definites_uniqueness',\n",
    "    'presupposition_question_presupposition'\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "if not exists('combined_imppres_presuppositions.parquet'):\n",
    "    # Load each section\n",
    "    for section in sections:\n",
    "        print(f\"Loading dataset for section: {section}\")\n",
    "        dataset[section] = load_dataset(\"facebook/imppres\", section)\n",
    "\n",
    "    # Convert to dataframes and combine\n",
    "    dataframes_list = []\n",
    "    for section, data in dataset.items():\n",
    "        df = data.to_pandas()\n",
    "        df['section'] = section\n",
    "        dataframes_list.append(df)\n",
    "\n",
    "    combined_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "else:\n",
    "    combined_df = pd.read_parquet('combined_imppres_presuppositions.parquet')\n",
    "    print(f\"Loaded combined_imppres_presuppositions.parquet\")\n",
    "\n",
    "# Convert back to datasets\n",
    "dataset = {}\n",
    "for section, group in combined_df.groupby(\"section\"):\n",
    "    dataset[section] = Dataset.from_pandas(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4a87f5ab7d4a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:13.237138Z",
     "start_time": "2025-07-31T20:35:13.233739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presupposition_all_n_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_both_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_change_of_state': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_cleft_existence': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_cleft_uniqueness': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_only_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_possessed_definites_existence': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_possessed_definites_uniqueness': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_question_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " })}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d7536a65ae709e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:13.298996Z",
     "start_time": "2025-07-31T20:35:13.289097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>0e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten senators that proved to ...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>2n</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>3e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>4c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17095</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>Travel shocks Janet.</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1895n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17096</th>\n",
       "      <td>The actors didn't conceal where that mall shoc...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1896c</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17097</th>\n",
       "      <td>Did the actors conceal where that mall shocks ...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1897n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17098</th>\n",
       "      <td>The actors might have concealed where that mal...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>modal</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1898n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17099</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>conditional</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1899n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0      All ten guys that proved to boast were divorcing.   \n",
       "1      All ten guys that proved to boast were divorcing.   \n",
       "2      All ten guys that proved to boast were divorcing.   \n",
       "3      All ten guys that proved to boast weren't divo...   \n",
       "4      All ten guys that proved to boast weren't divo...   \n",
       "...                                                  ...   \n",
       "17095  If the actors do conceal where that mall shock...   \n",
       "17096  The actors didn't conceal where that mall shoc...   \n",
       "17097  Did the actors conceal where that mall shocks ...   \n",
       "17098  The actors might have concealed where that mal...   \n",
       "17099  If the actors do conceal where that mall shock...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "0       There are exactly ten guys that proved to boast.      unembedded   \n",
       "1      There are exactly eleven guys that proved to b...      unembedded   \n",
       "2      There are exactly ten senators that proved to ...      unembedded   \n",
       "3       There are exactly ten guys that proved to boast.         negated   \n",
       "4      There are exactly eleven guys that proved to b...         negated   \n",
       "...                                                  ...             ...   \n",
       "17095                               Travel shocks Janet.     conditional   \n",
       "17096  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17097  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17098  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17099  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "0      Not_In_Example  Not_In_Example        positive           0   \n",
       "1      Not_In_Example  Not_In_Example         negated           2   \n",
       "2      Not_In_Example  Not_In_Example         neutral           1   \n",
       "3      Not_In_Example  Not_In_Example        positive           0   \n",
       "4      Not_In_Example  Not_In_Example         negated           2   \n",
       "...               ...             ...             ...         ...   \n",
       "17095  Not_In_Example  Not_In_Example         neutral           1   \n",
       "17096         negated      unembedded  Not_In_Example           2   \n",
       "17097   interrogative      unembedded  Not_In_Example           1   \n",
       "17098           modal      unembedded  Not_In_Example           1   \n",
       "17099     conditional      unembedded  Not_In_Example           1   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "0         all_n_presupposition     0e           0   \n",
       "1         all_n_presupposition     1c           0   \n",
       "2         all_n_presupposition     2n           0   \n",
       "3         all_n_presupposition     3e           0   \n",
       "4         all_n_presupposition     4c           0   \n",
       "...                        ...    ...         ...   \n",
       "17095  question_presupposition  1895n          99   \n",
       "17096  question_presupposition  1896c          99   \n",
       "17097  question_presupposition  1897n          99   \n",
       "17098  question_presupposition  1898n          99   \n",
       "17099  question_presupposition  1899n          99   \n",
       "\n",
       "                                      section  \n",
       "0         presupposition_all_n_presupposition  \n",
       "1         presupposition_all_n_presupposition  \n",
       "2         presupposition_all_n_presupposition  \n",
       "3         presupposition_all_n_presupposition  \n",
       "4         presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "17095  presupposition_question_presupposition  \n",
       "17096  presupposition_question_presupposition  \n",
       "17097  presupposition_question_presupposition  \n",
       "17098  presupposition_question_presupposition  \n",
       "17099  presupposition_question_presupposition  \n",
       "\n",
       "[17100 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5302e1aa89410d",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e7a0ffbd08457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:21.148590Z",
     "start_time": "2025-07-31T20:35:13.343125Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd1d6e2a92bf94",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c419a6458455fe6",
   "metadata": {},
   "source": [
    "We will first run the dspy classifier through the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c068b1c86ed42dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:21.160332Z",
     "start_time": "2025-07-31T20:35:21.158586Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_metric(example, pred, *args):\n",
    "     return pred.label == example.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33c73280e69e73b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:22.448237Z",
     "start_time": "2025-07-31T20:35:21.208582Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Convert to DSPy Example objects\n",
    "dspy_examples = {}\n",
    "for section_name, section in dataset.items():\n",
    "    dspy_examples[section_name] = [\n",
    "        dspy.Example(\n",
    "            premise=ex['premise'],\n",
    "            hypothesis=ex['hypothesis'],\n",
    "            label=label_names[ex['gold_label']]\n",
    "        ).with_inputs(\"premise\", \"hypothesis\")\n",
    "        for ex in section\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame(dspy_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c65566eacfa62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:48.677735Z",
     "start_time": "2025-07-31T20:35:22.471783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating section:\tpresupposition_all_n_presupposition\n",
      "  0%|          | 0/1900 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:32 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'The five waiters that approached Paul depart.', 'hypothesis': 'There are exactly five waiters that approached Paul.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):   0%|          | 1/1900 [00:09<4:59:30,  9.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:32 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"All ten guys that proved to boast weren't divorcing.\", 'hypothesis': 'There are exactly ten senators that proved to boast.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):   0%|          | 2/1900 [00:09<2:07:04,  4.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:32 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Were all ten guys that proved to boast divorcing?', 'hypothesis': 'All ten guys that proved to boast were divorcing.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):   0%|          | 2/1900 [00:09<2:07:04,  4.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:32 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Were all ten guys that proved to boast divorcing?', 'hypothesis': 'There are exactly ten guys that proved to boast.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):   0%|          | 3/1900 [00:09<2:07:00,  4.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten guys that proved to boast were divorcing.', 'hypothesis': 'There are exactly ten guys that proved to boast.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):   3%|▎         | 61/1900 [00:09<2:03:07,  4.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"All ten reports that can bore some waiter aren't disagreeing with Naomi.\", 'hypothesis': 'There are exactly ten reports that can bore some waiter.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):   5%|▌         | 104/1900 [00:09<01:39, 18.07it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'The five waiters that approached Paul might depart.', 'hypothesis': 'There are exactly six waiters that approached Paul.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):   7%|▋         | 137/1900 [00:09<01:37, 18.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten reports that can bore some waiter might be disagreeing with Naomi.', 'hypothesis': 'There are exactly ten reports that can bore some waiter.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):   7%|▋         | 142/1900 [00:09<01:37, 18.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Are all ten reports that can bore some waiter disagreeing with Naomi?', 'hypothesis': 'There are exactly ten reports that can bore some waiter.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):   9%|▊         | 164/1900 [00:09<01:36, 18.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten reports that can bore some waiter might be disagreeing with Naomi.', 'hypothesis': 'There are exactly ten waitresses that can bore some waiter.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):   9%|▉         | 176/1900 [00:09<01:35, 18.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten guys that proved to boast might have been divorcing.', 'hypothesis': 'There are exactly ten senators that proved to boast.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  14%|█▍        | 267/1900 [00:09<00:35, 45.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Do the five waiters that approached Paul depart?', 'hypothesis': 'There are exactly five waiters that approached Paul.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  16%|█▌        | 300/1900 [00:09<00:35, 45.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Are all ten reports that can bore some waiter disagreeing with Naomi?', 'hypothesis': 'There are exactly ten waitresses that can bore some waiter.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  18%|█▊        | 346/1900 [00:09<00:34, 45.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If all ten guys that proved to boast were divorcing, it's okay.\", 'hypothesis': 'All ten guys that proved to boast were divorcing.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  21%|██        | 403/1900 [00:09<00:11, 125.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If all ten reports that can bore some waiter are disagreeing with Naomi, it's okay.\", 'hypothesis': 'There are exactly ten reports that can bore some waiter.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  22%|██▏       | 411/1900 [00:10<00:11, 125.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Are all ten reports that can bore some waiter disagreeing with Naomi?', 'hypothesis': 'There are exactly eleven reports that can bore some waiter.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  22%|██▏       | 415/1900 [00:10<00:11, 125.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten guys that proved to boast were divorcing.', 'hypothesis': 'There are exactly eleven guys that proved to boast.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  23%|██▎       | 435/1900 [00:10<00:11, 125.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If all ten reports that can bore some waiter are disagreeing with Naomi, it's okay.\", 'hypothesis': 'There are exactly ten waitresses that can bore some waiter.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  31%|███▏      | 596/1900 [00:10<00:06, 196.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"All ten guys that proved to boast weren't divorcing.\", 'hypothesis': 'There are exactly eleven guys that proved to boast.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  49%|████▉     | 928/1900 [00:10<00:03, 270.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten guys that proved to boast might have been divorcing.', 'hypothesis': 'There are exactly eleven guys that proved to boast.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  86%|████████▌ | 1627/1900 [00:10<00:00, 969.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:33 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten guys that proved to boast might have been divorcing.', 'hypothesis': 'All ten guys that proved to boast were divorcing.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  93%|█████████▎| 1764/1900 [00:10<00:00, 969.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If all ten guys that proved to boast were divorcing, it's okay.\", 'hypothesis': 'There are exactly eleven guys that proved to boast.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  94%|█████████▎| 1780/1900 [00:10<00:00, 1117.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"The five waiters that approached Paul don't depart.\", 'hypothesis': 'There are exactly five waiters that approached Paul.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):  97%|█████████▋| 1851/1900 [00:10<00:00, 171.25it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:34 WARNING dspy.utils.parallelizer: Execution cancelled due to errors or interruption.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Were all ten guys that proved to boast divorcing?', 'hypothesis': 'There are exactly ten senators that proved to boast.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten reports that can bore some waiter are disagreeing with Naomi.', 'hypothesis': 'There are exactly eleven reports that can bore some waiter.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"All ten guys that proved to boast weren't divorcing.\", 'hypothesis': 'All ten guys that proved to boast were divorcing.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Execution cancelled due to errors or interruption.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating section:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msec\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m evaluator = Evaluate(\n\u001b[32m     10\u001b[39m     devset=dspy_examples[sec],\n\u001b[32m     11\u001b[39m     metric=accuracy_metric,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# max_errors=30\u001b[39;00m\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m eval_res = \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m _, result_tuples = eval_res\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnumber of results:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result_tuples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/UNIVERSITY/סמסטר ח/עיבוד שפה טבעית עם LLM/עבודות/Assignment2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m callbacks = _get_active_callbacks(instance)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n\u001b[32m    330\u001b[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/UNIVERSITY/סמסטר ח/עיבוד שפה טבעית עם LLM/עבודות/Assignment2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:171\u001b[39m, in \u001b[36mEvaluate.__call__\u001b[39m\u001b[34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs, callback_metadata)\u001b[39m\n\u001b[32m    167\u001b[39m         program._suggest_failures += dspy.settings.get(\u001b[33m\"\u001b[39m\u001b[33msuggest_failures\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction, score\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m results = \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devset) == \u001b[38;5;28mlen\u001b[39m(results)\n\u001b[32m    174\u001b[39m results = [((dspy.Prediction(), \u001b[38;5;28mself\u001b[39m.failure_score) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/UNIVERSITY/סמסטר ח/עיבוד שפה טבעית עם LLM/עבודות/Assignment2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/parallelizer.py:48\u001b[39m, in \u001b[36mParallelExecutor.execute\u001b[39m\u001b[34m(self, function, data)\u001b[39m\n\u001b[32m     46\u001b[39m tqdm.tqdm._instances.clear()\n\u001b[32m     47\u001b[39m wrapped = \u001b[38;5;28mself\u001b[39m._wrap_function(function)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/UNIVERSITY/סמסטר ח/עיבוד שפה טבעית עם LLM/עבודות/Assignment2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/parallelizer.py:203\u001b[39m, in \u001b[36mParallelExecutor._execute_parallel\u001b[39m\u001b[34m(self, function, data)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cancel_jobs.is_set():\n\u001b[32m    202\u001b[39m     logger.warning(\u001b[33m\"\u001b[39m\u001b[33mExecution cancelled due to errors or interruption.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mExecution cancelled due to errors or interruption.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[31mException\u001b[39m: Execution cancelled due to errors or interruption."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten reports that can bore some waiter might be disagreeing with Naomi.', 'hypothesis': 'All ten reports that can bore some waiter are disagreeing with Naomi.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Do the five waiters that approached Paul depart?', 'hypothesis': 'There are exactly six waiters that approached Paul.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'The five waiters that approached Paul depart.', 'hypothesis': 'There are exactly five doctors that approached Paul.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"All ten reports that can bore some waiter aren't disagreeing with Naomi.\", 'hypothesis': 'There are exactly ten waitresses that can bore some waiter.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"All ten guys that proved to boast weren't divorcing.\", 'hypothesis': 'There are exactly ten guys that proved to boast.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten guys that proved to boast were divorcing.', 'hypothesis': 'There are exactly ten senators that proved to boast.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Are all ten reports that can bore some waiter disagreeing with Naomi?', 'hypothesis': 'All ten reports that can bore some waiter are disagreeing with Naomi.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten reports that can bore some waiter are disagreeing with Naomi.', 'hypothesis': 'There are exactly ten reports that can bore some waiter.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten reports that can bore some waiter might be disagreeing with Naomi.', 'hypothesis': 'There are exactly eleven reports that can bore some waiter.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"All ten reports that can bore some waiter aren't disagreeing with Naomi.\", 'hypothesis': 'All ten reports that can bore some waiter are disagreeing with Naomi.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If all ten guys that proved to boast were divorcing, it's okay.\", 'hypothesis': 'There are exactly ten senators that proved to boast.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'The five waiters that approached Paul depart.', 'hypothesis': 'There are exactly six waiters that approached Paul.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If all ten guys that proved to boast were divorcing, it's okay.\", 'hypothesis': 'There are exactly ten guys that proved to boast.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Were all ten guys that proved to boast divorcing?', 'hypothesis': 'There are exactly eleven guys that proved to boast.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'The five waiters that approached Paul might depart.', 'hypothesis': 'There are exactly five waiters that approached Paul.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"The five waiters that approached Paul don't depart.\", 'hypothesis': 'There are exactly five doctors that approached Paul.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If all ten reports that can bore some waiter are disagreeing with Naomi, it's okay.\", 'hypothesis': 'All ten reports that can bore some waiter are disagreeing with Naomi.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten guys that proved to boast might have been divorcing.', 'hypothesis': 'There are exactly ten guys that proved to boast.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:34 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'All ten reports that can bore some waiter are disagreeing with Naomi.', 'hypothesis': 'There are exactly ten waitresses that can bore some waiter.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:35 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If all ten reports that can bore some waiter are disagreeing with Naomi, it's okay.\", 'hypothesis': 'There are exactly eleven reports that can bore some waiter.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:35 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"All ten reports that can bore some waiter aren't disagreeing with Naomi.\", 'hypothesis': 'There are exactly eleven reports that can bore some waiter.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:36 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'The five waiters that approached Paul might depart.', 'hypothesis': 'There are exactly five doctors that approached Paul.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:36 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Do the five waiters that approached Paul depart?', 'hypothesis': 'There are exactly five doctors that approached Paul.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:36 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"The five waiters that approached Paul don't depart.\", 'hypothesis': 'There are exactly six waiters that approached Paul.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:41 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If the five waiters that approached Paul depart, it's okay.\", 'hypothesis': 'There are exactly five doctors that approached Paul.', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:41 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If the five waiters that approached Paul depart, it's okay.\", 'hypothesis': 'There are exactly six waiters that approached Paul.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:42 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If the five waiters that approached Paul depart, it's okay.\", 'hypothesis': 'There are exactly five waiters that approached Paul.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n",
      "2025/08/11 22:44:42 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"The five waiters that approached Paul don't depart.\", 'hypothesis': 'The five waiters that approached Paul depart.', 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.BadRequestError: XaiException - Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Incorrect API key provided: yo***re. You can obtain an API key from https://console.x.ai.'}. Set `provide_traceback=True` for traceback.\n"
     ]
    }
   ],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "from evaluate import combine, load\n",
    "\n",
    "# 1. Run DSPy evaluation for each section (here, limited to first 10 for demo)\n",
    "results = {}  # Store per-section predictions\n",
    "not_predicted = {}\n",
    "for sec in dspy_examples:\n",
    "    print(f\"Evaluating section:\\t{sec}\")\n",
    "    evaluator = Evaluate(\n",
    "        devset=dspy_examples[sec],\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    "        # max_errors=30\n",
    "    )\n",
    "    eval_res = evaluator(predictor)\n",
    "    _, result_tuples = eval_res\n",
    "    print(f\"number of results:\\t{len(result_tuples)}\")\n",
    "    preds, refs = [], []\n",
    "    not_predicted[sec] = {\n",
    "        'section':sec,\n",
    "        'num_not_predicted':0,\n",
    "        'not_predicted':[]\n",
    "    }\n",
    "    for example, prediction, correct in result_tuples:\n",
    "        if not hasattr(prediction, \"label\"):\n",
    "            not_predicted[sec]['num_not_predicted']+=1\n",
    "            not_predicted[sec]['not_predicted'].append((example, prediction, correct))\n",
    "            continue\n",
    "        preds.append(prediction.label)\n",
    "        refs.append(example.label)\n",
    "    results[sec] = {\"preds\": preds, \"refs\": refs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4746c046bb1c6e8",
   "metadata": {},
   "source": [
    "Let's display some statistics about the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7b91f2dc29bf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:48.741685Z",
     "start_time": "2025-07-31T20:35:48.730404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: presupposition_all_n_presupposition\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 866, 'contradiction': 571, 'entailment': 463})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1828\n",
      "  Accuracy (quick): 0.962\n",
      "\n",
      "Section: presupposition_both_presupposition\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 841, 'contradiction': 555, 'entailment': 504})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1833\n",
      "  Accuracy (quick): 0.965\n",
      "\n",
      "Section: presupposition_change_of_state\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 1562, 'contradiction': 222, 'entailment': 116})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1058\n",
      "  Accuracy (quick): 0.557\n",
      "\n",
      "Section: presupposition_cleft_existence\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 1405, 'contradiction': 311, 'entailment': 184})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1287\n",
      "  Accuracy (quick): 0.677\n",
      "\n",
      "Section: presupposition_cleft_uniqueness\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 1799, 'contradiction': 100, 'entailment': 1})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 901\n",
      "  Accuracy (quick): 0.474\n",
      "\n",
      "Section: presupposition_only_presupposition\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 1346, 'contradiction': 342, 'entailment': 212})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1282\n",
      "  Accuracy (quick): 0.675\n",
      "\n",
      "Section: presupposition_possessed_definites_existence\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 890, 'contradiction': 553, 'entailment': 457})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1773\n",
      "  Accuracy (quick): 0.933\n",
      "\n",
      "Section: presupposition_possessed_definites_uniqueness\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 1787, 'contradiction': 108, 'entailment': 5})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 908\n",
      "  Accuracy (quick): 0.478\n",
      "\n",
      "Section: presupposition_question_presupposition\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 996, 'contradiction': 565, 'entailment': 339})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1618\n",
      "  Accuracy (quick): 0.852\n",
      "\n",
      "=== OVERALL ===\n",
      "Total predictions: 17100\n",
      "Total references:  17100\n",
      "Class distribution in predictions: Counter({'neutral': 11492, 'contradiction': 3327, 'entailment': 2281})\n",
      "Class distribution in references:  Counter({'neutral': 7200, 'contradiction': 5400, 'entailment': 4500})\n",
      "Number of matches (agreement): 12488\n",
      "Accuracy (quick): 0.730\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for sec, data in results.items():\n",
    "    preds = data['preds']\n",
    "    refs = data['refs']\n",
    "    print(f\"Section: {sec}\")\n",
    "    print(f\"  Total predictions: {len(preds)}\")\n",
    "    print(f\"  Total references:  {len(refs)}\")\n",
    "    print(f\"  Class distribution in predictions: {Counter(preds)}\")\n",
    "    print(f\"  Class distribution in references:  {Counter(refs)}\")\n",
    "    agree = sum([p == r for p, r in zip(preds, refs)])\n",
    "    print(f\"  Number of matches (agreement): {agree}\")\n",
    "    print(f\"  Accuracy (quick): {agree / len(refs):.3f}\")\n",
    "    print()\n",
    "\n",
    "# Overall stats\n",
    "all_preds = sum([v['preds'] for v in results.values()], [])\n",
    "all_refs  = sum([v['refs']  for v in results.values()], [])\n",
    "print(\"=== OVERALL ===\")\n",
    "print(f\"Total predictions: {len(all_preds)}\")\n",
    "print(f\"Total references:  {len(all_refs)}\")\n",
    "print(f\"Class distribution in predictions: {Counter(all_preds)}\")\n",
    "print(f\"Class distribution in references:  {Counter(all_refs)}\")\n",
    "agree = sum([p == r for p, r in zip(all_preds, all_refs)])\n",
    "print(f\"Number of matches (agreement): {agree}\")\n",
    "print(f\"Accuracy (quick): {agree / len(all_refs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2ca8f76c7a8e6",
   "metadata": {},
   "source": [
    "We will now show information about non-predicted examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7955257ecf8e406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:48.797642Z",
     "start_time": "2025-07-31T20:35:48.787351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>presupposition_both_presupposition</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>presupposition_change_of_state</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presupposition_cleft_existence</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>presupposition_cleft_uniqueness</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>presupposition_only_presupposition</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>presupposition_possessed_definites_existence</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>presupposition_possessed_definites_uniqueness</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         section detail\n",
       "0            presupposition_all_n_presupposition    NaN\n",
       "1             presupposition_both_presupposition    NaN\n",
       "2                 presupposition_change_of_state    NaN\n",
       "3                 presupposition_cleft_existence    NaN\n",
       "4                presupposition_cleft_uniqueness    NaN\n",
       "5             presupposition_only_presupposition    NaN\n",
       "6   presupposition_possessed_definites_existence    NaN\n",
       "7  presupposition_possessed_definites_uniqueness    NaN\n",
       "8         presupposition_question_presupposition    NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Section: presupposition_all_n_presupposition — 0 failures ===\n",
      "=== Section: presupposition_both_presupposition — 0 failures ===\n",
      "=== Section: presupposition_change_of_state — 0 failures ===\n",
      "=== Section: presupposition_cleft_existence — 0 failures ===\n",
      "=== Section: presupposition_cleft_uniqueness — 0 failures ===\n",
      "=== Section: presupposition_only_presupposition — 0 failures ===\n",
      "=== Section: presupposition_possessed_definites_existence — 0 failures ===\n",
      "=== Section: presupposition_possessed_definites_uniqueness — 0 failures ===\n",
      "=== Section: presupposition_question_presupposition — 0 failures ===\n"
     ]
    }
   ],
   "source": [
    "df_np = pd.DataFrame(list(not_predicted.values())).set_index(\"section\")\n",
    "exploded = df_np[\"not_predicted\"].explode()\n",
    "df_details = (\n",
    "    exploded\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"section\", \"not_predicted\": \"detail\"})\n",
    "    .join(pd.json_normalize(exploded).add_prefix(\"detail.\"))\n",
    ")\n",
    "display(df_details)\n",
    "for sec, info in not_predicted.items():\n",
    "    print(f\"=== Section: {sec} — {info['num_not_predicted']} failures ===\")\n",
    "    for ex, raw_out, score in info['not_predicted']:\n",
    "        print(ex)\n",
    "        premise, hypothesis, ref,= ex\n",
    "        print(f\"🎯 Ref label: {ex[ref]}\")\n",
    "        print(f\"💬 Premise: {ex[premise]}\")\n",
    "        print(f\"💬 Hypothesis: {ex[hypothesis]}\")\n",
    "        print(f\"🛑 Raw output: {raw_out!r}\")\n",
    "        print(f\"⚠️ Score: {score}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f5b67929c66534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:53.470522Z",
     "start_time": "2025-07-31T20:35:48.936298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for section: presupposition_all_n_presupposition\n",
      "Computing metrics for section: presupposition_both_presupposition\n",
      "Computing metrics for section: presupposition_change_of_state\n",
      "Computing metrics for section: presupposition_cleft_existence\n",
      "Computing metrics for section: presupposition_cleft_uniqueness\n",
      "Computing metrics for section: presupposition_only_presupposition\n",
      "Computing metrics for section: presupposition_possessed_definites_existence\n",
      "Computing metrics for section: presupposition_possessed_definites_uniqueness\n",
      "Computing metrics for section: presupposition_question_presupposition\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>presupposition_all_n_presupposition</th>\n",
       "      <td>0.962105</td>\n",
       "      <td>0.964747</td>\n",
       "      <td>0.962105</td>\n",
       "      <td>0.962232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_both_presupposition</th>\n",
       "      <td>0.964737</td>\n",
       "      <td>0.966177</td>\n",
       "      <td>0.964737</td>\n",
       "      <td>0.964739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_change_of_state</th>\n",
       "      <td>0.556842</td>\n",
       "      <td>0.653256</td>\n",
       "      <td>0.556842</td>\n",
       "      <td>0.491811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_cleft_existence</th>\n",
       "      <td>0.677368</td>\n",
       "      <td>0.811773</td>\n",
       "      <td>0.677368</td>\n",
       "      <td>0.658113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_cleft_uniqueness</th>\n",
       "      <td>0.474211</td>\n",
       "      <td>0.766186</td>\n",
       "      <td>0.474211</td>\n",
       "      <td>0.350485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_only_presupposition</th>\n",
       "      <td>0.674737</td>\n",
       "      <td>0.783427</td>\n",
       "      <td>0.674737</td>\n",
       "      <td>0.661399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_possessed_definites_existence</th>\n",
       "      <td>0.933158</td>\n",
       "      <td>0.937578</td>\n",
       "      <td>0.933158</td>\n",
       "      <td>0.933266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_possessed_definites_uniqueness</th>\n",
       "      <td>0.477895</td>\n",
       "      <td>0.603701</td>\n",
       "      <td>0.477895</td>\n",
       "      <td>0.357054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_question_presupposition</th>\n",
       "      <td>0.851579</td>\n",
       "      <td>0.871453</td>\n",
       "      <td>0.851579</td>\n",
       "      <td>0.849228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.730292</td>\n",
       "      <td>0.817167</td>\n",
       "      <td>0.730292</td>\n",
       "      <td>0.721710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               accuracy  precision    recall  \\\n",
       "section                                                                        \n",
       "presupposition_all_n_presupposition            0.962105   0.964747  0.962105   \n",
       "presupposition_both_presupposition             0.964737   0.966177  0.964737   \n",
       "presupposition_change_of_state                 0.556842   0.653256  0.556842   \n",
       "presupposition_cleft_existence                 0.677368   0.811773  0.677368   \n",
       "presupposition_cleft_uniqueness                0.474211   0.766186  0.474211   \n",
       "presupposition_only_presupposition             0.674737   0.783427  0.674737   \n",
       "presupposition_possessed_definites_existence   0.933158   0.937578  0.933158   \n",
       "presupposition_possessed_definites_uniqueness  0.477895   0.603701  0.477895   \n",
       "presupposition_question_presupposition         0.851579   0.871453  0.851579   \n",
       "all                                            0.730292   0.817167  0.730292   \n",
       "\n",
       "                                                     f1  \n",
       "section                                                  \n",
       "presupposition_all_n_presupposition            0.962232  \n",
       "presupposition_both_presupposition             0.964739  \n",
       "presupposition_change_of_state                 0.491811  \n",
       "presupposition_cleft_existence                 0.658113  \n",
       "presupposition_cleft_uniqueness                0.350485  \n",
       "presupposition_only_presupposition             0.661399  \n",
       "presupposition_possessed_definites_existence   0.933266  \n",
       "presupposition_possessed_definites_uniqueness  0.357054  \n",
       "presupposition_question_presupposition         0.849228  \n",
       "all                                            0.721710  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Prepare for metric calculation\n",
    "metric_prf = combine([\"precision\", \"recall\", \"f1\"])\n",
    "acc = load(\"accuracy\")\n",
    "rows = []\n",
    "all_preds, all_refs = [], []\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "for sec, data in results.items():\n",
    "    print(f\"Computing metrics for section: {sec}\")\n",
    "    preds = [label2id[label] for label in data[\"preds\"]]\n",
    "    refs  = [label2id[label] for label in data[\"refs\"]]\n",
    "    prf = metric_prf.compute(predictions=preds, references=refs, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "\n",
    "    rows.append({\"section\": sec, \"accuracy\": accuracy, **prf})\n",
    "    all_preds += preds\n",
    "    all_refs += refs\n",
    "\n",
    "# 3. Compute overall metrics\n",
    "overall_prf = metric_prf.compute(predictions=all_preds, references=all_refs, average=\"weighted\")\n",
    "overall_acc = acc.compute(predictions=all_preds, references=all_refs)[\"accuracy\"]\n",
    "rows.append({\"section\": \"all\", \"accuracy\": overall_acc, **overall_prf})\n",
    "\n",
    "# Create DataFrame and display\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "display(df_metrics.set_index(\"section\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fa69903b6adb1",
   "metadata": {},
   "source": [
    "In our experiment we got the following results:\n",
    "| section | accuracy | precision | recall | f1 |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| presupposition_all_n_presupposition | 0.962105 | 0.964902 | 0.962105 | 0.962229 |\n",
    "| presupposition_both_presupposition | 0.965771 | 0.967112 | 0.965771 | 0.965764 |\n",
    "| presupposition_change_of_state | 0.556842 | 0.654658 | 0.556842 | 0.491799 |\n",
    "| presupposition_cleft_existence | 0.677199 | 0.811803 | 0.677199 | 0.658082 |\n",
    "| presupposition_cleft_uniqueness | 0.474710 | 0.766148 | 0.474710 | 0.351054 |\n",
    "| presupposition_only_presupposition | 0.674737 | 0.783490 | 0.674737 | 0.661324 |\n",
    "| presupposition_possessed_definites_existence | 0.933158 | 0.937578 | 0.933158 | 0.933266 |\n",
    "| presupposition_possessed_definites_uniqueness | 0.477895 | 0.603701 | 0.477895 | 0.357054 |\n",
    "| presupposition_question_presupposition | 0.851053 | 0.871095 | 0.851053 | 0.848644 |\n",
    "| **all** | **0.730405** | **0.817392** | **0.730405** | **0.721824** |\n",
    "\n",
    "\n",
    "\n",
    "With a total F1 score of 0.730405 with grok-3-mini. Let's try to optimize the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8992aa33876994",
   "metadata": {},
   "source": [
    "## Optimizing the model\n",
    "We are going to try optimize the model in a couple ways.\n",
    "we will first create a dev\\test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61f1096174010d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:53.505361Z",
     "start_time": "2025-07-31T20:35:53.488812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set size: 81\n",
      "Test set size: 17019\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "rng = random.default_rng(42)\n",
    "\n",
    "def stratified_split(df, n_per_section=10):\n",
    "    \"\"\"Split data keeping n examples per section for dev set.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        n_per_section: Number of examples to keep per section for dev set\n",
    "    \"\"\"\n",
    "    idx_dev = []\n",
    "    for (sec, lab), g in df.groupby([\"section\",\"gold_label\"]):\n",
    "        n = min(len(g), n_per_section // 3)  # Divide by 3 to account for label classes\n",
    "        idx = rng.permutation(g.index)\n",
    "        idx_dev.extend(idx[:n])\n",
    "    dev = df.loc[idx_dev]\n",
    "    test = df.drop(idx_dev)\n",
    "    return dev, test\n",
    "\n",
    "dev_df, test_df = stratified_split(combined_df)\n",
    "print(f\"Dev set size: {len(dev_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c40424bea1b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:53.563211Z",
     "start_time": "2025-07-31T20:35:53.554165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>All eight women that compel libraries to appre...</td>\n",
       "      <td>There are exactly eight women that compel libr...</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1795e</td>\n",
       "      <td>94</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Have the six guests that had badgered a lot of...</td>\n",
       "      <td>There are exactly six guests that had badgered...</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>500e</td>\n",
       "      <td>26</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>Do all eight women that compel libraries to ap...</td>\n",
       "      <td>There are exactly eight women that compel libr...</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1792e</td>\n",
       "      <td>94</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>Do all ten cashiers who weren't running around...</td>\n",
       "      <td>All ten cashiers who weren't running around th...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>567n</td>\n",
       "      <td>29</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>All four doors that open might have flung open.</td>\n",
       "      <td>There are exactly four mouths that open.</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1113n</td>\n",
       "      <td>58</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16912</th>\n",
       "      <td>Marla finds out why these waitresses have reta...</td>\n",
       "      <td>A lot of teachers have retaliated.</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1712n</td>\n",
       "      <td>90</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15824</th>\n",
       "      <td>Had Mark figured out where Monet sells sweaters?</td>\n",
       "      <td>Mark has figured out where Monet sells sweaters.</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>624n</td>\n",
       "      <td>32</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15910</th>\n",
       "      <td>Does Ruth conceal why Derek figures out who mu...</td>\n",
       "      <td>Derek doesn't figure out who murmurs.</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>710c</td>\n",
       "      <td>37</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15707</th>\n",
       "      <td>If Tina does remember when Anne had bored Debr...</td>\n",
       "      <td>Anne hadn't bored Debra.</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>507c</td>\n",
       "      <td>26</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15462</th>\n",
       "      <td>Samuel doesn't forget about when those doctors...</td>\n",
       "      <td>Samuel does forget about when those doctors bake.</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>262c</td>\n",
       "      <td>13</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "1795   All eight women that compel libraries to appre...   \n",
       "500    Have the six guests that had badgered a lot of...   \n",
       "1792   Do all eight women that compel libraries to ap...   \n",
       "567    Do all ten cashiers who weren't running around...   \n",
       "1113     All four doors that open might have flung open.   \n",
       "...                                                  ...   \n",
       "16912  Marla finds out why these waitresses have reta...   \n",
       "15824   Had Mark figured out where Monet sells sweaters?   \n",
       "15910  Does Ruth conceal why Derek figures out who mu...   \n",
       "15707  If Tina does remember when Anne had bored Debr...   \n",
       "15462  Samuel doesn't forget about when those doctors...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "1795   There are exactly eight women that compel libr...           modal   \n",
       "500    There are exactly six guests that had badgered...   interrogative   \n",
       "1792   There are exactly eight women that compel libr...   interrogative   \n",
       "567    All ten cashiers who weren't running around th...  Not_In_Example   \n",
       "1113            There are exactly four mouths that open.           modal   \n",
       "...                                                  ...             ...   \n",
       "16912                 A lot of teachers have retaliated.      unembedded   \n",
       "15824   Mark has figured out where Monet sells sweaters.  Not_In_Example   \n",
       "15910              Derek doesn't figure out who murmurs.   interrogative   \n",
       "15707                           Anne hadn't bored Debra.     conditional   \n",
       "15462  Samuel does forget about when those doctors bake.  Not_In_Example   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "1795   Not_In_Example  Not_In_Example        positive           0   \n",
       "500    Not_In_Example  Not_In_Example        positive           0   \n",
       "1792   Not_In_Example  Not_In_Example        positive           0   \n",
       "567     interrogative      unembedded  Not_In_Example           1   \n",
       "1113   Not_In_Example  Not_In_Example         neutral           1   \n",
       "...               ...             ...             ...         ...   \n",
       "16912  Not_In_Example  Not_In_Example         neutral           1   \n",
       "15824   interrogative      unembedded  Not_In_Example           1   \n",
       "15910  Not_In_Example  Not_In_Example         negated           2   \n",
       "15707  Not_In_Example  Not_In_Example         negated           2   \n",
       "15462         negated      unembedded  Not_In_Example           2   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "1795      all_n_presupposition  1795e          94   \n",
       "500       all_n_presupposition   500e          26   \n",
       "1792      all_n_presupposition  1792e          94   \n",
       "567       all_n_presupposition   567n          29   \n",
       "1113      all_n_presupposition  1113n          58   \n",
       "...                        ...    ...         ...   \n",
       "16912  question_presupposition  1712n          90   \n",
       "15824  question_presupposition   624n          32   \n",
       "15910  question_presupposition   710c          37   \n",
       "15707  question_presupposition   507c          26   \n",
       "15462  question_presupposition   262c          13   \n",
       "\n",
       "                                      section  \n",
       "1795      presupposition_all_n_presupposition  \n",
       "500       presupposition_all_n_presupposition  \n",
       "1792      presupposition_all_n_presupposition  \n",
       "567       presupposition_all_n_presupposition  \n",
       "1113      presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "16912  presupposition_question_presupposition  \n",
       "15824  presupposition_question_presupposition  \n",
       "15910  presupposition_question_presupposition  \n",
       "15707  presupposition_question_presupposition  \n",
       "15462  presupposition_question_presupposition  \n",
       "\n",
       "[81 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>0e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten senators that proved to ...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>2n</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>3e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>4c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17095</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>Travel shocks Janet.</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1895n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17096</th>\n",
       "      <td>The actors didn't conceal where that mall shoc...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1896c</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17097</th>\n",
       "      <td>Did the actors conceal where that mall shocks ...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1897n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17098</th>\n",
       "      <td>The actors might have concealed where that mal...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>modal</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1898n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17099</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>conditional</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1899n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17019 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0      All ten guys that proved to boast were divorcing.   \n",
       "1      All ten guys that proved to boast were divorcing.   \n",
       "2      All ten guys that proved to boast were divorcing.   \n",
       "3      All ten guys that proved to boast weren't divo...   \n",
       "4      All ten guys that proved to boast weren't divo...   \n",
       "...                                                  ...   \n",
       "17095  If the actors do conceal where that mall shock...   \n",
       "17096  The actors didn't conceal where that mall shoc...   \n",
       "17097  Did the actors conceal where that mall shocks ...   \n",
       "17098  The actors might have concealed where that mal...   \n",
       "17099  If the actors do conceal where that mall shock...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "0       There are exactly ten guys that proved to boast.      unembedded   \n",
       "1      There are exactly eleven guys that proved to b...      unembedded   \n",
       "2      There are exactly ten senators that proved to ...      unembedded   \n",
       "3       There are exactly ten guys that proved to boast.         negated   \n",
       "4      There are exactly eleven guys that proved to b...         negated   \n",
       "...                                                  ...             ...   \n",
       "17095                               Travel shocks Janet.     conditional   \n",
       "17096  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17097  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17098  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17099  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "0      Not_In_Example  Not_In_Example        positive           0   \n",
       "1      Not_In_Example  Not_In_Example         negated           2   \n",
       "2      Not_In_Example  Not_In_Example         neutral           1   \n",
       "3      Not_In_Example  Not_In_Example        positive           0   \n",
       "4      Not_In_Example  Not_In_Example         negated           2   \n",
       "...               ...             ...             ...         ...   \n",
       "17095  Not_In_Example  Not_In_Example         neutral           1   \n",
       "17096         negated      unembedded  Not_In_Example           2   \n",
       "17097   interrogative      unembedded  Not_In_Example           1   \n",
       "17098           modal      unembedded  Not_In_Example           1   \n",
       "17099     conditional      unembedded  Not_In_Example           1   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "0         all_n_presupposition     0e           0   \n",
       "1         all_n_presupposition     1c           0   \n",
       "2         all_n_presupposition     2n           0   \n",
       "3         all_n_presupposition     3e           0   \n",
       "4         all_n_presupposition     4c           0   \n",
       "...                        ...    ...         ...   \n",
       "17095  question_presupposition  1895n          99   \n",
       "17096  question_presupposition  1896c          99   \n",
       "17097  question_presupposition  1897n          99   \n",
       "17098  question_presupposition  1898n          99   \n",
       "17099  question_presupposition  1899n          99   \n",
       "\n",
       "                                      section  \n",
       "0         presupposition_all_n_presupposition  \n",
       "1         presupposition_all_n_presupposition  \n",
       "2         presupposition_all_n_presupposition  \n",
       "3         presupposition_all_n_presupposition  \n",
       "4         presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "17095  presupposition_question_presupposition  \n",
       "17096  presupposition_question_presupposition  \n",
       "17097  presupposition_question_presupposition  \n",
       "17098  presupposition_question_presupposition  \n",
       "17099  presupposition_question_presupposition  \n",
       "\n",
       "[17019 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(dev_df))\n",
    "display(pd.DataFrame(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72daff261fe243f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:54.186205Z",
     "start_time": "2025-07-31T20:35:53.623220Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_examples(df):\n",
    "    return [dspy.Example(\n",
    "        premise=r.premise, hypothesis=r.hypothesis,\n",
    "        label=label_names[r.gold_label]\n",
    "    ).with_inputs(\"premise\",\"hypothesis\") for r in df.itertuples()]\n",
    "dev_ex  = to_examples(dev_df)\n",
    "test_ex = to_examples(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aea4441ccc9fc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:54.204836Z",
     "start_time": "2025-07-31T20:35:54.202644Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    results = Evaluate(\n",
    "        devset=test_ex[:120],  # Limit to 500 for faster evaluation\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=20,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    "    )(model)\n",
    "    score,results = results\n",
    "    print(f\"Score:\\t{score}\")\n",
    "    test_pred = [label2id[out[1].label] for out in results]\n",
    "    return score, results, test_pred\n",
    "\n",
    "def compute_matrices(test_pred):\n",
    "    prf = metric_prf.compute(predictions=test_pred, references=y_true[:120], average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=test_pred, references=y_true[:120])\n",
    "    return {**prf, **accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec84683f7362d93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:59.168528Z",
     "start_time": "2025-07-31T20:35:54.258841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12435.00 / 17019 (73.1%): 100%|██████████| 17019/17019 [00:13<00:00, 1304.49it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:38:20 INFO dspy.evaluate.evaluate: Average Metric: 12435 / 17019 (73.1%)\n"
     ]
    }
   ],
   "source": [
    "predictor_test_predictions = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    ")(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfea0647d46d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:01.852911Z",
     "start_time": "2025-07-31T20:36:01.845253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 73.07\n"
     ]
    }
   ],
   "source": [
    "score,predictor_test_predictions_results = predictor_test_predictions\n",
    "print(f\"Score: {score}\")\n",
    "predictor_test_pred = [label2id[out[1].label] for out in predictor_test_predictions_results]\n",
    "y_true = [label2id[ex.label]  for ex in test_ex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24acf6b7d33c6e07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:01.965259Z",
     "start_time": "2025-07-31T20:36:01.915311Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_prf = metric_prf.compute(predictions=predictor_test_pred, references=y_true, average=\"weighted\")\n",
    "predictor_accuracy = acc.compute(predictions=predictor_test_pred, references=y_true)\n",
    "predictor_combined = {**predictor_prf, **predictor_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e5b6b99f149eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:01.989135Z",
     "start_time": "2025-07-31T20:36:01.984840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original predictor</th>\n",
       "      <td>0.817396</td>\n",
       "      <td>0.730654</td>\n",
       "      <td>0.722072</td>\n",
       "      <td>0.730654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    precision    recall        f1  accuracy\n",
       "Original predictor   0.817396  0.730654  0.722072  0.730654"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(predictor_combined, index=[\"Original predictor\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bc4a21287ce75",
   "metadata": {},
   "source": [
    "In our code we saw a F1 score of 0.722009 on the test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77973717f459d0b3",
   "metadata": {},
   "source": [
    "### Simply few-shot strategy over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1318dbc67f041c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:08.222077Z",
     "start_time": "2025-07-31T20:36:02.046845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 23/81 [00:00<00:00, 80.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 20 full traces after 23 examples for up to 1 rounds, amounting to 23 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "bs = BootstrapFewShot(metric=accuracy_metric, max_bootstrapped_demos=20, max_labeled_demos=16)\n",
    "overall_optimized = bs.compile(student=predictor, trainset=dev_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6030edd5815279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:22.099046Z",
     "start_time": "2025-07-31T20:36:08.241009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 120.00 / 120 (100.0%): 100%|██████████| 120/120 [00:00<00:00, 370.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:38:32 INFO dspy.evaluate.evaluate: Average Metric: 120 / 120 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:\t100.0\n"
     ]
    }
   ],
   "source": [
    "# 3. Evaluate\n",
    "overall_report = evaluate(overall_optimized)\n",
    "overall_score, overall_results, overall_test_pred = overall_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7499fb9b43a28e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:22.255384Z",
     "start_time": "2025-07-31T20:36:22.251422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall_optimized predictor</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             precision  recall   f1  accuracy\n",
       "Overall_optimized predictor        1.0     1.0  1.0       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "overall_combined = compute_matrices(overall_test_pred)\n",
    "display(pd.DataFrame(overall_combined, index=[\"Overall_optimized predictor\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3196a2632c1f64",
   "metadata": {},
   "source": [
    "When testing the overall model, we saw F1 Score of 0.758346, an improvement of 6.087%!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03933fa38856e3",
   "metadata": {},
   "source": [
    "### Adaptive few-shot strategy\n",
    "We will now try to optimize for each section and create a new model which will predicate by majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6539e3ac2fee22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:48.644429Z",
     "start_time": "2025-07-31T13:55:48.592251Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_dev_ex = {sec: to_examples(group) for sec, group in dev_df.groupby(\"section\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfbe1173b8e658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:50.395721Z",
     "start_time": "2025-07-31T13:55:48.667649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for section: presupposition_all_n_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:00<00:00, 78.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n",
      "Optimizing for section: presupposition_both_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 81.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_change_of_state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 84.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_cleft_existence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 83.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_cleft_uniqueness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 91.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_only_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 80.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_possessed_definites_existence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:00<00:00, 92.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n",
      "Optimizing for section: presupposition_possessed_definites_uniqueness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 88.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_question_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:00<00:00, 81.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_pipelines = {}\n",
    "\n",
    "for sec in sec_dev_ex:\n",
    "    print(f\"Optimizing for section: {sec}\")\n",
    "    # Flatten dev examples for prompt tuning\n",
    "    dev_set = sec_dev_ex[sec]\n",
    "\n",
    "    # Initialize optimizer\n",
    "    bs = BootstrapFewShot(\n",
    "        metric=accuracy_metric,\n",
    "        max_bootstrapped_demos=8,\n",
    "        max_labeled_demos=4\n",
    "    )\n",
    "\n",
    "    # Compile and tune using dev split\n",
    "    compiled = bs.compile(\n",
    "        student=predictor,\n",
    "        trainset=dev_set\n",
    "    )\n",
    "    optimized_pipelines[sec] = compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e76930f5c4558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:50.399127009Z",
     "start_time": "2025-07-23T16:05:04.050005Z"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_tp = dspy.Ensemble(reduce_fn=dspy.majority)\n",
    "adaptive_optimized = ensemble_tp.compile(list(optimized_pipelines.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d08aed1459bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:50.399739464Z",
     "start_time": "2025-07-23T16:05:04.112934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 114.00 / 120 (95.0%): 100%|██████████| 120/120 [00:04<00:00, 26.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:40:07 INFO dspy.evaluate.evaluate: Average Metric: 114 / 120 (95.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:\t95.0\n"
     ]
    }
   ],
   "source": [
    "adaptive_report = evaluate(adaptive_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca3387c359351f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:50.407770259Z",
     "start_time": "2025-07-23T17:44:38.474610Z"
    }
   },
   "outputs": [],
   "source": [
    "adaptive_score,adaptive_report_results,adaptive_test_pred = adaptive_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d148a01fdfb84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:50.409530486Z",
     "start_time": "2025-07-23T17:44:40.076006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adaptive_optimized predictor</th>\n",
       "      <td>0.955455</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.949911</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              precision  recall        f1  accuracy\n",
       "Adaptive_optimized predictor   0.955455    0.95  0.949911      0.95"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adaptive_combined = compute_matrices(adaptive_test_pred)\n",
    "display(pd.DataFrame(adaptive_combined, index=[\"Adaptive_optimized predictor\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a03cf39db51171",
   "metadata": {},
   "source": [
    "We got:\n",
    "0.821135,0.744834,0.738229,0.744834\n",
    "this shows around 0.02 improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e408e95c79e5bb1",
   "metadata": {},
   "source": [
    "### Few shots with Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8288e547ee09a9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:38:27.331641Z",
     "start_time": "2025-07-31T20:36:22.321419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 20 traces per predictor.\n",
      "Will attempt to bootstrap 1 candidate sets.\n",
      "Average Metric: 53.00 / 81 (65.4%): 100%|██████████| 81/81 [00:00<00:00, 3140.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:40:09 INFO dspy.evaluate.evaluate: Average Metric: 53 / 81 (65.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 65.43 for seed -3\n",
      "Scores so far: [65.43]\n",
      "Best score so far: 65.43\n",
      "Average Metric: 61.00 / 81 (75.3%): 100%|██████████| 81/81 [00:00<00:00, 615.88it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:40:11 INFO dspy.evaluate.evaluate: Average Metric: 61 / 81 (75.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 75.31 for seed -2\n",
      "Scores so far: [65.43, 75.31]\n",
      "Best score so far: 75.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 23/81 [00:00<00:00, 87.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 20 full traces after 23 examples for up to 1 rounds, amounting to 23 attempts.\n",
      "Average Metric: 53.00 / 81 (65.4%): 100%|██████████| 81/81 [00:00<00:00, 455.39it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:40:22 INFO dspy.evaluate.evaluate: Average Metric: 53 / 81 (65.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [65.43, 75.31, 65.43]\n",
      "Best score so far: 75.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 21/81 [00:00<00:00, 85.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 13 full traces after 21 examples for up to 1 rounds, amounting to 21 attempts.\n",
      "Average Metric: 55.00 / 81 (67.9%): 100%|██████████| 81/81 [00:00<00:00, 569.49it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:40:33 INFO dspy.evaluate.evaluate: Average Metric: 55 / 81 (67.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [65.43, 75.31, 65.43, 67.9]\n",
      "Best score so far: 75.31\n",
      "4 candidate programs found.\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "bsrs = BootstrapFewShotWithRandomSearch(\n",
    "    metric=accuracy_metric,\n",
    "    max_bootstrapped_demos=20,\n",
    "    max_labeled_demos=9,\n",
    "    num_candidate_programs=1,\n",
    "    num_threads=30\n",
    ")\n",
    "opted_rs = bsrs.compile(student=predictor, trainset=dev_ex, valset=dev_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aade7a8f2e5f2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:48:51.620718Z",
     "start_time": "2025-07-31T20:38:31.572843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 115.00 / 120 (95.8%): 100%|██████████| 120/120 [00:00<00:00, 802.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:40:34 INFO dspy.evaluate.evaluate: Average Metric: 115 / 120 (95.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:\t95.83\n"
     ]
    }
   ],
   "source": [
    "opted_rs_report = evaluate(opted_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c6cd63229607e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:48:51.672843Z",
     "start_time": "2025-07-31T20:48:51.663071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>opted_rs[_optimized predictor</th>\n",
       "      <td>0.962191</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958355</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               precision    recall        f1  accuracy\n",
       "opted_rs[_optimized predictor   0.962191  0.958333  0.958355  0.958333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opted_rs_score,opted_rs_report_results,opted_rs_test_pred= opted_rs_report\n",
    "opted_rs_combined = compute_matrices(opted_rs_test_pred)\n",
    "display(pd.DataFrame(opted_rs_combined, index=[\"opted_rs[_optimized predictor\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a46f92ab6043c",
   "metadata": {},
   "source": [
    "## Optimizing with MIPRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dffca042122d056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:39:04.597544Z",
     "start_time": "2025-07-31T20:59:53.434829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/08/02 14:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/08/02 14:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=12 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/12\n",
      "Bootstrapping set 2/12\n",
      "Bootstrapping set 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8/17 [00:00<00:00, 94.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n",
      "Bootstrapping set 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3/17 [00:00<00:00, 94.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 4/17 [00:00<00:00, 76.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Bootstrapping set 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3/17 [00:00<00:00, 92.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3/17 [00:00<00:00, 98.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 5/17 [00:00<00:00, 94.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n",
      "Bootstrapping set 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 6/17 [00:00<00:00, 82.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 6 examples for up to 1 rounds, amounting to 6 attempts.\n",
      "Bootstrapping set 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 6/17 [00:00<00:00, 75.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 6 examples for up to 1 rounds, amounting to 6 attempts.\n",
      "Bootstrapping set 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2/17 [00:00<00:00, 62.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Bootstrapping set 12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3/17 [00:00<00:00, 73.91it/s]\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=12 instructions...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Error getting source code: unhashable type: 'dict'.\n",
      "\n",
      "Running without program aware proposer.\n",
      "DATA SUMMARY: This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: none\n",
      "task_demos No task demos provided.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.332066]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "No task demos provided.\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "No task demos provided.\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "You are an expert in natural language inference (NLI). Given a premise and a hypothesis, analyze their logical relationship and classify it as one of the following: \n",
      "- **Entailment**: The hypothesis is fully supported and logically follows from the premise.\n",
      "- **Neutral**: The premise does not provide enough information to confirm or deny the hypothesis; it is neither supported nor contradicted.\n",
      "- **Contradiction**: The hypothesis directly conflicts with the premise.\n",
      "\n",
      "Pay close attention to precise quantification (e.g., exact numbers or quantities), complex sentence structures (e.g., questions, negations, and conditional statements), and subtle logical nuances in everyday scenarios. Reason step by step to ensure accurate classification, breaking down the premise and hypothesis to evaluate their relationships thoroughly.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: You are an expert in natural language inference (NLI). Given a premise and a hypothesis, analyze their logical relationship and classify it as one of the following: \n",
      "- **Entailment**: The hypothesis is fully supported and logically follows from the premise.\n",
      "- **Neutral**: The premise does not provide enough information to confirm or deny the hypothesis; it is neither supported nor contradicted.\n",
      "- **Contradiction**: The hypothesis directly conflicts with the premise.\n",
      "\n",
      "Pay close attention to precise quantification (e.g., exact numbers or quantities), complex sentence structures (e.g., questions, negations, and conditional statements), and subtle logical nuances in everyday scenarios. Reason step by step to ensure accurate classification, breaking down the premise and hypothesis to evaluate their relationships thoroughly.\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: description\n",
      "task_demos Premise: All eight women that compel libraries to appreciate an actress might like the skirt.\n",
      "Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "Label: entailment\n",
      "\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment\n",
      "\n",
      "Premise: Do all eight women that compel libraries to appreciate an actress like the skirt?\n",
      "Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "Label: entailment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.351871]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "4. `tip` (str): A suggestion for how to go about generating the new instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "{tip}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: All eight women that compel libraries to appreciate an actress might like the skirt.\n",
      "Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "Label: entailment\n",
      "\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment\n",
      "\n",
      "Premise: Do all eight women that compel libraries to appreciate an actress like the skirt?\n",
      "Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "Label: entailment\n",
      "\n",
      "\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "Make sure your instruction is very informative and descriptive.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "You are an advanced Natural Language Inference (NLI) classifier tasked with analyzing pairs of sentences—a premise and a hypothesis—to determine their logical relationship. The premise provides a statement or question about everyday scenarios involving people, objects, relationships, precise quantifications (e.g., \"exactly eight women\"), and complex sentence structures such as questions, negations, or intricate descriptions. Your goal is to classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis logically follows from the premise, meaning the premise supports or implies the hypothesis (e.g., if the premise states facts that directly confirm the hypothesis).\n",
      "- **Neutral**: The premise neither supports nor contradicts the hypothesis; they are unrelated or the premise provides no clear implication.\n",
      "- **Contradiction**: The hypothesis directly conflicts with or negates the premise.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Carefully evaluate the logical connections, paying special attention to numerical accuracy, structural parsing, and fine-grained details in the language. Output your classification as a single word: \"entailment\", \"neutral\", or \"contradiction\".\n",
      "\n",
      "Example:\n",
      "- Premise: All eight women that compel libraries to appreciate an actress might like the skirt.\n",
      "- Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "- Classification: entailment\n",
      "\n",
      "Ensure your reasoning is thorough, considering the context of creative and diverse language use to promote accurate AI reasoning.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: You are an advanced Natural Language Inference (NLI) classifier tasked with analyzing pairs of sentences—a premise and a hypothesis—to determine their logical relationship. The premise provides a statement or question about everyday scenarios involving people, objects, relationships, precise quantifications (e.g., \"exactly eight women\"), and complex sentence structures such as questions, negations, or intricate descriptions. Your goal is to classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis logically follows from the premise, meaning the premise supports or implies the hypothesis (e.g., if the premise states facts that directly confirm the hypothesis).\n",
      "- **Neutral**: The premise neither supports nor contradicts the hypothesis; they are unrelated or the premise provides no clear implication.\n",
      "- **Contradiction**: The hypothesis directly conflicts with or negates the premise.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Carefully evaluate the logical connections, paying special attention to numerical accuracy, structural parsing, and fine-grained details in the language. Output your classification as a single word: \"entailment\", \"neutral\", or \"contradiction\".\n",
      "\n",
      "Example:\n",
      "- Premise: All eight women that compel libraries to appreciate an actress might like the skirt.\n",
      "- Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "- Classification: entailment\n",
      "\n",
      "Ensure your reasoning is thorough, considering the context of creative and diverse language use to promote accurate AI reasoning.\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: description\n",
      "task_demos Premise: All eight women that compel libraries to appreciate an actress might like the skirt.\n",
      "Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "Label: entailment\n",
      "\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment\n",
      "\n",
      "Premise: Do all eight women that compel libraries to appreciate an actress like the skirt?\n",
      "Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "Label: entailment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.570091]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "4. `tip` (str): A suggestion for how to go about generating the new instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "{tip}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: All eight women that compel libraries to appreciate an actress might like the skirt.\n",
      "Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "Label: entailment\n",
      "\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment\n",
      "\n",
      "Premise: Do all eight women that compel libraries to appreciate an actress like the skirt?\n",
      "Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "Label: entailment\n",
      "\n",
      "\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "Make sure your instruction is very informative and descriptive.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "You are an advanced natural language inference (NLI) model tasked with analyzing pairs of sentences—a premise and a hypothesis—and determining their logical relationship. The premise provides a statement or context, while the hypothesis is another statement that may or may not align with it. Your goal is to classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis is fully and logically supported by the premise, meaning the premise implies the hypothesis is true.\n",
      "- **Neutral**: The premise does not provide sufficient information to confirm or deny the hypothesis; it could be true or false based on the premise alone.\n",
      "- **Contradiction**: The hypothesis directly conflicts with the premise, making it false if the premise is true.\n",
      "\n",
      "This dataset emphasizes precise quantification (e.g., words like \"exactly eight\" or \"all six\"), complex sentence structures (such as questions, negations, and intricate relationships), and everyday scenarios involving people, objects, and interactions. You must carefully parse these elements to ensure accurate reasoning, focusing on numerical accuracy, structural details, and logical implications to handle diverse and creative language use effectively.\n",
      "\n",
      "For reference, here are examples of premise-hypothesis pairs and their labels:\n",
      "\n",
      "- Premise: All eight women that compel libraries to appreciate an actress might like the skirt.  \n",
      "  Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.  \n",
      "  Label: entailment (The premise implies the exact number mentioned.)\n",
      "\n",
      "- Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?  \n",
      "  Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.  \n",
      "  Label: entailment (The premise suggests the exact quantity.)\n",
      "\n",
      "- Premise: Do all eight women that compel libraries to appreciate an actress like the skirt?  \n",
      "  Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.  \n",
      "  Label: entailment (The premise confirms the exact group.)\n",
      "\n",
      "When given a new premise and hypothesis, analyze them step by step: First, identify key elements like quantities, negations, and relationships. Then, evaluate if the hypothesis follows from, is unrelated to, or contradicts the premise. Finally, output only the label in the following format: \"Label: [entailment/neutral/contradiction]\". Ensure your reasoning is thorough and precise to promote balanced and reliable AI performance on this task.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: You are an advanced natural language inference (NLI) model tasked with analyzing pairs of sentences—a premise and a hypothesis—and determining their logical relationship. The premise provides a statement or context, while the hypothesis is another statement that may or may not align with it. Your goal is to classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis is fully and logically supported by the premise, meaning the premise implies the hypothesis is true.\n",
      "- **Neutral**: The premise does not provide sufficient information to confirm or deny the hypothesis; it could be true or false based on the premise alone.\n",
      "- **Contradiction**: The hypothesis directly conflicts with the premise, making it false if the premise is true.\n",
      "\n",
      "This dataset emphasizes precise quantification (e.g., words like \"exactly eight\" or \"all six\"), complex sentence structures (such as questions, negations, and intricate relationships), and everyday scenarios involving people, objects, and interactions. You must carefully parse these elements to ensure accurate reasoning, focusing on numerical accuracy, structural details, and logical implications to handle diverse and creative language use effectively.\n",
      "\n",
      "For reference, here are examples of premise-hypothesis pairs and their labels:\n",
      "\n",
      "- Premise: All eight women that compel libraries to appreciate an actress might like the skirt.  \n",
      "  Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.  \n",
      "  Label: entailment (The premise implies the exact number mentioned.)\n",
      "\n",
      "- Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?  \n",
      "  Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.  \n",
      "  Label: entailment (The premise suggests the exact quantity.)\n",
      "\n",
      "- Premise: Do all eight women that compel libraries to appreciate an actress like the skirt?  \n",
      "  Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.  \n",
      "  Label: entailment (The premise confirms the exact group.)\n",
      "\n",
      "When given a new premise and hypothesis, analyze them step by step: First, identify key elements like quantities, negations, and relationships. Then, evaluate if the hypothesis follows from, is unrelated to, or contradicts the premise. Finally, output only the label in the following format: \"Label: [entailment/neutral/contradiction]\". Ensure your reasoning is thorough and precise to promote balanced and reliable AI performance on this task.\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: none\n",
      "task_demos Premise: Do all ten cashiers who weren't running around this school need to bring the lamp?\n",
      "Hypothesis: All ten cashiers who weren't running around this school do need to bring the lamp.\n",
      "Label: neutral\n",
      "\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment\n",
      "\n",
      "Premise: The two shoes that aren't bothering Rachelle haven't soaked.\n",
      "Hypothesis: The two shoes that aren't bothering Rachelle have soaked.\n",
      "Label: contradiction\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.590511]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: Do all ten cashiers who weren't running around this school need to bring the lamp?\n",
      "Hypothesis: All ten cashiers who weren't running around this school do need to bring the lamp.\n",
      "Label: neutral\n",
      "\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment\n",
      "\n",
      "Premise: The two shoes that aren't bothering Rachelle haven't soaked.\n",
      "Hypothesis: The two shoes that aren't bothering Rachelle have soaked.\n",
      "Label: contradiction\n",
      "\n",
      "\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "You are an expert in Natural Language Inference (NLI). Given a premise and a hypothesis, analyze their logical relationship by considering precise quantification (e.g., exact numbers), complex sentence structures (e.g., questions, negations), and everyday scenarios. Determine and output one of the following labels: \n",
      "- 'entailment' if the hypothesis logically follows from the premise,\n",
      "- 'neutral' if the hypothesis is neither entailed nor contradicted by the premise,\n",
      "- 'contradiction' if the hypothesis is inconsistent with the premise.\n",
      "\n",
      "For each classification, reason step by step to demonstrate your understanding of the relationships, ensuring attention to numerical accuracy and structural details.\n",
      "\n",
      "Example 1:\n",
      "Premise: Do all ten cashiers who weren't running around this school need to bring the lamp?\n",
      "Hypothesis: All ten cashiers who weren't running around this school do need to bring the lamp.\n",
      "Label: neutral (The premise is a question and does not definitively state the need, so it's neither fully entailed nor contradicted.)\n",
      "\n",
      "Example 2:\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment (The premise implies the existence of exactly six guests with the described actions.)\n",
      "\n",
      "Example 3:\n",
      "Premise: The two shoes that aren't bothering Rachelle haven't soaked.\n",
      "Hypothesis: The two shoes that aren't bothering Rachelle have soaked.\n",
      "Label: contradiction (The premise states the shoes haven't soaked, directly opposing the hypothesis.)\n",
      "\n",
      "Now, for the input premise and hypothesis, provide your classification and reasoning.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: You are an expert in Natural Language Inference (NLI). Given a premise and a hypothesis, analyze their logical relationship by considering precise quantification (e.g., exact numbers), complex sentence structures (e.g., questions, negations), and everyday scenarios. Determine and output one of the following labels: \n",
      "- 'entailment' if the hypothesis logically follows from the premise,\n",
      "- 'neutral' if the hypothesis is neither entailed nor contradicted by the premise,\n",
      "- 'contradiction' if the hypothesis is inconsistent with the premise.\n",
      "\n",
      "For each classification, reason step by step to demonstrate your understanding of the relationships, ensuring attention to numerical accuracy and structural details.\n",
      "\n",
      "Example 1:\n",
      "Premise: Do all ten cashiers who weren't running around this school need to bring the lamp?\n",
      "Hypothesis: All ten cashiers who weren't running around this school do need to bring the lamp.\n",
      "Label: neutral (The premise is a question and does not definitively state the need, so it's neither fully entailed nor contradicted.)\n",
      "\n",
      "Example 2:\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment (The premise implies the existence of exactly six guests with the described actions.)\n",
      "\n",
      "Example 3:\n",
      "Premise: The two shoes that aren't bothering Rachelle haven't soaked.\n",
      "Hypothesis: The two shoes that aren't bothering Rachelle have soaked.\n",
      "Label: contradiction (The premise states the shoes haven't soaked, directly opposing the hypothesis.)\n",
      "\n",
      "Now, for the input premise and hypothesis, provide your classification and reasoning.\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: simple\n",
      "task_demos Premise: Both customers that did hug practiced.\n",
      "Hypothesis: There are exactly two customers that did hug\n",
      "Label: entailment\n",
      "\n",
      "Premise: All four doors that open might have flung open.\n",
      "Hypothesis: There are exactly four mouths that open.\n",
      "Label: neutral\n",
      "\n",
      "Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.620724]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "4. `tip` (str): A suggestion for how to go about generating the new instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "{tip}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: Both customers that did hug practiced.\n",
      "Hypothesis: There are exactly two customers that did hug\n",
      "Label: entailment\n",
      "\n",
      "Premise: All four doors that open might have flung open.\n",
      "Hypothesis: There are exactly four mouths that open.\n",
      "Label: neutral\n",
      "\n",
      "Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "Keep the instruction clear and concise.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "Given a premise and a hypothesis, classify their logical relationship as 'entailment', 'neutral', or 'contradiction'.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: Given a premise and a hypothesis, classify their logical relationship as 'entailment', 'neutral', or 'contradiction'.\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: description\n",
      "task_demos Premise: If both skateboards that shouldn't shatter are rolling, it's okay.\n",
      "Hypothesis: There are exactly two skateboards that shouldn't shatter\n",
      "Label: entailment\n",
      "\n",
      "Premise: Both mushrooms that haven't charred have blackened.\n",
      "Hypothesis: There are exactly three mushrooms that haven't charred.\n",
      "Label: contradiction\n",
      "\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.641458]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "4. `tip` (str): A suggestion for how to go about generating the new instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "{tip}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: If both skateboards that shouldn't shatter are rolling, it's okay.\n",
      "Hypothesis: There are exactly two skateboards that shouldn't shatter\n",
      "Label: entailment\n",
      "\n",
      "Premise: Both mushrooms that haven't charred have blackened.\n",
      "Hypothesis: There are exactly three mushrooms that haven't charred.\n",
      "Label: contradiction\n",
      "\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment\n",
      "\n",
      "\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "Make sure your instruction is very informative and descriptive.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "As an expert in natural language inference (NLI), your task is to evaluate pairs of sentences—a premise and a hypothesis—and determine their logical relationship. The premise provides a statement or context, while the hypothesis is another statement that may or may not follow from it. Classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: If the hypothesis logically follows from the premise, meaning the premise implies the hypothesis is true (e.g., if the premise mentions specific quantities or events that directly support the hypothesis).\n",
      "- **Contradiction**: If the hypothesis directly conflicts with the premise, making it impossible for both to be true simultaneously (e.g., due to opposing quantities, negations, or incompatible events).\n",
      "- **Neutral**: If the premise neither supports nor contradicts the hypothesis, leaving the truth of the hypothesis uncertain.\n",
      "\n",
      "This dataset focuses on precise quantification (e.g., exact numbers like \"exactly two\"), complex sentence structures (e.g., questions, negations, and intricate relationships involving people, objects, and scenarios), and fine-grained logical reasoning. Analyze each pair carefully, considering everyday contexts and ensuring balanced evaluation of numerical accuracy and structural parsing.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Output only the label: \"entailment\", \"neutral\", or \"contradiction\", based on your analysis. Here's an example for clarity:\n",
      "\n",
      "- Premise: \"If both skateboards that shouldn't shatter are rolling, it's okay.\"  \n",
      "  Hypothesis: \"There are exactly two skateboards that shouldn't shatter.\"  \n",
      "  Label: entailment\n",
      "\n",
      "- Premise: \"Both mushrooms that haven't charred have blackened.\"  \n",
      "  Hypothesis: \"There are exactly three mushrooms that haven't charred.\"  \n",
      "  Label: contradiction\n",
      "\n",
      "- Premise: \"Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\"  \n",
      "  Hypothesis: \"There are exactly six guests that had badgered a lot of analyses to shock people.\"  \n",
      "  Label: entailment\n",
      "\n",
      "Provide your response as just the single-word label for each pair.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: As an expert in natural language inference (NLI), your task is to evaluate pairs of sentences—a premise and a hypothesis—and determine their logical relationship. The premise provides a statement or context, while the hypothesis is another statement that may or may not follow from it. Classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: If the hypothesis logically follows from the premise, meaning the premise implies the hypothesis is true (e.g., if the premise mentions specific quantities or events that directly support the hypothesis).\n",
      "- **Contradiction**: If the hypothesis directly conflicts with the premise, making it impossible for both to be true simultaneously (e.g., due to opposing quantities, negations, or incompatible events).\n",
      "- **Neutral**: If the premise neither supports nor contradicts the hypothesis, leaving the truth of the hypothesis uncertain.\n",
      "\n",
      "This dataset focuses on precise quantification (e.g., exact numbers like \"exactly two\"), complex sentence structures (e.g., questions, negations, and intricate relationships involving people, objects, and scenarios), and fine-grained logical reasoning. Analyze each pair carefully, considering everyday contexts and ensuring balanced evaluation of numerical accuracy and structural parsing.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Output only the label: \"entailment\", \"neutral\", or \"contradiction\", based on your analysis. Here's an example for clarity:\n",
      "\n",
      "- Premise: \"If both skateboards that shouldn't shatter are rolling, it's okay.\"  \n",
      "  Hypothesis: \"There are exactly two skateboards that shouldn't shatter.\"  \n",
      "  Label: entailment\n",
      "\n",
      "- Premise: \"Both mushrooms that haven't charred have blackened.\"  \n",
      "  Hypothesis: \"There are exactly three mushrooms that haven't charred.\"  \n",
      "  Label: contradiction\n",
      "\n",
      "- Premise: \"Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\"  \n",
      "  Hypothesis: \"There are exactly six guests that had badgered a lot of analyses to shock people.\"  \n",
      "  Label: entailment\n",
      "\n",
      "Provide your response as just the single-word label for each pair.\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: creative\n",
      "task_demos Premise: All four doors that open might have flung open.\n",
      "Hypothesis: There are exactly four mouths that open.\n",
      "Label: neutral\n",
      "\n",
      "Premise: If both cashiers who fired Walter have donated, it's okay.\n",
      "Hypothesis: There are exactly two girls who fired Walter\n",
      "Label: neutral\n",
      "\n",
      "Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.661465]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "4. `tip` (str): A suggestion for how to go about generating the new instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "{tip}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: All four doors that open might have flung open.\n",
      "Hypothesis: There are exactly four mouths that open.\n",
      "Label: neutral\n",
      "\n",
      "Premise: If both cashiers who fired Walter have donated, it's okay.\n",
      "Hypothesis: There are exactly two girls who fired Walter\n",
      "Label: neutral\n",
      "\n",
      "Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "Don't be afraid to be creative when creating the new instruction!\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "As an advanced Natural Language Inference expert, carefully analyze the given premise and hypothesis to determine their logical relationship. Pay special attention to precise quantifiers (like \"exactly eight\"), numerical accuracy, complex sentence structures such as questions, negations, and everyday scenarios involving people, objects, or relationships. Think step-by-step: first, break down the key elements in the premise and hypothesis; second, evaluate if the hypothesis logically follows from the premise (entailment), does not affect it (neutral), or directly opposes it (contradiction); finally, output only one label: \"entailment\", \"neutral\", or \"contradiction\". Be creative in your reasoning to handle diverse and tricky language patterns for the most accurate classification.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: As an advanced Natural Language Inference expert, carefully analyze the given premise and hypothesis to determine their logical relationship. Pay special attention to precise quantifiers (like \"exactly eight\"), numerical accuracy, complex sentence structures such as questions, negations, and everyday scenarios involving people, objects, or relationships. Think step-by-step: first, break down the key elements in the premise and hypothesis; second, evaluate if the hypothesis logically follows from the premise (entailment), does not affect it (neutral), or directly opposes it (contradiction); finally, output only one label: \"entailment\", \"neutral\", or \"contradiction\". Be creative in your reasoning to handle diverse and tricky language patterns for the most accurate classification.\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: persona\n",
      "task_demos Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "Premise: If both guests who weren't arriving at all movie theaters stretched, it's okay.\n",
      "Hypothesis: There are exactly two guests who weren't arriving at all movie theaters\n",
      "Label: entailment\n",
      "\n",
      "Premise: Both customers that did hug practiced.\n",
      "Hypothesis: There are exactly two customers that did hug\n",
      "Label: entailment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.681680]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "4. `tip` (str): A suggestion for how to go about generating the new instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "{tip}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "Premise: If both guests who weren't arriving at all movie theaters stretched, it's okay.\n",
      "Hypothesis: There are exactly two guests who weren't arriving at all movie theaters\n",
      "Label: entailment\n",
      "\n",
      "Premise: Both customers that did hug practiced.\n",
      "Hypothesis: There are exactly two customers that did hug\n",
      "Label: entailment\n",
      "\n",
      "\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "Include a persona that is relevant to the task in the instruction (ie. \"You are a ...\")\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "You are an expert natural language inference analyst, specializing in evaluating logical relationships between sentences, including precise quantification, complex structures like questions and negations, and everyday scenarios. Your task is to take a premise and a hypothesis as input and classify their relationship as one of the following: 'entailment', 'neutral', or 'contradiction'. Base your classification on careful analysis of the premise's implications. Output only the label in the format: Label: [your label here].\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: You are an expert natural language inference analyst, specializing in evaluating logical relationships between sentences, including precise quantification, complex structures like questions and negations, and everyday scenarios. Your task is to take a premise and a hypothesis as input and classify their relationship as one of the following: 'entailment', 'neutral', or 'contradiction'. Base your classification on careful analysis of the premise's implications. Output only the label in the format: Label: [your label here].\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: persona\n",
      "task_demos Premise: Both governments that haven't retaliated were remembering who complained.\n",
      "Hypothesis: There are exactly two actors that haven't retaliated\n",
      "Label: neutral\n",
      "\n",
      "Premise: If both cashiers who fired Walter have donated, it's okay.\n",
      "Hypothesis: There are exactly two girls who fired Walter\n",
      "Label: neutral\n",
      "\n",
      "Premise: Did both waiters who examined some commentaries discover what will stun a lot of banks.\n",
      "Hypothesis: There aren't exactly two waiters who examined some commentaries\n",
      "Label: contradiction\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.701748]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "4. `tip` (str): A suggestion for how to go about generating the new instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "{tip}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: Both governments that haven't retaliated were remembering who complained.\n",
      "Hypothesis: There are exactly two actors that haven't retaliated\n",
      "Label: neutral\n",
      "\n",
      "Premise: If both cashiers who fired Walter have donated, it's okay.\n",
      "Hypothesis: There are exactly two girls who fired Walter\n",
      "Label: neutral\n",
      "\n",
      "Premise: Did both waiters who examined some commentaries discover what will stun a lot of banks.\n",
      "Hypothesis: There aren't exactly two waiters who examined some commentaries\n",
      "Label: contradiction\n",
      "\n",
      "\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "Include a persona that is relevant to the task in the instruction (ie. \"You are a ...\")\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: Both governments that haven't retaliated were remembering who complained.\n",
      "Hypothesis: There are exactly two actors that haven't retaliated\n",
      "Label: neutral\n",
      "\n",
      "Premise: If both cashiers who fired Walter have donated, it's okay.\n",
      "Hypothesis: There are exactly two girls who fired Walter\n",
      "Label: neutral\n",
      "\n",
      "Premise: Did both waiters who examined some commentaries discover what will stun a lot of banks.\n",
      "Hypothesis: There aren't exactly two waiters who examined some commentaries\n",
      "Label: contradiction\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "Include a persona that is relevant to the task in the instruction (ie. \"You are a ...\")\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "You are a precise natural language inference expert, specializing in analyzing logical relationships, precise quantification (such as exact numbers or counts), and complex sentence structures like questions and negations. Your task is to evaluate the relationship between a given premise and a hypothesis, classifying it as one of the following: 'entailment' (the premise logically implies the hypothesis), 'neutral' (the premise is irrelevant or unrelated to the hypothesis), or 'contradiction' (the premise logically opposes the hypothesis). For each input, you will receive a premise and a hypothesis. Respond with only the label: 'entailment', 'neutral', or 'contradiction', based on your expert analysis.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: You are a precise natural language inference expert, specializing in analyzing logical relationships, precise quantification (such as exact numbers or counts), and complex sentence structures like questions and negations. Your task is to evaluate the relationship between a given premise and a hypothesis, classifying it as one of the following: 'entailment' (the premise logically implies the hypothesis), 'neutral' (the premise is irrelevant or unrelated to the hypothesis), or 'contradiction' (the premise logically opposes the hypothesis). For each input, you will receive a premise and a hypothesis. Respond with only the label: 'entailment', 'neutral', or 'contradiction', based on your expert analysis.\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: description\n",
      "task_demos Premise: The seven patients who had preferred Lori to scan a lot of plays hadn't commissioned Nina's teachers to boycott a lot of museums.\n",
      "Hypothesis: There aren't exactly seven patients who had preferred Lori to scan a lot of plays.\n",
      "Label: contradiction\n",
      "\n",
      "Premise: Do all eight women that compel libraries to appreciate an actress like the skirt?\n",
      "Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "Label: entailment\n",
      "\n",
      "Premise: Both governments that haven't retaliated were remembering who complained.\n",
      "Hypothesis: There are exactly two actors that haven't retaliated\n",
      "Label: neutral\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.721758]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "4. `tip` (str): A suggestion for how to go about generating the new instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "{tip}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: The seven patients who had preferred Lori to scan a lot of plays hadn't commissioned Nina's teachers to boycott a lot of museums.\n",
      "Hypothesis: There aren't exactly seven patients who had preferred Lori to scan a lot of plays.\n",
      "Label: contradiction\n",
      "\n",
      "Premise: Do all eight women that compel libraries to appreciate an actress like the skirt?\n",
      "Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "Label: entailment\n",
      "\n",
      "Premise: Both governments that haven't retaliated were remembering who complained.\n",
      "Hypothesis: There are exactly two actors that haven't retaliated\n",
      "Label: neutral\n",
      "\n",
      "\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "Make sure your instruction is very informative and descriptive.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "You are a highly accurate natural language inference (NLI) classifier. Your task is to analyze a given premise and hypothesis, then classify their logical relationship as one of the following: 'entailment', 'neutral', or 'contradiction'. \n",
      "\n",
      "- **Entailment**: The hypothesis is logically true if the premise is true. For example, if the premise states a fact that directly supports the hypothesis, especially with precise quantification (e.g., \"exactly eight women\"), then this label applies.\n",
      "- **Neutral**: The premise does not provide enough information to confirm or deny the hypothesis. It may be unrelated or insufficiently specific.\n",
      "- **Contradiction**: The hypothesis is logically false if the premise is true, such as when the premise directly opposes the hypothesis through negations, numerical discrepancies, or conflicting structures.\n",
      "\n",
      "This dataset focuses on everyday scenarios involving people, objects, and relationships, often featuring complex sentence structures like questions, negations, and precise numerical details (e.g., \"exactly seven patients\"). Pay close attention to these elements to ensure accurate reasoning, as the goal is to test fine-grained entailment and promote balanced handling of creative language use.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Output only the label ('entailment', 'neutral', or 'contradiction') based on your analysis, without additional explanation.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: You are a highly accurate natural language inference (NLI) classifier. Your task is to analyze a given premise and hypothesis, then classify their logical relationship as one of the following: 'entailment', 'neutral', or 'contradiction'. \n",
      "\n",
      "- **Entailment**: The hypothesis is logically true if the premise is true. For example, if the premise states a fact that directly supports the hypothesis, especially with precise quantification (e.g., \"exactly eight women\"), then this label applies.\n",
      "- **Neutral**: The premise does not provide enough information to confirm or deny the hypothesis. It may be unrelated or insufficiently specific.\n",
      "- **Contradiction**: The hypothesis is logically false if the premise is true, such as when the premise directly opposes the hypothesis through negations, numerical discrepancies, or conflicting structures.\n",
      "\n",
      "This dataset focuses on everyday scenarios involving people, objects, and relationships, often featuring complex sentence structures like questions, negations, and precise numerical details (e.g., \"exactly seven patients\"). Pay close attention to these elements to ensure accurate reasoning, as the goal is to test fine-grained entailment and promote balanced handling of creative language use.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Output only the label ('entailment', 'neutral', or 'contradiction') based on your analysis, without additional explanation.\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: persona\n",
      "task_demos Premise: If the five waiters that approached Paul depart, it's okay.\n",
      "Hypothesis: The five waiters that approached Paul depart.\n",
      "Label: neutral\n",
      "\n",
      "Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.741641]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "4. `tip` (str): A suggestion for how to go about generating the new instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "{tip}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: If the five waiters that approached Paul depart, it's okay.\n",
      "Hypothesis: The five waiters that approached Paul depart.\n",
      "Label: neutral\n",
      "\n",
      "Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "Include a persona that is relevant to the task in the instruction (ie. \"You are a ...\")\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "You are an expert natural language inference (NLI) specialist, skilled in analyzing logical relationships in complex sentences involving precise quantification, numerical accuracy, negations, and everyday scenarios. Your task is to take a premise and a hypothesis as input and determine their logical relationship, classifying it as one of the following: 'entailment' (the hypothesis follows directly from the premise), 'neutral' (the premise neither supports nor contradicts the hypothesis), or 'contradiction' (the hypothesis conflicts with the premise). Provide your classification clearly and concisely.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: You are an expert natural language inference (NLI) specialist, skilled in analyzing logical relationships in complex sentences involving precise quantification, numerical accuracy, negations, and everyday scenarios. Your task is to take a premise and a hypothesis as input and determine their logical relationship, classifying it as one of the following: 'entailment' (the hypothesis follows directly from the premise), 'neutral' (the premise neither supports nor contradicts the hypothesis), or 'contradiction' (the hypothesis conflicts with the premise). Provide your classification clearly and concisely.\n",
      "Using a randomly generated configuration for our grounded proposer.\n",
      "Selected tip: description\n",
      "task_demos Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "Premise: Do all ten cashiers who weren't running around this school need to bring the lamp?\n",
      "Hypothesis: All ten cashiers who weren't running around this school do need to bring the lamp.\n",
      "Label: neutral\n",
      "\n",
      "Premise: Did both waiters who examined some commentaries discover what will stun a lot of banks.\n",
      "Hypothesis: There aren't exactly two waiters who examined some commentaries\n",
      "Label: contradiction\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 0: A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 1: You are an advanced Natural Language Inference (NLI) classifier tasked with analyzing pairs of sentences—a premise and a hypothesis—to determine their logical relationship. The premise provides a statement or question about everyday scenarios involving people, objects, relationships, precise quantifications (e.g., \"exactly eight women\"), and complex sentence structures such as questions, negations, or intricate descriptions. Your goal is to classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis logically follows from the premise, meaning the premise supports or implies the hypothesis (e.g., if the premise states facts that directly confirm the hypothesis).\n",
      "- **Neutral**: The premise neither supports nor contradicts the hypothesis; they are unrelated or the premise provides no clear implication.\n",
      "- **Contradiction**: The hypothesis directly conflicts with or negates the premise.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Carefully evaluate the logical connections, paying special attention to numerical accuracy, structural parsing, and fine-grained details in the language. Output your classification as a single word: \"entailment\", \"neutral\", or \"contradiction\".\n",
      "\n",
      "Example:\n",
      "- Premise: All eight women that compel libraries to appreciate an actress might like the skirt.\n",
      "- Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "- Classification: entailment\n",
      "\n",
      "Ensure your reasoning is thorough, considering the context of creative and diverse language use to promote accurate AI reasoning.\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 2: You are an advanced natural language inference (NLI) model tasked with analyzing pairs of sentences—a premise and a hypothesis—and determining their logical relationship. The premise provides a statement or context, while the hypothesis is another statement that may or may not align with it. Your goal is to classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis is fully and logically supported by the premise, meaning the premise implies the hypothesis is true.\n",
      "- **Neutral**: The premise does not provide sufficient information to confirm or deny the hypothesis; it could be true or false based on the premise alone.\n",
      "- **Contradiction**: The hypothesis directly conflicts with the premise, making it false if the premise is true.\n",
      "\n",
      "This dataset emphasizes precise quantification (e.g., words like \"exactly eight\" or \"all six\"), complex sentence structures (such as questions, negations, and intricate relationships), and everyday scenarios involving people, objects, and interactions. You must carefully parse these elements to ensure accurate reasoning, focusing on numerical accuracy, structural details, and logical implications to handle diverse and creative language use effectively.\n",
      "\n",
      "For reference, here are examples of premise-hypothesis pairs and their labels:\n",
      "\n",
      "- Premise: All eight women that compel libraries to appreciate an actress might like the skirt.  \n",
      "  Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.  \n",
      "  Label: entailment (The premise implies the exact number mentioned.)\n",
      "\n",
      "- Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?  \n",
      "  Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.  \n",
      "  Label: entailment (The premise suggests the exact quantity.)\n",
      "\n",
      "- Premise: Do all eight women that compel libraries to appreciate an actress like the skirt?  \n",
      "  Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.  \n",
      "  Label: entailment (The premise confirms the exact group.)\n",
      "\n",
      "When given a new premise and hypothesis, analyze them step by step: First, identify key elements like quantities, negations, and relationships. Then, evaluate if the hypothesis follows from, is unrelated to, or contradicts the premise. Finally, output only the label in the following format: \"Label: [entailment/neutral/contradiction]\". Ensure your reasoning is thorough and precise to promote balanced and reliable AI performance on this task.\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 3: You are an expert in Natural Language Inference (NLI). Given a premise and a hypothesis, analyze their logical relationship by considering precise quantification (e.g., exact numbers), complex sentence structures (e.g., questions, negations), and everyday scenarios. Determine and output one of the following labels: \n",
      "- 'entailment' if the hypothesis logically follows from the premise,\n",
      "- 'neutral' if the hypothesis is neither entailed nor contradicted by the premise,\n",
      "- 'contradiction' if the hypothesis is inconsistent with the premise.\n",
      "\n",
      "For each classification, reason step by step to demonstrate your understanding of the relationships, ensuring attention to numerical accuracy and structural details.\n",
      "\n",
      "Example 1:\n",
      "Premise: Do all ten cashiers who weren't running around this school need to bring the lamp?\n",
      "Hypothesis: All ten cashiers who weren't running around this school do need to bring the lamp.\n",
      "Label: neutral (The premise is a question and does not definitively state the need, so it's neither fully entailed nor contradicted.)\n",
      "\n",
      "Example 2:\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment (The premise implies the existence of exactly six guests with the described actions.)\n",
      "\n",
      "Example 3:\n",
      "Premise: The two shoes that aren't bothering Rachelle haven't soaked.\n",
      "Hypothesis: The two shoes that aren't bothering Rachelle have soaked.\n",
      "Label: contradiction (The premise states the shoes haven't soaked, directly opposing the hypothesis.)\n",
      "\n",
      "Now, for the input premise and hypothesis, provide your classification and reasoning.\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 4: Given a premise and a hypothesis, classify their logical relationship as 'entailment', 'neutral', or 'contradiction'.\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 5: As an expert in natural language inference (NLI), your task is to evaluate pairs of sentences—a premise and a hypothesis—and determine their logical relationship. The premise provides a statement or context, while the hypothesis is another statement that may or may not follow from it. Classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: If the hypothesis logically follows from the premise, meaning the premise implies the hypothesis is true (e.g., if the premise mentions specific quantities or events that directly support the hypothesis).\n",
      "- **Contradiction**: If the hypothesis directly conflicts with the premise, making it impossible for both to be true simultaneously (e.g., due to opposing quantities, negations, or incompatible events).\n",
      "- **Neutral**: If the premise neither supports nor contradicts the hypothesis, leaving the truth of the hypothesis uncertain.\n",
      "\n",
      "This dataset focuses on precise quantification (e.g., exact numbers like \"exactly two\"), complex sentence structures (e.g., questions, negations, and intricate relationships involving people, objects, and scenarios), and fine-grained logical reasoning. Analyze each pair carefully, considering everyday contexts and ensuring balanced evaluation of numerical accuracy and structural parsing.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Output only the label: \"entailment\", \"neutral\", or \"contradiction\", based on your analysis. Here's an example for clarity:\n",
      "\n",
      "- Premise: \"If both skateboards that shouldn't shatter are rolling, it's okay.\"  \n",
      "  Hypothesis: \"There are exactly two skateboards that shouldn't shatter.\"  \n",
      "  Label: entailment\n",
      "\n",
      "- Premise: \"Both mushrooms that haven't charred have blackened.\"  \n",
      "  Hypothesis: \"There are exactly three mushrooms that haven't charred.\"  \n",
      "  Label: contradiction\n",
      "\n",
      "- Premise: \"Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\"  \n",
      "  Hypothesis: \"There are exactly six guests that had badgered a lot of analyses to shock people.\"  \n",
      "  Label: entailment\n",
      "\n",
      "Provide your response as just the single-word label for each pair.\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 6: As an advanced Natural Language Inference expert, carefully analyze the given premise and hypothesis to determine their logical relationship. Pay special attention to precise quantifiers (like \"exactly eight\"), numerical accuracy, complex sentence structures such as questions, negations, and everyday scenarios involving people, objects, or relationships. Think step-by-step: first, break down the key elements in the premise and hypothesis; second, evaluate if the hypothesis logically follows from the premise (entailment), does not affect it (neutral), or directly opposes it (contradiction); finally, output only one label: \"entailment\", \"neutral\", or \"contradiction\". Be creative in your reasoning to handle diverse and tricky language patterns for the most accurate classification.\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 7: You are an expert natural language inference analyst, specializing in evaluating logical relationships between sentences, including precise quantification, complex structures like questions and negations, and everyday scenarios. Your task is to take a premise and a hypothesis as input and classify their relationship as one of the following: 'entailment', 'neutral', or 'contradiction'. Base your classification on careful analysis of the premise's implications. Output only the label in the format: Label: [your label here].\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 8: You are a precise natural language inference expert, specializing in analyzing logical relationships, precise quantification (such as exact numbers or counts), and complex sentence structures like questions and negations. Your task is to evaluate the relationship between a given premise and a hypothesis, classifying it as one of the following: 'entailment' (the premise logically implies the hypothesis), 'neutral' (the premise is irrelevant or unrelated to the hypothesis), or 'contradiction' (the premise logically opposes the hypothesis). For each input, you will receive a premise and a hypothesis. Respond with only the label: 'entailment', 'neutral', or 'contradiction', based on your expert analysis.\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 9: You are a highly accurate natural language inference (NLI) classifier. Your task is to analyze a given premise and hypothesis, then classify their logical relationship as one of the following: 'entailment', 'neutral', or 'contradiction'. \n",
      "\n",
      "- **Entailment**: The hypothesis is logically true if the premise is true. For example, if the premise states a fact that directly supports the hypothesis, especially with precise quantification (e.g., \"exactly eight women\"), then this label applies.\n",
      "- **Neutral**: The premise does not provide enough information to confirm or deny the hypothesis. It may be unrelated or insufficiently specific.\n",
      "- **Contradiction**: The hypothesis is logically false if the premise is true, such as when the premise directly opposes the hypothesis through negations, numerical discrepancies, or conflicting structures.\n",
      "\n",
      "This dataset focuses on everyday scenarios involving people, objects, and relationships, often featuring complex sentence structures like questions, negations, and precise numerical details (e.g., \"exactly seven patients\"). Pay close attention to these elements to ensure accurate reasoning, as the goal is to test fine-grained entailment and promote balanced handling of creative language use.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Output only the label ('entailment', 'neutral', or 'contradiction') based on your analysis, without additional explanation.\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 10: You are an expert natural language inference (NLI) specialist, skilled in analyzing logical relationships in complex sentences involving precise quantification, numerical accuracy, negations, and everyday scenarios. Your task is to take a premise and a hypothesis as input and determine their logical relationship, classifying it as one of the following: 'entailment' (the hypothesis follows directly from the premise), 'neutral' (the premise neither supports nor contradicts the hypothesis), or 'contradiction' (the hypothesis conflicts with the premise). Provide your classification clearly and concisely.\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: 11: You are an expert in natural language inference (NLI), tasked with analyzing pairs of premises and hypotheses to determine their logical relationship. For each pair, classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis is fully and logically supported by the premise, meaning the premise implies the hypothesis is true.\n",
      "- **Neutral**: The premise neither confirms nor contradicts the hypothesis; it provides insufficient information to determine the truth of the hypothesis.\n",
      "- **Contradiction**: The hypothesis directly conflicts with the premise, making it impossible for both to be true simultaneously.\n",
      "\n",
      "This dataset focuses on precise quantification (e.g., \"exactly eight women\"), complex sentence structures such as questions and negations, and everyday scenarios involving people, objects, and relationships. These elements test your ability to handle numerical accuracy, structural parsing, and fine-grained logical reasoning. Aim for balanced consideration of these aspects to ensure accurate classifications.\n",
      "\n",
      "To perform this task effectively:\n",
      "- Carefully examine the premise and hypothesis for subtle details, including exact numbers, negations, and implied meanings.\n",
      "- Reason step by step: Break down the logical connections, identify any conflicts or ambiguities, and explain your thought process before stating the final label.\n",
      "- Use the provided examples as a guide:\n",
      "\n",
      "  - **Example 1**: Premise: \"Were all ten guys that proved to boast divorcing?\" Hypothesis: \"There are exactly eleven guys that proved to boast.\"  \n",
      "    Label: Contradiction (The premise implies there are exactly ten guys, which directly contradicts the hypothesis of eleven guys.)\n",
      "\n",
      "  - **Example 2**: Premise: \"Do all ten cashiers who weren't running around this school need to bring the lamp?\" Hypothesis: \"All ten cashiers who weren't running around this school do need to bring the lamp.\"  \n",
      "    Label: Neutral (The premise poses a question about necessity, but does not confirm or deny it, leaving the hypothesis unconfirmed.)\n",
      "\n",
      "  - **Example 3**: Premise: \"Did both waiters who examined some commentaries discover what will stun a lot of banks.\" Hypothesis: \"There aren't exactly two waiters who examined some commentaries.\"  \n",
      "    Label: Contradiction (The premise refers to \"both waiters,\" implying exactly two, which contradicts the hypothesis.)\n",
      "\n",
      "For any input, respond in the format:  \n",
      "**Label:** [entailment/neutral/contradiction]  \n",
      "**Explanation:** [A brief, clear explanation of your reasoning.]\n",
      "\n",
      "Input format: Premise: [premise text] Hypothesis: [hypothesis text]\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 19 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-02T14:42:29.771671]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `dataset_description` (str): A description of the dataset that we are using.\n",
      "2. `task_demos` (str): Example inputs/outputs of our module.\n",
      "3. `basic_instruction` (str): Basic instruction.\n",
      "4. `tip` (str): A suggestion for how to go about generating the new instruction.\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): Propose an instruction that will be used to prompt a Language Model to perform this task.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "{dataset_description}\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "{task_demos}\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "{tip}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the information below to learn about a task that we are trying to solve using calls to an LM, then generate a new instruction that will be used to prompt a Language Model to better solve the task.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## dataset_description ## ]]\n",
      "This dataset is designed for natural language inference (NLI) tasks, featuring pairs of premises and hypotheses labeled as 'entailment', 'neutral', or 'contradiction', with a strong emphasis on precise quantification (e.g., \"exactly eight women\") and complex sentence structures like questions and negations to test logical relationships. It draws from everyday scenarios involving people, objects, and relationships to challenge models on numerical accuracy and structural parsing, promoting balanced label distribution for advanced AI reasoning. Overall, it aims to enhance AI's ability to detect fine-grained entailment and handle diverse, creative language use.\n",
      "\n",
      "[[ ## task_demos ## ]]\n",
      "Premise: Were all ten guys that proved to boast divorcing?\n",
      "Hypothesis: There are exactly eleven guys that proved to boast.\n",
      "Label: contradiction\n",
      "\n",
      "Premise: Do all ten cashiers who weren't running around this school need to bring the lamp?\n",
      "Hypothesis: All ten cashiers who weren't running around this school do need to bring the lamp.\n",
      "Label: neutral\n",
      "\n",
      "Premise: Did both waiters who examined some commentaries discover what will stun a lot of banks.\n",
      "Hypothesis: There aren't exactly two waiters who examined some commentaries\n",
      "Label: contradiction\n",
      "\n",
      "\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "\n",
      "[[ ## tip ## ]]\n",
      "Make sure your instruction is very informative and descriptive.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "You are an expert in natural language inference (NLI), tasked with analyzing pairs of premises and hypotheses to determine their logical relationship. For each pair, classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis is fully and logically supported by the premise, meaning the premise implies the hypothesis is true.\n",
      "- **Neutral**: The premise neither confirms nor contradicts the hypothesis; it provides insufficient information to determine the truth of the hypothesis.\n",
      "- **Contradiction**: The hypothesis directly conflicts with the premise, making it impossible for both to be true simultaneously.\n",
      "\n",
      "This dataset focuses on precise quantification (e.g., \"exactly eight women\"), complex sentence structures such as questions and negations, and everyday scenarios involving people, objects, and relationships. These elements test your ability to handle numerical accuracy, structural parsing, and fine-grained logical reasoning. Aim for balanced consideration of these aspects to ensure accurate classifications.\n",
      "\n",
      "To perform this task effectively:\n",
      "- Carefully examine the premise and hypothesis for subtle details, including exact numbers, negations, and implied meanings.\n",
      "- Reason step by step: Break down the logical connections, identify any conflicts or ambiguities, and explain your thought process before stating the final label.\n",
      "- Use the provided examples as a guide:\n",
      "\n",
      "  - **Example 1**: Premise: \"Were all ten guys that proved to boast divorcing?\" Hypothesis: \"There are exactly eleven guys that proved to boast.\"  \n",
      "    Label: Contradiction (The premise implies there are exactly ten guys, which directly contradicts the hypothesis of eleven guys.)\n",
      "\n",
      "  - **Example 2**: Premise: \"Do all ten cashiers who weren't running around this school need to bring the lamp?\" Hypothesis: \"All ten cashiers who weren't running around this school do need to bring the lamp.\"  \n",
      "    Label: Neutral (The premise poses a question about necessity, but does not confirm or deny it, leaving the hypothesis unconfirmed.)\n",
      "\n",
      "  - **Example 3**: Premise: \"Did both waiters who examined some commentaries discover what will stun a lot of banks.\" Hypothesis: \"There aren't exactly two waiters who examined some commentaries.\"  \n",
      "    Label: Contradiction (The premise refers to \"both waiters,\" implying exactly two, which contradicts the hypothesis.)\n",
      "\n",
      "For any input, respond in the format:  \n",
      "**Label:** [entailment/neutral/contradiction]  \n",
      "**Explanation:** [A brief, clear explanation of your reasoning.]\n",
      "\n",
      "Input format: Premise: [premise text] Hypothesis: [hypothesis text]\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROPOSED INSTRUCTION: You are an expert in natural language inference (NLI), tasked with analyzing pairs of premises and hypotheses to determine their logical relationship. For each pair, classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis is fully and logically supported by the premise, meaning the premise implies the hypothesis is true.\n",
      "- **Neutral**: The premise neither confirms nor contradicts the hypothesis; it provides insufficient information to determine the truth of the hypothesis.\n",
      "- **Contradiction**: The hypothesis directly conflicts with the premise, making it impossible for both to be true simultaneously.\n",
      "\n",
      "This dataset focuses on precise quantification (e.g., \"exactly eight women\"), complex sentence structures such as questions and negations, and everyday scenarios involving people, objects, and relationships. These elements test your ability to handle numerical accuracy, structural parsing, and fine-grained logical reasoning. Aim for balanced consideration of these aspects to ensure accurate classifications.\n",
      "\n",
      "To perform this task effectively:\n",
      "- Carefully examine the premise and hypothesis for subtle details, including exact numbers, negations, and implied meanings.\n",
      "- Reason step by step: Break down the logical connections, identify any conflicts or ambiguities, and explain your thought process before stating the final label.\n",
      "- Use the provided examples as a guide:\n",
      "\n",
      "  - **Example 1**: Premise: \"Were all ten guys that proved to boast divorcing?\" Hypothesis: \"There are exactly eleven guys that proved to boast.\"  \n",
      "    Label: Contradiction (The premise implies there are exactly ten guys, which directly contradicts the hypothesis of eleven guys.)\n",
      "\n",
      "  - **Example 2**: Premise: \"Do all ten cashiers who weren't running around this school need to bring the lamp?\" Hypothesis: \"All ten cashiers who weren't running around this school do need to bring the lamp.\"  \n",
      "    Label: Neutral (The premise poses a question about necessity, but does not confirm or deny it, leaving the hypothesis unconfirmed.)\n",
      "\n",
      "  - **Example 3**: Premise: \"Did both waiters who examined some commentaries discover what will stun a lot of banks.\" Hypothesis: \"There aren't exactly two waiters who examined some commentaries.\"  \n",
      "    Label: Contradiction (The premise refers to \"both waiters,\" implying exactly two, which contradicts the hypothesis.)\n",
      "\n",
      "For any input, respond in the format:  \n",
      "**Label:** [entailment/neutral/contradiction]  \n",
      "**Explanation:** [A brief, clear explanation of your reasoning.]\n",
      "\n",
      "Input format: Premise: [premise text] Hypothesis: [hypothesis text]\n",
      "Average Metric: 37.00 / 64 (57.8%): 100%|██████████| 64/64 [00:00<00:00, 3403.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:42:29 INFO dspy.evaluate.evaluate: Average Metric: 37 / 64 (57.8%)\n",
      "2025/08/02 14:42:29 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 57.81\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lotems/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/08/02 14:42:34 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 2 / 19 - Minibatch ==\n",
      "2025/08/02 14:42:41 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: You are an expert in natural language inference (NLI), tasked with analyzing pairs of premises and hypotheses to determine their logical relationship. For each pair, classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis is fully and logically supported by the premise, meaning the premise implies the hypothesis is true.\n",
      "- **Neutral**: The premise neither confirms nor contradicts the hypothesis; it provides insufficient information to determine the truth of the hypothesis.\n",
      "- **Contradiction**: The hypothesis directly conflicts with the premise, making it impossible for both to be true simultaneously.\n",
      "\n",
      "This dataset focuses on precise quantification (e.g., \"exactly eight women\"), complex sentence structures such as questions and negations, and everyday scenarios involving people, objects, and relationships. These elements test your ability to handle numerical accuracy, structural parsing, and fine-grained logical reasoning. Aim for balanced consideration of these aspects to ensure accurate classifications.\n",
      "\n",
      "To perform this task effectively:\n",
      "- Carefully examine the premise and hypothesis for subtle details, including exact numbers, negations, and implied meanings.\n",
      "- Reason step by step: Break down the logical connections, identify any conflicts or ambiguities, and explain your thought process before stating the final label.\n",
      "- Use the provided examples as a guide:\n",
      "\n",
      "  - **Example 1**: Premise: \"Were all ten guys that proved to boast divorcing?\" Hypothesis: \"There are exactly eleven guys that proved to boast.\"  \n",
      "    Label: Contradiction (The premise implies there are exactly ten guys, which directly contradicts the hypothesis of eleven guys.)\n",
      "\n",
      "  - **Example 2**: Premise: \"Do all ten cashiers who weren't running around this school need to bring the lamp?\" Hypothesis: \"All ten cashiers who weren't running around this school do need to bring the lamp.\"  \n",
      "    Label: Neutral (The premise poses a question about necessity, but does not confirm or deny it, leaving the hypothesis unconfirmed.)\n",
      "\n",
      "  - **Example 3**: Premise: \"Did both waiters who examined some commentaries discover what will stun a lot of banks.\" Hypothesis: \"There aren't exactly two waiters who examined some commentaries.\"  \n",
      "    Label: Contradiction (The premise refers to \"both waiters,\" implying exactly two, which contradicts the hypothesis.)\n",
      "\n",
      "For any input, respond in the format:  \n",
      "**Label:** [entailment/neutral/contradiction]  \n",
      "**Explanation:** [A brief, clear explanation of your reasoning.]\n",
      "\n",
      "Input format: Premise: [premise text] Hypothesis: [hypothesis text]\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 13.00 / 20 (65.0%): 100%|██████████| 20/20 [00:00<00:00, 606.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:42:41 INFO dspy.evaluate.evaluate: Average Metric: 13 / 20 (65.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:42:42 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 65.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 11', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/08/02 14:42:42 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0]\n",
      "2025/08/02 14:42:42 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81]\n",
      "2025/08/02 14:42:42 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 57.81\n",
      "2025/08/02 14:42:42 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:42:42 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 3 / 19 - Minibatch ==\n",
      "2025/08/02 14:42:44 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: You are an advanced natural language inference (NLI) model tasked with analyzing pairs of sentences—a premise and a hypothesis—and determining their logical relationship. The premise provides a statement or context, while the hypothesis is another statement that may or may not align with it. Your goal is to classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis is fully and logically supported by the premise, meaning the premise implies the hypothesis is true.\n",
      "- **Neutral**: The premise does not provide sufficient information to confirm or deny the hypothesis; it could be true or false based on the premise alone.\n",
      "- **Contradiction**: The hypothesis directly conflicts with the premise, making it false if the premise is true.\n",
      "\n",
      "This dataset emphasizes precise quantification (e.g., words like \"exactly eight\" or \"all six\"), complex sentence structures (such as questions, negations, and intricate relationships), and everyday scenarios involving people, objects, and interactions. You must carefully parse these elements to ensure accurate reasoning, focusing on numerical accuracy, structural details, and logical implications to handle diverse and creative language use effectively.\n",
      "\n",
      "For reference, here are examples of premise-hypothesis pairs and their labels:\n",
      "\n",
      "- Premise: All eight women that compel libraries to appreciate an actress might like the skirt.  \n",
      "  Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.  \n",
      "  Label: entailment (The premise implies the exact number mentioned.)\n",
      "\n",
      "- Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?  \n",
      "  Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.  \n",
      "  Label: entailment (The premise suggests the exact quantity.)\n",
      "\n",
      "- Premise: Do all eight women that compel libraries to appreciate an actress like the skirt?  \n",
      "  Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.  \n",
      "  Label: entailment (The premise confirms the exact group.)\n",
      "\n",
      "When given a new premise and hypothesis, analyze them step by step: First, identify key elements like quantities, negations, and relationships. Then, evaluate if the hypothesis follows from, is unrelated to, or contradicts the premise. Finally, output only the label in the following format: \"Label: [entailment/neutral/contradiction]\". Ensure your reasoning is thorough and precise to promote balanced and reliable AI performance on this task.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 14.00 / 20 (70.0%): 100%|██████████| 20/20 [00:00<00:00, 900.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:42:44 INFO dspy.evaluate.evaluate: Average Metric: 14 / 20 (70.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:42:46 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/08/02 14:42:46 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0]\n",
      "2025/08/02 14:42:46 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81]\n",
      "2025/08/02 14:42:46 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 57.81\n",
      "2025/08/02 14:42:46 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:42:46 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 4 / 19 - Minibatch ==\n",
      "2025/08/02 14:42:47 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 16.00 / 20 (80.0%): 100%|██████████| 20/20 [00:00<00:00, 805.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:42:47 INFO dspy.evaluate.evaluate: Average Metric: 16 / 20 (80.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:42:49 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 80.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/08/02 14:42:49 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0]\n",
      "2025/08/02 14:42:49 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81]\n",
      "2025/08/02 14:42:49 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 57.81\n",
      "2025/08/02 14:42:49 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:42:49 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 5 / 19 - Minibatch ==\n",
      "2025/08/02 14:42:57 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: You are an expert in natural language inference (NLI), tasked with analyzing pairs of premises and hypotheses to determine their logical relationship. For each pair, classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis is fully and logically supported by the premise, meaning the premise implies the hypothesis is true.\n",
      "- **Neutral**: The premise neither confirms nor contradicts the hypothesis; it provides insufficient information to determine the truth of the hypothesis.\n",
      "- **Contradiction**: The hypothesis directly conflicts with the premise, making it impossible for both to be true simultaneously.\n",
      "\n",
      "This dataset focuses on precise quantification (e.g., \"exactly eight women\"), complex sentence structures such as questions and negations, and everyday scenarios involving people, objects, and relationships. These elements test your ability to handle numerical accuracy, structural parsing, and fine-grained logical reasoning. Aim for balanced consideration of these aspects to ensure accurate classifications.\n",
      "\n",
      "To perform this task effectively:\n",
      "- Carefully examine the premise and hypothesis for subtle details, including exact numbers, negations, and implied meanings.\n",
      "- Reason step by step: Break down the logical connections, identify any conflicts or ambiguities, and explain your thought process before stating the final label.\n",
      "- Use the provided examples as a guide:\n",
      "\n",
      "  - **Example 1**: Premise: \"Were all ten guys that proved to boast divorcing?\" Hypothesis: \"There are exactly eleven guys that proved to boast.\"  \n",
      "    Label: Contradiction (The premise implies there are exactly ten guys, which directly contradicts the hypothesis of eleven guys.)\n",
      "\n",
      "  - **Example 2**: Premise: \"Do all ten cashiers who weren't running around this school need to bring the lamp?\" Hypothesis: \"All ten cashiers who weren't running around this school do need to bring the lamp.\"  \n",
      "    Label: Neutral (The premise poses a question about necessity, but does not confirm or deny it, leaving the hypothesis unconfirmed.)\n",
      "\n",
      "  - **Example 3**: Premise: \"Did both waiters who examined some commentaries discover what will stun a lot of banks.\" Hypothesis: \"There aren't exactly two waiters who examined some commentaries.\"  \n",
      "    Label: Contradiction (The premise refers to \"both waiters,\" implying exactly two, which contradicts the hypothesis.)\n",
      "\n",
      "For any input, respond in the format:  \n",
      "**Label:** [entailment/neutral/contradiction]  \n",
      "**Explanation:** [A brief, clear explanation of your reasoning.]\n",
      "\n",
      "Input format: Premise: [premise text] Hypothesis: [hypothesis text]\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 14.00 / 20 (70.0%): 100%|██████████| 20/20 [00:00<00:00, 597.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:42:57 INFO dspy.evaluate.evaluate: Average Metric: 14 / 20 (70.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:42:59 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 11', 'Predictor 0: Few-Shot Set 10'].\n",
      "2025/08/02 14:42:59 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0]\n",
      "2025/08/02 14:42:59 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81]\n",
      "2025/08/02 14:42:59 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 57.81\n",
      "2025/08/02 14:42:59 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:42:59 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 6 / 19 - Minibatch ==\n",
      "2025/08/02 14:43:00 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: As an advanced Natural Language Inference expert, carefully analyze the given premise and hypothesis to determine their logical relationship. Pay special attention to precise quantifiers (like \"exactly eight\"), numerical accuracy, complex sentence structures such as questions, negations, and everyday scenarios involving people, objects, or relationships. Think step-by-step: first, break down the key elements in the premise and hypothesis; second, evaluate if the hypothesis logically follows from the premise (entailment), does not affect it (neutral), or directly opposes it (contradiction); finally, output only one label: \"entailment\", \"neutral\", or \"contradiction\". Be creative in your reasoning to handle diverse and tricky language patterns for the most accurate classification.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 14.00 / 20 (70.0%): 100%|██████████| 20/20 [00:00<00:00, 555.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:00 INFO dspy.evaluate.evaluate: Average Metric: 14 / 20 (70.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:02 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 6', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/08/02 14:43:02 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0, 70.0]\n",
      "2025/08/02 14:43:02 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81]\n",
      "2025/08/02 14:43:02 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 57.81\n",
      "2025/08/02 14:43:02 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:43:02 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 19 - Full Evaluation =====\n",
      "2025/08/02 14:43:02 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 80.0) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 39.00 / 64 (60.9%): 100%|██████████| 64/64 [00:00<00:00, 577.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:02 INFO dspy.evaluate.evaluate: Average Metric: 39 / 64 (60.9%)\n",
      "2025/08/02 14:43:02 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mNew best full eval score!\u001b[0m Score: 60.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:04 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94]\n",
      "2025/08/02 14:43:04 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.94\n",
      "2025/08/02 14:43:04 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/08/02 14:43:04 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/08/02 14:43:05 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 8 / 19 - Minibatch ==\n",
      "2025/08/02 14:43:06 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: You are a highly accurate natural language inference (NLI) classifier. Your task is to analyze a given premise and hypothesis, then classify their logical relationship as one of the following: 'entailment', 'neutral', or 'contradiction'. \n",
      "\n",
      "- **Entailment**: The hypothesis is logically true if the premise is true. For example, if the premise states a fact that directly supports the hypothesis, especially with precise quantification (e.g., \"exactly eight women\"), then this label applies.\n",
      "- **Neutral**: The premise does not provide enough information to confirm or deny the hypothesis. It may be unrelated or insufficiently specific.\n",
      "- **Contradiction**: The hypothesis is logically false if the premise is true, such as when the premise directly opposes the hypothesis through negations, numerical discrepancies, or conflicting structures.\n",
      "\n",
      "This dataset focuses on everyday scenarios involving people, objects, and relationships, often featuring complex sentence structures like questions, negations, and precise numerical details (e.g., \"exactly seven patients\"). Pay close attention to these elements to ensure accurate reasoning, as the goal is to test fine-grained entailment and promote balanced handling of creative language use.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Output only the label ('entailment', 'neutral', or 'contradiction') based on your analysis, without additional explanation.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 12.00 / 20 (60.0%): 100%|██████████| 20/20 [00:00<00:00, 683.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:06 INFO dspy.evaluate.evaluate: Average Metric: 12 / 20 (60.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:18 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 9', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/08/02 14:43:18 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0, 70.0, 60.0]\n",
      "2025/08/02 14:43:18 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94]\n",
      "2025/08/02 14:43:18 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.94\n",
      "2025/08/02 14:43:18 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:43:18 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 9 / 19 - Minibatch ==\n",
      "2025/08/02 14:43:20 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: You are an expert in Natural Language Inference (NLI). Given a premise and a hypothesis, analyze their logical relationship by considering precise quantification (e.g., exact numbers), complex sentence structures (e.g., questions, negations), and everyday scenarios. Determine and output one of the following labels: \n",
      "- 'entailment' if the hypothesis logically follows from the premise,\n",
      "- 'neutral' if the hypothesis is neither entailed nor contradicted by the premise,\n",
      "- 'contradiction' if the hypothesis is inconsistent with the premise.\n",
      "\n",
      "For each classification, reason step by step to demonstrate your understanding of the relationships, ensuring attention to numerical accuracy and structural details.\n",
      "\n",
      "Example 1:\n",
      "Premise: Do all ten cashiers who weren't running around this school need to bring the lamp?\n",
      "Hypothesis: All ten cashiers who weren't running around this school do need to bring the lamp.\n",
      "Label: neutral (The premise is a question and does not definitively state the need, so it's neither fully entailed nor contradicted.)\n",
      "\n",
      "Example 2:\n",
      "Premise: Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\n",
      "Hypothesis: There are exactly six guests that had badgered a lot of analyses to shock people.\n",
      "Label: entailment (The premise implies the existence of exactly six guests with the described actions.)\n",
      "\n",
      "Example 3:\n",
      "Premise: The two shoes that aren't bothering Rachelle haven't soaked.\n",
      "Hypothesis: The two shoes that aren't bothering Rachelle have soaked.\n",
      "Label: contradiction (The premise states the shoes haven't soaked, directly opposing the hypothesis.)\n",
      "\n",
      "Now, for the input premise and hypothesis, provide your classification and reasoning.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 14.00 / 20 (70.0%): 100%|██████████| 20/20 [00:00<00:00, 552.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:20 INFO dspy.evaluate.evaluate: Average Metric: 14 / 20 (70.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:21 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/08/02 14:43:21 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0, 70.0, 60.0, 70.0]\n",
      "2025/08/02 14:43:21 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94]\n",
      "2025/08/02 14:43:21 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.94\n",
      "2025/08/02 14:43:21 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:43:21 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 10 / 19 - Minibatch ==\n",
      "2025/08/02 14:43:23 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: You are a highly accurate natural language inference (NLI) classifier. Your task is to analyze a given premise and hypothesis, then classify their logical relationship as one of the following: 'entailment', 'neutral', or 'contradiction'. \n",
      "\n",
      "- **Entailment**: The hypothesis is logically true if the premise is true. For example, if the premise states a fact that directly supports the hypothesis, especially with precise quantification (e.g., \"exactly eight women\"), then this label applies.\n",
      "- **Neutral**: The premise does not provide enough information to confirm or deny the hypothesis. It may be unrelated or insufficiently specific.\n",
      "- **Contradiction**: The hypothesis is logically false if the premise is true, such as when the premise directly opposes the hypothesis through negations, numerical discrepancies, or conflicting structures.\n",
      "\n",
      "This dataset focuses on everyday scenarios involving people, objects, and relationships, often featuring complex sentence structures like questions, negations, and precise numerical details (e.g., \"exactly seven patients\"). Pay close attention to these elements to ensure accurate reasoning, as the goal is to test fine-grained entailment and promote balanced handling of creative language use.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Output only the label ('entailment', 'neutral', or 'contradiction') based on your analysis, without additional explanation.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 16.00 / 20 (80.0%): 100%|██████████| 20/20 [00:00<00:00, 563.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:23 INFO dspy.evaluate.evaluate: Average Metric: 16 / 20 (80.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:25 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 80.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 9', 'Predictor 0: Few-Shot Set 9'].\n",
      "2025/08/02 14:43:25 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0, 70.0, 60.0, 70.0, 80.0]\n",
      "2025/08/02 14:43:25 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94]\n",
      "2025/08/02 14:43:25 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.94\n",
      "2025/08/02 14:43:25 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:43:25 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 11 / 19 - Minibatch ==\n",
      "2025/08/02 14:43:27 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: You are an expert natural language inference (NLI) specialist, skilled in analyzing logical relationships in complex sentences involving precise quantification, numerical accuracy, negations, and everyday scenarios. Your task is to take a premise and a hypothesis as input and determine their logical relationship, classifying it as one of the following: 'entailment' (the hypothesis follows directly from the premise), 'neutral' (the premise neither supports nor contradicts the hypothesis), or 'contradiction' (the hypothesis conflicts with the premise). Provide your classification clearly and concisely.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 12.00 / 20 (60.0%): 100%|██████████| 20/20 [00:00<00:00, 683.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:27 INFO dspy.evaluate.evaluate: Average Metric: 12 / 20 (60.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:28 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 10', 'Predictor 0: Few-Shot Set 11'].\n",
      "2025/08/02 14:43:28 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0, 70.0, 60.0, 70.0, 80.0, 60.0]\n",
      "2025/08/02 14:43:28 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94]\n",
      "2025/08/02 14:43:28 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.94\n",
      "2025/08/02 14:43:28 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:43:28 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 12 / 19 - Minibatch ==\n",
      "2025/08/02 14:43:30 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: Given a premise and a hypothesis, classify their logical relationship as 'entailment', 'neutral', or 'contradiction'.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 11.00 / 20 (55.0%): 100%|██████████| 20/20 [00:00<00:00, 606.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:30 INFO dspy.evaluate.evaluate: Average Metric: 11 / 20 (55.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:43 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 55.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 9'].\n",
      "2025/08/02 14:43:43 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0, 70.0, 60.0, 70.0, 80.0, 60.0, 55.0]\n",
      "2025/08/02 14:43:43 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94]\n",
      "2025/08/02 14:43:43 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.94\n",
      "2025/08/02 14:43:43 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:43:43 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 13 / 19 - Full Evaluation =====\n",
      "2025/08/02 14:43:43 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 80.0) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 41.00 / 64 (64.1%): 100%|██████████| 64/64 [00:00<00:00, 566.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:44 INFO dspy.evaluate.evaluate: Average Metric: 41 / 64 (64.1%)\n",
      "2025/08/02 14:43:44 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mNew best full eval score!\u001b[0m Score: 64.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:45 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94, 64.06]\n",
      "2025/08/02 14:43:45 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 64.06\n",
      "2025/08/02 14:43:45 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/08/02 14:43:45 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/08/02 14:43:46 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 14 / 19 - Minibatch ==\n",
      "2025/08/02 14:43:47 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: A DSPy signature for Natural Language Inference classification.\n",
      "\n",
      "This classifier takes a premise and hypothesis as input and determines their \n",
      "logical relationship: entailment, neutral, or contradiction.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 11.00 / 20 (55.0%): 100%|██████████| 20/20 [00:00<00:00, 3709.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:47 INFO dspy.evaluate.evaluate: Average Metric: 11 / 20 (55.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:49 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 55.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/08/02 14:43:49 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0, 70.0, 60.0, 70.0, 80.0, 60.0, 55.0, 55.0]\n",
      "2025/08/02 14:43:49 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94, 64.06]\n",
      "2025/08/02 14:43:49 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 64.06\n",
      "2025/08/02 14:43:49 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:43:49 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 15 / 19 - Minibatch ==\n",
      "2025/08/02 14:43:51 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: As an expert in natural language inference (NLI), your task is to evaluate pairs of sentences—a premise and a hypothesis—and determine their logical relationship. The premise provides a statement or context, while the hypothesis is another statement that may or may not follow from it. Classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: If the hypothesis logically follows from the premise, meaning the premise implies the hypothesis is true (e.g., if the premise mentions specific quantities or events that directly support the hypothesis).\n",
      "- **Contradiction**: If the hypothesis directly conflicts with the premise, making it impossible for both to be true simultaneously (e.g., due to opposing quantities, negations, or incompatible events).\n",
      "- **Neutral**: If the premise neither supports nor contradicts the hypothesis, leaving the truth of the hypothesis uncertain.\n",
      "\n",
      "This dataset focuses on precise quantification (e.g., exact numbers like \"exactly two\"), complex sentence structures (e.g., questions, negations, and intricate relationships involving people, objects, and scenarios), and fine-grained logical reasoning. Analyze each pair carefully, considering everyday contexts and ensuring balanced evaluation of numerical accuracy and structural parsing.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Output only the label: \"entailment\", \"neutral\", or \"contradiction\", based on your analysis. Here's an example for clarity:\n",
      "\n",
      "- Premise: \"If both skateboards that shouldn't shatter are rolling, it's okay.\"  \n",
      "  Hypothesis: \"There are exactly two skateboards that shouldn't shatter.\"  \n",
      "  Label: entailment\n",
      "\n",
      "- Premise: \"Both mushrooms that haven't charred have blackened.\"  \n",
      "  Hypothesis: \"There are exactly three mushrooms that haven't charred.\"  \n",
      "  Label: contradiction\n",
      "\n",
      "- Premise: \"Have the six guests that had badgered a lot of analyses to shock people skated around these public parks?\"  \n",
      "  Hypothesis: \"There are exactly six guests that had badgered a lot of analyses to shock people.\"  \n",
      "  Label: entailment\n",
      "\n",
      "Provide your response as just the single-word label for each pair.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 11.00 / 20 (55.0%): 100%|██████████| 20/20 [00:00<00:00, 520.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:51 INFO dspy.evaluate.evaluate: Average Metric: 11 / 20 (55.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:53 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 55.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/08/02 14:43:53 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0, 70.0, 60.0, 70.0, 80.0, 60.0, 55.0, 55.0, 55.0]\n",
      "2025/08/02 14:43:53 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94, 64.06]\n",
      "2025/08/02 14:43:53 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 64.06\n",
      "2025/08/02 14:43:53 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:43:53 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 16 / 19 - Minibatch ==\n",
      "2025/08/02 14:43:55 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: You are a precise natural language inference expert, specializing in analyzing logical relationships, precise quantification (such as exact numbers or counts), and complex sentence structures like questions and negations. Your task is to evaluate the relationship between a given premise and a hypothesis, classifying it as one of the following: 'entailment' (the premise logically implies the hypothesis), 'neutral' (the premise is irrelevant or unrelated to the hypothesis), or 'contradiction' (the premise logically opposes the hypothesis). For each input, you will receive a premise and a hypothesis. Respond with only the label: 'entailment', 'neutral', or 'contradiction', based on your expert analysis.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 11.00 / 20 (55.0%): 100%|██████████| 20/20 [00:00<00:00, 417.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:55 INFO dspy.evaluate.evaluate: Average Metric: 11 / 20 (55.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:57 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 55.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 8', 'Predictor 0: Few-Shot Set 8'].\n",
      "2025/08/02 14:43:57 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0, 70.0, 60.0, 70.0, 80.0, 60.0, 55.0, 55.0, 55.0, 55.0]\n",
      "2025/08/02 14:43:57 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94, 64.06]\n",
      "2025/08/02 14:43:57 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 64.06\n",
      "2025/08/02 14:43:57 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:43:57 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 17 / 19 - Minibatch ==\n",
      "2025/08/02 14:43:59 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: You are an advanced Natural Language Inference (NLI) classifier tasked with analyzing pairs of sentences—a premise and a hypothesis—to determine their logical relationship. The premise provides a statement or question about everyday scenarios involving people, objects, relationships, precise quantifications (e.g., \"exactly eight women\"), and complex sentence structures such as questions, negations, or intricate descriptions. Your goal is to classify the relationship as one of the following:\n",
      "\n",
      "- **Entailment**: The hypothesis logically follows from the premise, meaning the premise supports or implies the hypothesis (e.g., if the premise states facts that directly confirm the hypothesis).\n",
      "- **Neutral**: The premise neither supports nor contradicts the hypothesis; they are unrelated or the premise provides no clear implication.\n",
      "- **Contradiction**: The hypothesis directly conflicts with or negates the premise.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Carefully evaluate the logical connections, paying special attention to numerical accuracy, structural parsing, and fine-grained details in the language. Output your classification as a single word: \"entailment\", \"neutral\", or \"contradiction\".\n",
      "\n",
      "Example:\n",
      "- Premise: All eight women that compel libraries to appreciate an actress might like the skirt.\n",
      "- Hypothesis: There are exactly eight women that compel libraries to appreciate an actress.\n",
      "- Classification: entailment\n",
      "\n",
      "Ensure your reasoning is thorough, considering the context of creative and diverse language use to promote accurate AI reasoning.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 11.00 / 20 (55.0%): 100%|██████████| 20/20 [00:00<00:00, 566.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:43:59 INFO dspy.evaluate.evaluate: Average Metric: 11 / 20 (55.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:44:00 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 55.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/08/02 14:44:00 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0, 70.0, 60.0, 70.0, 80.0, 60.0, 55.0, 55.0, 55.0, 55.0, 55.0]\n",
      "2025/08/02 14:44:00 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94, 64.06]\n",
      "2025/08/02 14:44:00 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 64.06\n",
      "2025/08/02 14:44:00 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:44:00 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 18 / 19 - Minibatch ==\n",
      "2025/08/02 14:44:39 INFO dspy.teleprompt.mipro_optimizer_v2: Evaluating the following candidate program...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor 0\n",
      "i: You are a highly accurate natural language inference (NLI) classifier. Your task is to analyze a given premise and hypothesis, then classify their logical relationship as one of the following: 'entailment', 'neutral', or 'contradiction'. \n",
      "\n",
      "- **Entailment**: The hypothesis is logically true if the premise is true. For example, if the premise states a fact that directly supports the hypothesis, especially with precise quantification (e.g., \"exactly eight women\"), then this label applies.\n",
      "- **Neutral**: The premise does not provide enough information to confirm or deny the hypothesis. It may be unrelated or insufficiently specific.\n",
      "- **Contradiction**: The hypothesis is logically false if the premise is true, such as when the premise directly opposes the hypothesis through negations, numerical discrepancies, or conflicting structures.\n",
      "\n",
      "This dataset focuses on everyday scenarios involving people, objects, and relationships, often featuring complex sentence structures like questions, negations, and precise numerical details (e.g., \"exactly seven patients\"). Pay close attention to these elements to ensure accurate reasoning, as the goal is to test fine-grained entailment and promote balanced handling of creative language use.\n",
      "\n",
      "For each input, you will receive a premise and a hypothesis. Output only the label ('entailment', 'neutral', or 'contradiction') based on your analysis, without additional explanation.\n",
      "p: Label:\n",
      "\n",
      "\n",
      "Average Metric: 10.00 / 20 (50.0%): 100%|██████████| 20/20 [00:00<00:00, 328.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:44:39 INFO dspy.evaluate.evaluate: Average Metric: 10 / 20 (50.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 50.0 on minibatch of size 20 with parameters ['Predictor 0: Instruction 9', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [65.0, 70.0, 80.0, 70.0, 70.0, 60.0, 70.0, 80.0, 60.0, 55.0, 55.0, 55.0, 55.0, 55.0, 50.0]\n",
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94, 64.06]\n",
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 64.06\n",
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 19 / 19 - Full Evaluation =====\n",
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 70.0) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 39.00 / 64 (60.9%): 100%|██████████| 64/64 [00:00<00:00, 607.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:44:41 INFO dspy.evaluate.evaluate: Average Metric: 39 / 64 (60.9%)\n",
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [57.81, 60.94, 64.06, 60.94]\n",
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 64.06\n",
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/08/02 14:44:41 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 64.06!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "random.seed(42)\n",
    "mipro = MIPROv2(\n",
    "    metric=accuracy_metric,\n",
    "    verbose=True,\n",
    "    auto=None,  # Disable auto mode to set custom params\n",
    "    num_candidates=12,  # Required when auto=None; controls candidates for few-shots/instructions\n",
    "    init_temperature=1.0\n",
    ")\n",
    "opted_mipro = mipro.compile(\n",
    "    predictor,\n",
    "    trainset=dev_ex,\n",
    "    num_trials=15,  # Number of optimization trials\n",
    "    max_bootstrapped_demos=8,  # Demos per few-shot set\n",
    "    max_labeled_demos=4,\n",
    "    minibatch=True,  # Enable minibatching for efficiency\n",
    "    minibatch_size=20,\n",
    "    minibatch_full_eval_steps=5,  # Full val eval every 5 minibatch steps\n",
    "    requires_permission_to_run=False  # Skip confirmation prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488952d6e2e2a5c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:39:09.810337Z",
     "start_time": "2025-07-31T21:39:04.740529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 120.00 / 120 (100.0%): 100%|██████████| 120/120 [00:00<00:00, 477.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:44:41 INFO dspy.evaluate.evaluate: Average Metric: 120 / 120 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:\t100.0\n"
     ]
    }
   ],
   "source": [
    "mipro_report = evaluate(opted_mipro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3b4dc66a11dc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:39:09.940197Z",
     "start_time": "2025-07-31T21:39:09.829953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mipro_combined predictor</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          precision  recall   f1  accuracy\n",
       "mipro_combined predictor        1.0     1.0  1.0       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mipro_score,mipro_report_results,mipro_test_pred = mipro_report\n",
    "mipro_combined = compute_matrices(mipro_test_pred)\n",
    "display(pd.DataFrame(mipro_combined, index=[\"mipro_combined predictor\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd022cac18fef56d",
   "metadata": {},
   "source": [
    "### ensembling mipro2 with BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb3f14d590a71b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:39:10.025844Z",
     "start_time": "2025-07-31T21:39:10.023206Z"
    }
   },
   "outputs": [],
   "source": [
    "from dspy.teleprompt import Ensemble\n",
    "ensemble = Ensemble(reduce_fn=dspy.majority)\n",
    "combined = ensemble.compile([opted_rs, opted_mipro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977c881515b38b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:39:45.542846Z",
     "start_time": "2025-07-31T21:39:10.124719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 115.00 / 120 (95.8%): 100%|██████████| 120/120 [00:00<00:00, 3205.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:44:41 INFO dspy.evaluate.evaluate: Average Metric: 115 / 120 (95.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:\t95.83\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>combined predictor</th>\n",
       "      <td>0.962191</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958355</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    precision    recall        f1  accuracy\n",
       "combined predictor   0.962191  0.958333  0.958355  0.958333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_score, combined_report, combined_test_pred = evaluate(combined)\n",
    "combined_combined = compute_matrices(combined_test_pred)\n",
    "display(pd.DataFrame(combined_combined, index=[\"combined predictor\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5ae8dcfa60120",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "metric_prf = combine([\"precision\", \"recall\", \"f1\"])\n",
    "acc = load(\"accuracy\")\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc383dc3a22118",
   "metadata": {},
   "source": [
    "let's try to optimize it in another way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2cfb5e9f7637ee",
   "metadata": {},
   "source": [
    "### Prepare prediction variables for comparison\n",
    "\n",
    "Before comparing with DeBERTa, let's prepare all the prediction variables we need:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396b7e758643182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 115.00 / 120 (95.8%): 100%|██████████| 120/120 [00:00<00:00, 4129.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:44:46 INFO dspy.evaluate.evaluate: Average Metric: 115 / 120 (95.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All prediction variables prepared successfully!\n",
      "zs_preds length: 17019\n",
      "bs_preds length: 120\n",
      "rs_preds length: 120\n",
      "mi_preds length: 120\n",
      "ens_preds length: 120\n"
     ]
    }
   ],
   "source": [
    "# Convert numeric predictions back to string labels for comparison\n",
    "id2label = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "\n",
    "# Zero-shot predictions (from the original predictor)\n",
    "zs_preds = [id2label[pred] for pred in predictor_test_pred]\n",
    "\n",
    "# Bootstrap few-shot predictions (from overall_optimized)\n",
    "bs_preds = [id2label[pred] for pred in overall_test_pred]\n",
    "\n",
    "# Random search predictions (from opted_rs)\n",
    "rs_report = Evaluate(\n",
    "    devset=test_ex[:120],  # Limit to 120 for faster evaluation\n",
    "    metric=accuracy_metric,\n",
    "    return_outputs=True,\n",
    "    num_threads=50,\n",
    "    display_progress=True,\n",
    "    display_table=False,\n",
    "    provide_traceback=False\n",
    ")(opted_rs)\n",
    "rs_score, rs_results = rs_report\n",
    "rs_preds = [id2label[label2id[out[1].label]] for out in rs_results]\n",
    "\n",
    "# MIPROv2 predictions (from opted_mipro)\n",
    "mi_preds = [id2label[pred] for pred in mipro_test_pred]\n",
    "\n",
    "# Ensemble predictions (from combined model)\n",
    "ens_preds = [id2label[pred] for pred in combined_test_pred]\n",
    "\n",
    "# Add gold_label_str column to test_df\n",
    "test_df = test_df.copy()\n",
    "test_df['gold_label_str'] = test_df['gold_label'].map(id2label)\n",
    "\n",
    "# Define hf_metrics function\n",
    "def hf_metrics(preds, refs):\n",
    "    \"\"\"Compute HuggingFace metrics for predictions and references\"\"\"\n",
    "    pred_ids = [label2id[p] for p in preds]\n",
    "    ref_ids = [label2id[r] for r in refs]\n",
    "    \n",
    "    prf = metric_prf.compute(predictions=pred_ids, references=ref_ids, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=pred_ids, references=ref_ids)\n",
    "    \n",
    "    return {**prf, **accuracy}\n",
    "\n",
    "print(\"All prediction variables prepared successfully!\")\n",
    "print(f\"zs_preds length: {len(zs_preds)}\")\n",
    "print(f\"bs_preds length: {len(bs_preds)}\")\n",
    "print(f\"rs_preds length: {len(rs_preds)}\")\n",
    "print(f\"mi_preds length: {len(mi_preds)}\")\n",
    "print(f\"ens_preds length: {len(ens_preds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdcf07f41b5409",
   "metadata": {},
   "source": [
    "### Comparing with DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a08c81b0aec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DeBERTa predictions from deberta_item_preds.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct (both)</th>\n",
       "      <th>Correct1 (LLM only)</th>\n",
       "      <th>Correct2 (DeBERTa only)</th>\n",
       "      <th>Incorrect (both)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZeroShot_vs_DeBERTa</th>\n",
       "      <td>8576273</td>\n",
       "      <td>15050227</td>\n",
       "      <td>2563627</td>\n",
       "      <td>6145973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Correct (both)  Correct1 (LLM only)  \\\n",
       "ZeroShot_vs_DeBERTa         8576273             15050227   \n",
       "\n",
       "                     Correct2 (DeBERTa only)  Incorrect (both)  \n",
       "ZeroShot_vs_DeBERTa                  2563627           6145973  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct</th>\n",
       "      <th>Correct1</th>\n",
       "      <th>Correct2</th>\n",
       "      <th>Incorrect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>presupposition_all_n_presupposition</th>\n",
       "      <td>1239628</td>\n",
       "      <td>2218372</td>\n",
       "      <td>35972</td>\n",
       "      <td>98928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_both_presupposition</th>\n",
       "      <td>1313472</td>\n",
       "      <td>2152128</td>\n",
       "      <td>41328</td>\n",
       "      <td>85972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_change_of_state</th>\n",
       "      <td>756509</td>\n",
       "      <td>1247991</td>\n",
       "      <td>484891</td>\n",
       "      <td>1103509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_cleft_existence</th>\n",
       "      <td>703176</td>\n",
       "      <td>1734524</td>\n",
       "      <td>453424</td>\n",
       "      <td>701776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_cleft_uniqueness</th>\n",
       "      <td>813946</td>\n",
       "      <td>888454</td>\n",
       "      <td>478654</td>\n",
       "      <td>1411846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_only_presupposition</th>\n",
       "      <td>717950</td>\n",
       "      <td>1712150</td>\n",
       "      <td>441950</td>\n",
       "      <td>720850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_possessed_definites_existence</th>\n",
       "      <td>1063730</td>\n",
       "      <td>2287870</td>\n",
       "      <td>89570</td>\n",
       "      <td>151730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_possessed_definites_uniqueness</th>\n",
       "      <td>1007025</td>\n",
       "      <td>712475</td>\n",
       "      <td>358075</td>\n",
       "      <td>1515325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_question_presupposition</th>\n",
       "      <td>960837</td>\n",
       "      <td>2096263</td>\n",
       "      <td>179763</td>\n",
       "      <td>356037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Correct  Correct1  Correct2  \\\n",
       "section                                                                      \n",
       "presupposition_all_n_presupposition            1239628   2218372     35972   \n",
       "presupposition_both_presupposition             1313472   2152128     41328   \n",
       "presupposition_change_of_state                  756509   1247991    484891   \n",
       "presupposition_cleft_existence                  703176   1734524    453424   \n",
       "presupposition_cleft_uniqueness                 813946    888454    478654   \n",
       "presupposition_only_presupposition              717950   1712150    441950   \n",
       "presupposition_possessed_definites_existence   1063730   2287870     89570   \n",
       "presupposition_possessed_definites_uniqueness  1007025    712475    358075   \n",
       "presupposition_question_presupposition          960837   2096263    179763   \n",
       "\n",
       "                                               Incorrect  \n",
       "section                                                   \n",
       "presupposition_all_n_presupposition                98928  \n",
       "presupposition_both_presupposition                 85972  \n",
       "presupposition_change_of_state                   1103509  \n",
       "presupposition_cleft_existence                    701776  \n",
       "presupposition_cleft_uniqueness                  1411846  \n",
       "presupposition_only_presupposition                720850  \n",
       "presupposition_possessed_definites_existence      151730  \n",
       "presupposition_possessed_definites_uniqueness    1515325  \n",
       "presupposition_question_presupposition            356037  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct</th>\n",
       "      <th>Correct1</th>\n",
       "      <th>Correct2</th>\n",
       "      <th>Incorrect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZeroShot</th>\n",
       "      <td>78778</td>\n",
       "      <td>143522</td>\n",
       "      <td>1320</td>\n",
       "      <td>4380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BootstrapFS</th>\n",
       "      <td>80098</td>\n",
       "      <td>147902</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandSearch</th>\n",
       "      <td>76589</td>\n",
       "      <td>141911</td>\n",
       "      <td>3509</td>\n",
       "      <td>5991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIPROv2</th>\n",
       "      <td>80098</td>\n",
       "      <td>147902</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ensemble(RS+MI)</th>\n",
       "      <td>76589</td>\n",
       "      <td>141911</td>\n",
       "      <td>3509</td>\n",
       "      <td>5991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Correct  Correct1  Correct2  Incorrect\n",
       "model                                                  \n",
       "ZeroShot           78778    143522      1320       4380\n",
       "BootstrapFS        80098    147902         0          0\n",
       "RandSearch         76589    141911      3509       5991\n",
       "MIPROv2            80098    147902         0          0\n",
       "Ensemble(RS+MI)    76589    141911      3509       5991"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BootstrapFS</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIPROv2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZeroShot</th>\n",
       "      <td>0.976442</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975055</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandSearch</th>\n",
       "      <td>0.962191</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958355</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ensemble(RS+MI)</th>\n",
       "      <td>0.962191</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958355</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeBERTa</th>\n",
       "      <td>0.316649</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.311321</td>\n",
       "      <td>0.316667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 precision    recall        f1  accuracy\n",
       "model                                                   \n",
       "BootstrapFS       1.000000  1.000000  1.000000  1.000000\n",
       "MIPROv2           1.000000  1.000000  1.000000  1.000000\n",
       "ZeroShot          0.976442  0.975000  0.975055  0.975000\n",
       "RandSearch        0.962191  0.958333  0.958355  0.958333\n",
       "Ensemble(RS+MI)   0.962191  0.958333  0.958355  0.958333\n",
       "DeBERTa           0.316649  0.316667  0.311321  0.316667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os.path import exists\n",
    "\n",
    "# Load DeBERTa predictions if available\n",
    "if exists(\"deberta_item_preds.parquet\"):\n",
    "    deb = pd.read_parquet(\"deberta_item_preds.parquet\")\n",
    "    print(\"Loaded DeBERTa predictions from deberta_item_preds.parquet\")\n",
    "else:\n",
    "    print(\"Warning: deberta_item_preds.parquet not found!\")\n",
    "    print(\"This file should be generated by running imppres_baseline.ipynb first.\")\n",
    "    print(\"Skipping DeBERTa comparison section...\")\n",
    "    \n",
    "    # Create a summary of our LLM models without DeBERTa comparison\n",
    "    def pack_metrics_simple(name, preds):\n",
    "        return {\"model\": name, **hf_metrics(preds, test_df.gold_label_str.tolist())}\n",
    "\n",
    "    summary = [\n",
    "        pack_metrics_simple(\"ZeroShot\", zs_preds),\n",
    "        pack_metrics_simple(\"BootstrapFS\", bs_preds),\n",
    "        pack_metrics_simple(\"RandSearch\", rs_preds),\n",
    "        pack_metrics_simple(\"MIPROv2\", mi_preds),\n",
    "        pack_metrics_simple(\"Ensemble(RS+MI)\", ens_preds),\n",
    "    ]\n",
    "    summary_df = pd.DataFrame(summary).set_index(\"model\").sort_values(\"f1\", ascending=False)\n",
    "    print(\"\\nLLM Model Performance Summary:\")\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Exit early if no DeBERTa predictions\n",
    "    deb = None\n",
    "\n",
    "# Only run DeBERTa comparison if predictions are available\n",
    "if deb is not None:\n",
    "    # Build df for the current LLM model (example: zero-shot)\n",
    "    llm_df = test_df[[\"UID\",\"section\",\"gold_label_str\"]].copy()\n",
    "    llm_df[\"llm_pred\"] = zs_preds  # or bs_preds / rs_preds / ...\n",
    "\n",
    "    # Join\n",
    "    merged = llm_df.merge(deb[[\"UID\",\"deberta_pred\"]], on=\"UID\", how=\"inner\")\n",
    "\n",
    "    # Agreement counts\n",
    "    def agreement_counts(df, gold_col=\"gold_label_str\", p1=\"llm_pred\", p2=\"deberta_pred\"):\n",
    "        g = df[gold_col].values\n",
    "        a = df[p1].values\n",
    "        b = df[p2].values\n",
    "        both_correct  = ((a==g) & (b==g)).sum()\n",
    "        correct1_only = ((a==g) & (b!=g)).sum()\n",
    "        correct2_only = ((b==g) & (a!=g)).sum()\n",
    "        both_wrong    = ((a!=g) & (b!=g)).sum()\n",
    "        return both_correct, correct1_only, correct2_only, both_wrong\n",
    "\n",
    "    both, c1, c2, wrong = agreement_counts(merged)\n",
    "    agree_table = pd.DataFrame(\n",
    "        [[both, c1, c2, wrong]],\n",
    "        columns=[\"Correct (both)\", \"Correct1 (LLM only)\", \"Correct2 (DeBERTa only)\", \"Incorrect (both)\"],\n",
    "        index=[\"ZeroShot_vs_DeBERTa\"]\n",
    "    )\n",
    "    display(agree_table)\n",
    "\n",
    "    # Per-section agreement\n",
    "    def per_section_agreement(df):\n",
    "        rows = []\n",
    "        for sec, g in df.groupby(\"section\"):\n",
    "            b, c1, c2, w = agreement_counts(g)\n",
    "            rows.append([sec, b, c1, c2, w])\n",
    "        return pd.DataFrame(rows, columns=[\"section\",\"Correct\",\"Correct1\",\"Correct2\",\"Incorrect\"]).set_index(\"section\")\n",
    "\n",
    "    display(per_section_agreement(merged))\n",
    "\n",
    "    #%%\n",
    "    def compare_to_deberta(name, preds):\n",
    "        tmp = test_df[[\"UID\",\"section\",\"gold_label_str\"]].copy()[:120]\n",
    "        tmp[\"llm_pred\"] = preds[:120]\n",
    "        mer = tmp.merge(deb[[\"UID\",\"deberta_pred\"]], on=\"UID\")\n",
    "        b,c1,c2,w = agreement_counts(mer)\n",
    "        return pd.Series({\"model\":name,\"Correct\":b,\"Correct1\":c1,\"Correct2\":c2,\"Incorrect\":w})\n",
    "\n",
    "    rows = []\n",
    "    rows.append(compare_to_deberta(\"ZeroShot\", zs_preds))\n",
    "    rows.append(compare_to_deberta(\"BootstrapFS\", bs_preds))\n",
    "    rows.append(compare_to_deberta(\"RandSearch\", rs_preds))\n",
    "    rows.append(compare_to_deberta(\"MIPROv2\", mi_preds))\n",
    "    rows.append(compare_to_deberta(\"Ensemble(RS+MI)\", ens_preds))\n",
    "\n",
    "    agree_all_df = pd.DataFrame(rows).set_index(\"model\")\n",
    "    display(agree_all_df)\n",
    "    #%%\n",
    "    def pack_metrics(name, preds):\n",
    "        return {\"model\": name, **hf_metrics(preds[:120], test_df.gold_label_str.tolist()[:120])}\n",
    "\n",
    "    summary = [\n",
    "        pack_metrics(\"ZeroShot\", zs_preds),\n",
    "        pack_metrics(\"BootstrapFS\", bs_preds),\n",
    "        pack_metrics(\"RandSearch\", rs_preds),\n",
    "        pack_metrics(\"MIPROv2\", mi_preds),\n",
    "        pack_metrics(\"Ensemble(RS+MI)\", ens_preds),\n",
    "        pack_metrics(\"DeBERTa\", deb.loc[deb.UID.isin(test_df.UID),\"deberta_pred\"].tolist()),\n",
    "    ]\n",
    "    summary_df = pd.DataFrame(summary).set_index(\"model\").sort_values(\"f1\", ascending=False)\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print(\"DeBERTa comparison section skipped due to missing predictions file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
