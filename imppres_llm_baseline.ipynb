{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres LLM Baseline with Chain of Thought\n",
    "\n",
    "This notebook implements a baseline for ImpPres classification using an LLM through DSPy, including Chain of Thought reasoning.\n",
    "\n",
    "The implementation follows these steps:\n",
    "\n",
    "1. Basic Setup:\n",
    "   - Configure DSPy environment\n",
    "   - Define NLI Classifier Signature\n",
    "\n",
    "2. Dataset Loading:\n",
    "   - Load ImpPres presupposition sections\n",
    "   - Prepare data for evaluation\n",
    "\n",
    "3. Zero-shot Baseline:\n",
    "   - Implement basic NLI classifier\n",
    "   - Evaluate on all sections\n",
    "\n",
    "4. Optimization Strategies:\n",
    "   - Bootstrap Few-Shot learning\n",
    "   - MIPROv2 optimization\n",
    "   - Ensemble methods\n",
    "\n",
    "5. Chain of Thought Enhancement:\n",
    "   - Implement CoT-based classifier\n",
    "   - Compare with basic classifier\n",
    "\n",
    "6. Model Comparison:\n",
    "   - Compare with DeBERTa baseline\n",
    "   - Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9b979cbc0dc715",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:12.765252Z",
     "start_time": "2025-07-31T20:35:10.455958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "from os import environ\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import dspy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "logging.getLogger(\"dspy.adapters.json_adapter\").setLevel(logging.ERROR)\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=environ['XAI_API_KEY'])\n",
    "\n",
    "# for ollama\n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "# lm = dspy.LM(\n",
    "#     \"ollama/llama3.1:8b\",\n",
    "#     api_base=\"http://localhost:11434\",\n",
    "#     format=\"json\"        # litellm translates this to Ollama's stream=false\n",
    "# )\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686e6e259245fe7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:12.865734Z",
     "start_time": "2025-07-31T20:35:12.861941Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy program to classify pairs (premise, hypothesis) as entailment, contradiction, or neutral.\n",
    "class NLIImPresClassifier(dspy.Signature):\n",
    "    \"\"\"A DSPy signature for Natural Language Inference classification.\n",
    "    \n",
    "    This classifier takes a premise and hypothesis as input and determines their \n",
    "    logical relationship: entailment, neutral, or contradiction.\n",
    "    \"\"\"\n",
    "    premise     : str = dspy.InputField(desc=\"A short passage or statement. All facts should be inferred from this text alone.\")\n",
    "    hypothesis  : str = dspy.InputField(desc=\"A second statement to evaluate. Check if this follows from, contradicts, or is unrelated to the premise.\")\n",
    "    label       : Literal[\"entailment\", \"neutral\", \"contradiction\"] = dspy.OutputField(\n",
    "        desc=(\n",
    "            \"Return one of: 'entailment', 'neutral', or 'contradiction'.\\n\"\n",
    "            \"- 'entailment': The hypothesis must be true if the premise is true.\\n\"\n",
    "            \"- 'contradiction': The hypothesis must be false if the premise is true.\\n\"\n",
    "            \"- 'neutral': The hypothesis could be either true or false based on the premise.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create a basic predictor using the signature\n",
    "predictor = dspy.Predict(NLIImPresClassifier)\n",
    "\n",
    "# Define label names for mapping between numeric and string labels\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "def zero_shot_nli_classifier(x):\n",
    "    \"\"\"Apply zero-shot NLI classification to a single example.\n",
    "    \n",
    "    Args:\n",
    "        x: Dictionary containing 'premise', 'hypothesis', and 'gold_label'\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with the input fields plus predictions\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'premise': x['premise'],\n",
    "        'hypothesis': x['hypothesis'],\n",
    "        'pred_label': predictor(premise=x['premise'], hypothesis=x['hypothesis']).label,\n",
    "        'gold_label': label_names[x['gold_label']]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1719d8f8eaf51",
   "metadata": {},
   "source": [
    "## Load ImpPres dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47958515fef57a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:13.221042Z",
     "start_time": "2025-07-31T20:35:12.909475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_imppres_presuppositions.parquet\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from os.path import exists\n",
    "\n",
    "# Define sections\n",
    "sections = [\n",
    "    'presupposition_all_n_presupposition',\n",
    "    'presupposition_both_presupposition',\n",
    "    'presupposition_change_of_state',\n",
    "    'presupposition_cleft_existence',\n",
    "    'presupposition_cleft_uniqueness',\n",
    "    'presupposition_only_presupposition',\n",
    "    'presupposition_possessed_definites_existence',\n",
    "    'presupposition_possessed_definites_uniqueness',\n",
    "    'presupposition_question_presupposition'\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "if not exists('combined_imppres_presuppositions.parquet'):\n",
    "    # Load each section\n",
    "    for section in sections:\n",
    "        print(f\"Loading dataset for section: {section}\")\n",
    "        dataset[section] = load_dataset(\"facebook/imppres\", section)\n",
    "\n",
    "    # Convert to dataframes and combine\n",
    "    dataframes_list = []\n",
    "    for section, data in dataset.items():\n",
    "        df = data.to_pandas()\n",
    "        df['section'] = section\n",
    "        dataframes_list.append(df)\n",
    "\n",
    "    combined_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "else:\n",
    "    combined_df = pd.read_parquet('combined_imppres_presuppositions.parquet')\n",
    "    print(f\"Loaded combined_imppres_presuppositions.parquet\")\n",
    "\n",
    "# Convert back to datasets\n",
    "dataset = {}\n",
    "for section, group in combined_df.groupby(\"section\"):\n",
    "    dataset[section] = Dataset.from_pandas(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e4a87f5ab7d4a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:13.237138Z",
     "start_time": "2025-07-31T20:35:13.233739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presupposition_all_n_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_both_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_change_of_state': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_cleft_existence': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_cleft_uniqueness': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_only_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_possessed_definites_existence': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_possessed_definites_uniqueness': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " }),\n",
       " 'presupposition_question_presupposition': Dataset({\n",
       "     features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section', '__index_level_0__'],\n",
       "     num_rows: 1900\n",
       " })}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d7536a65ae709e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:13.298996Z",
     "start_time": "2025-07-31T20:35:13.289097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>0e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten senators that proved to ...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>2n</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>3e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>4c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17095</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>Travel shocks Janet.</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1895n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17096</th>\n",
       "      <td>The actors didn't conceal where that mall shoc...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1896c</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17097</th>\n",
       "      <td>Did the actors conceal where that mall shocks ...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1897n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17098</th>\n",
       "      <td>The actors might have concealed where that mal...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>modal</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1898n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17099</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>conditional</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1899n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17100 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0      All ten guys that proved to boast were divorcing.   \n",
       "1      All ten guys that proved to boast were divorcing.   \n",
       "2      All ten guys that proved to boast were divorcing.   \n",
       "3      All ten guys that proved to boast weren't divo...   \n",
       "4      All ten guys that proved to boast weren't divo...   \n",
       "...                                                  ...   \n",
       "17095  If the actors do conceal where that mall shock...   \n",
       "17096  The actors didn't conceal where that mall shoc...   \n",
       "17097  Did the actors conceal where that mall shocks ...   \n",
       "17098  The actors might have concealed where that mal...   \n",
       "17099  If the actors do conceal where that mall shock...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "0       There are exactly ten guys that proved to boast.      unembedded   \n",
       "1      There are exactly eleven guys that proved to b...      unembedded   \n",
       "2      There are exactly ten senators that proved to ...      unembedded   \n",
       "3       There are exactly ten guys that proved to boast.         negated   \n",
       "4      There are exactly eleven guys that proved to b...         negated   \n",
       "...                                                  ...             ...   \n",
       "17095                               Travel shocks Janet.     conditional   \n",
       "17096  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17097  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17098  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17099  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "0      Not_In_Example  Not_In_Example        positive           0   \n",
       "1      Not_In_Example  Not_In_Example         negated           2   \n",
       "2      Not_In_Example  Not_In_Example         neutral           1   \n",
       "3      Not_In_Example  Not_In_Example        positive           0   \n",
       "4      Not_In_Example  Not_In_Example         negated           2   \n",
       "...               ...             ...             ...         ...   \n",
       "17095  Not_In_Example  Not_In_Example         neutral           1   \n",
       "17096         negated      unembedded  Not_In_Example           2   \n",
       "17097   interrogative      unembedded  Not_In_Example           1   \n",
       "17098           modal      unembedded  Not_In_Example           1   \n",
       "17099     conditional      unembedded  Not_In_Example           1   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "0         all_n_presupposition     0e           0   \n",
       "1         all_n_presupposition     1c           0   \n",
       "2         all_n_presupposition     2n           0   \n",
       "3         all_n_presupposition     3e           0   \n",
       "4         all_n_presupposition     4c           0   \n",
       "...                        ...    ...         ...   \n",
       "17095  question_presupposition  1895n          99   \n",
       "17096  question_presupposition  1896c          99   \n",
       "17097  question_presupposition  1897n          99   \n",
       "17098  question_presupposition  1898n          99   \n",
       "17099  question_presupposition  1899n          99   \n",
       "\n",
       "                                      section  \n",
       "0         presupposition_all_n_presupposition  \n",
       "1         presupposition_all_n_presupposition  \n",
       "2         presupposition_all_n_presupposition  \n",
       "3         presupposition_all_n_presupposition  \n",
       "4         presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "17095  presupposition_question_presupposition  \n",
       "17096  presupposition_question_presupposition  \n",
       "17097  presupposition_question_presupposition  \n",
       "17098  presupposition_question_presupposition  \n",
       "17099  presupposition_question_presupposition  \n",
       "\n",
       "[17100 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5302e1aa89410d",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9e7a0ffbd08457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:21.148590Z",
     "start_time": "2025-07-31T20:35:13.343125Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd1d6e2a92bf94",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c419a6458455fe6",
   "metadata": {},
   "source": [
    "We will first run the dspy classifier through the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c068b1c86ed42dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:21.160332Z",
     "start_time": "2025-07-31T20:35:21.158586Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_metric(example, pred, *args):\n",
    "     return pred.label == example.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33c73280e69e73b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:22.448237Z",
     "start_time": "2025-07-31T20:35:21.208582Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Convert to DSPy Example objects\n",
    "dspy_examples = {}\n",
    "for section_name, section in dataset.items():\n",
    "    dspy_examples[section_name] = [\n",
    "        dspy.Example(\n",
    "            premise=ex['premise'],\n",
    "            hypothesis=ex['hypothesis'],\n",
    "            label=label_names[ex['gold_label']]\n",
    "        ).with_inputs(\"premise\", \"hypothesis\")\n",
    "        for ex in section\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame(dspy_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "409c65566eacfa62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:48.677735Z",
     "start_time": "2025-07-31T20:35:22.471783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating section:\tpresupposition_all_n_presupposition\n",
      "Average Metric: 1828.00 / 1900 (96.2%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1900/1900 [04:01<00:00,  7.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:25:50 INFO dspy.evaluate.evaluate: Average Metric: 1828 / 1900 (96.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of results:\t1900\n",
      "Evaluating section:\tpresupposition_both_presupposition\n",
      "Average Metric: 1418.00 / 1468 (96.6%):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1467/1900 [03:01<00:41, 10.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:28:52 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If both actors who weren't meeting do collaborate, it's okay.\", 'hypothesis': \"There are more than two actors who weren't meeting.\", 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is ‚Äî Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 484/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1834.00 / 1899 (96.6%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1900/1900 [03:54<00:00,  8.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:29:45 INFO dspy.evaluate.evaluate: Average Metric: 1834.0 / 1900 (96.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of results:\t1900\n",
      "Evaluating section:\tpresupposition_change_of_state\n",
      "Average Metric: 1058.00 / 1900 (55.7%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1900/1900 [04:12<00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:33:57 INFO dspy.evaluate.evaluate: Average Metric: 1058 / 1900 (55.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of results:\t1900\n",
      "Evaluating section:\tpresupposition_cleft_existence\n",
      "Average Metric: 674.00 / 1003 (67.2%):  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1003/1900 [02:01<08:16,  1.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:36:00 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"If it is that waitress who has failed to practice, it's okay\", 'hypothesis': 'Someone has failed to nod', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is ‚Äî Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 484/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1286.00 / 1899 (67.7%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1900/1900 [03:50<00:00,  8.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:37:48 INFO dspy.evaluate.evaluate: Average Metric: 1286.0 / 1900 (67.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of results:\t1900\n",
      "Evaluating section:\tpresupposition_cleft_uniqueness\n",
      "Average Metric: 261.00 / 549 (47.5%):  29%|‚ñà‚ñà‚ñâ       | 549/1900 [01:08<06:22,  3.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:38:56 ERROR dspy.utils.parallelizer: Error for Example({'premise': 'Is it Nancy that had driven to the college campuses?', 'hypothesis': 'Exactly one person had driven to the college campuses.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is ‚Äî Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 482/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 731.00 / 1535 (47.6%):  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1536/1900 [03:12<00:28, 12.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:41:00 ERROR dspy.utils.parallelizer: Error for Example({'premise': \"It isn't Cindy who was refusing to work with Chad.\", 'hypothesis': 'Exactly one person was refusing to work with Chad.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'}): Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.RateLimitError: RateLimitError: XaiException - Error code: 429 - {'code': 'Some resource has been exhausted', 'error': \"Too many requests for team bc29b82a-19d1-4c03-acf7-cd8b768453be and model grok-3-mini. Your team's rate limit is ‚Äî Requests per Second (actual/limit): 0/8, Requests per Minute (actual/limit): 481/480. You can view your team rate limits at https://console.x.ai.\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 901.00 / 1898 (47.5%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1900/1900 [03:48<00:00,  8.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:41:36 INFO dspy.evaluate.evaluate: Average Metric: 901.0 / 1900 (47.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of results:\t1900\n",
      "Evaluating section:\tpresupposition_only_presupposition\n",
      "Average Metric: 1282.00 / 1900 (67.5%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1900/1900 [03:59<00:00,  7.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:45:36 INFO dspy.evaluate.evaluate: Average Metric: 1282 / 1900 (67.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of results:\t1900\n",
      "Evaluating section:\tpresupposition_possessed_definites_existence\n",
      "Average Metric: 1773.00 / 1900 (93.3%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1900/1900 [03:57<00:00,  7.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:49:34 INFO dspy.evaluate.evaluate: Average Metric: 1773 / 1900 (93.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of results:\t1900\n",
      "Evaluating section:\tpresupposition_possessed_definites_uniqueness\n",
      "Average Metric: 908.00 / 1900 (47.8%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1900/1900 [03:53<00:00,  8.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:53:27 INFO dspy.evaluate.evaluate: Average Metric: 908 / 1900 (47.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of results:\t1900\n",
      "Evaluating section:\tpresupposition_question_presupposition\n",
      "Average Metric: 1617.00 / 1900 (85.1%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1900/1900 [04:01<00:00,  7.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:57:29 INFO dspy.evaluate.evaluate: Average Metric: 1617 / 1900 (85.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of results:\t1900\n"
     ]
    }
   ],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "from evaluate import combine, load\n",
    "\n",
    "# 1. Run DSPy evaluation for each section (here, limited to first 10 for demo)\n",
    "results = {}  # Store per-section predictions\n",
    "not_predicted = {}\n",
    "for sec in dspy_examples:\n",
    "    print(f\"Evaluating section:\\t{sec}\")\n",
    "    evaluator = Evaluate(\n",
    "        devset=dspy_examples[sec],\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    "        # max_errors=30\n",
    "    )\n",
    "    eval_res = evaluator(predictor)\n",
    "    _, result_tuples = eval_res\n",
    "    print(f\"number of results:\\t{len(result_tuples)}\")\n",
    "    preds, refs = [], []\n",
    "    not_predicted[sec] = {\n",
    "        'section':sec,\n",
    "        'num_not_predicted':0,\n",
    "        'not_predicted':[]\n",
    "    }\n",
    "    for example, prediction, correct in result_tuples:\n",
    "        if not hasattr(prediction, \"label\"):\n",
    "            not_predicted[sec]['num_not_predicted']+=1\n",
    "            not_predicted[sec]['not_predicted'].append((example, prediction, correct))\n",
    "            continue\n",
    "        preds.append(prediction.label)\n",
    "        refs.append(example.label)\n",
    "    results[sec] = {\"preds\": preds, \"refs\": refs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4746c046bb1c6e8",
   "metadata": {},
   "source": [
    "Let's display some statistics about the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ca7b91f2dc29bf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:48.741685Z",
     "start_time": "2025-07-31T20:35:48.730404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: presupposition_all_n_presupposition\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 868, 'contradiction': 571, 'entailment': 461})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1828\n",
      "  Accuracy (quick): 0.962\n",
      "\n",
      "Section: presupposition_both_presupposition\n",
      "  Total predictions: 1899\n",
      "  Total references:  1899\n",
      "  Class distribution in predictions: Counter({'neutral': 839, 'contradiction': 555, 'entailment': 505})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 599, 'entailment': 500})\n",
      "  Number of matches (agreement): 1834\n",
      "  Accuracy (quick): 0.966\n",
      "\n",
      "Section: presupposition_change_of_state\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 1563, 'contradiction': 222, 'entailment': 115})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1058\n",
      "  Accuracy (quick): 0.557\n",
      "\n",
      "Section: presupposition_cleft_existence\n",
      "  Total predictions: 1899\n",
      "  Total references:  1899\n",
      "  Class distribution in predictions: Counter({'neutral': 1404, 'contradiction': 310, 'entailment': 185})\n",
      "  Class distribution in references:  Counter({'neutral': 799, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1286\n",
      "  Accuracy (quick): 0.677\n",
      "\n",
      "Section: presupposition_cleft_uniqueness\n",
      "  Total predictions: 1898\n",
      "  Total references:  1898\n",
      "  Class distribution in predictions: Counter({'neutral': 1797, 'contradiction': 100, 'entailment': 1})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 498})\n",
      "  Number of matches (agreement): 901\n",
      "  Accuracy (quick): 0.475\n",
      "\n",
      "Section: presupposition_only_presupposition\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 1346, 'contradiction': 343, 'entailment': 211})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1282\n",
      "  Accuracy (quick): 0.675\n",
      "\n",
      "Section: presupposition_possessed_definites_existence\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 890, 'contradiction': 553, 'entailment': 457})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1773\n",
      "  Accuracy (quick): 0.933\n",
      "\n",
      "Section: presupposition_possessed_definites_uniqueness\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 1787, 'contradiction': 108, 'entailment': 5})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 908\n",
      "  Accuracy (quick): 0.478\n",
      "\n",
      "Section: presupposition_question_presupposition\n",
      "  Total predictions: 1900\n",
      "  Total references:  1900\n",
      "  Class distribution in predictions: Counter({'neutral': 997, 'contradiction': 565, 'entailment': 338})\n",
      "  Class distribution in references:  Counter({'neutral': 800, 'contradiction': 600, 'entailment': 500})\n",
      "  Number of matches (agreement): 1617\n",
      "  Accuracy (quick): 0.851\n",
      "\n",
      "=== OVERALL ===\n",
      "Total predictions: 17096\n",
      "Total references:  17096\n",
      "Class distribution in predictions: Counter({'neutral': 11491, 'contradiction': 3327, 'entailment': 2278})\n",
      "Class distribution in references:  Counter({'neutral': 7199, 'contradiction': 5399, 'entailment': 4498})\n",
      "Number of matches (agreement): 12487\n",
      "Accuracy (quick): 0.730\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for sec, data in results.items():\n",
    "    preds = data['preds']\n",
    "    refs = data['refs']\n",
    "    print(f\"Section: {sec}\")\n",
    "    print(f\"  Total predictions: {len(preds)}\")\n",
    "    print(f\"  Total references:  {len(refs)}\")\n",
    "    print(f\"  Class distribution in predictions: {Counter(preds)}\")\n",
    "    print(f\"  Class distribution in references:  {Counter(refs)}\")\n",
    "    agree = sum([p == r for p, r in zip(preds, refs)])\n",
    "    print(f\"  Number of matches (agreement): {agree}\")\n",
    "    print(f\"  Accuracy (quick): {agree / len(refs):.3f}\")\n",
    "    print()\n",
    "\n",
    "# Overall stats\n",
    "all_preds = sum([v['preds'] for v in results.values()], [])\n",
    "all_refs  = sum([v['refs']  for v in results.values()], [])\n",
    "print(\"=== OVERALL ===\")\n",
    "print(f\"Total predictions: {len(all_preds)}\")\n",
    "print(f\"Total references:  {len(all_refs)}\")\n",
    "print(f\"Class distribution in predictions: {Counter(all_preds)}\")\n",
    "print(f\"Class distribution in references:  {Counter(all_refs)}\")\n",
    "agree = sum([p == r for p, r in zip(all_preds, all_refs)])\n",
    "print(f\"Number of matches (agreement): {agree}\")\n",
    "print(f\"Accuracy (quick): {agree / len(all_refs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2ca8f76c7a8e6",
   "metadata": {},
   "source": [
    "We will now show information about non-predicted examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7955257ecf8e406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:48.797642Z",
     "start_time": "2025-07-31T20:35:48.787351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>presupposition_both_presupposition</td>\n",
       "      <td>([premise, hypothesis, label], [], 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>presupposition_change_of_state</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presupposition_cleft_existence</td>\n",
       "      <td>([premise, hypothesis, label], [], 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>presupposition_cleft_uniqueness</td>\n",
       "      <td>([premise, hypothesis, label], [], 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>presupposition_cleft_uniqueness</td>\n",
       "      <td>([premise, hypothesis, label], [], 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>presupposition_only_presupposition</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>presupposition_possessed_definites_existence</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>presupposition_possessed_definites_uniqueness</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         section  \\\n",
       "0            presupposition_all_n_presupposition   \n",
       "1             presupposition_both_presupposition   \n",
       "2                 presupposition_change_of_state   \n",
       "3                 presupposition_cleft_existence   \n",
       "4                presupposition_cleft_uniqueness   \n",
       "5                presupposition_cleft_uniqueness   \n",
       "6             presupposition_only_presupposition   \n",
       "7   presupposition_possessed_definites_existence   \n",
       "8  presupposition_possessed_definites_uniqueness   \n",
       "9         presupposition_question_presupposition   \n",
       "\n",
       "                                    detail  \n",
       "0                                      NaN  \n",
       "1  ([premise, hypothesis, label], [], 0.0)  \n",
       "2                                      NaN  \n",
       "3  ([premise, hypothesis, label], [], 0.0)  \n",
       "4  ([premise, hypothesis, label], [], 0.0)  \n",
       "5  ([premise, hypothesis, label], [], 0.0)  \n",
       "6                                      NaN  \n",
       "7                                      NaN  \n",
       "8                                      NaN  \n",
       "9                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Section: presupposition_all_n_presupposition ‚Äî 0 failures ===\n",
      "=== Section: presupposition_both_presupposition ‚Äî 1 failures ===\n",
      "Example({'premise': \"If both actors who weren't meeting do collaborate, it's okay.\", 'hypothesis': \"There are more than two actors who weren't meeting.\", 'label': 'contradiction'}) (input_keys={'premise', 'hypothesis'})\n",
      "üéØ Ref label: contradiction\n",
      "üí¨ Premise: If both actors who weren't meeting do collaborate, it's okay.\n",
      "üí¨ Hypothesis: There are more than two actors who weren't meeting.\n",
      "üõë Raw output: Prediction(\n",
      "    \n",
      ")\n",
      "‚ö†Ô∏è Score: 0.0\n",
      "----------------------------------------\n",
      "=== Section: presupposition_change_of_state ‚Äî 0 failures ===\n",
      "=== Section: presupposition_cleft_existence ‚Äî 1 failures ===\n",
      "Example({'premise': \"If it is that waitress who has failed to practice, it's okay\", 'hypothesis': 'Someone has failed to nod', 'label': 'neutral'}) (input_keys={'premise', 'hypothesis'})\n",
      "üéØ Ref label: neutral\n",
      "üí¨ Premise: If it is that waitress who has failed to practice, it's okay\n",
      "üí¨ Hypothesis: Someone has failed to nod\n",
      "üõë Raw output: Prediction(\n",
      "    \n",
      ")\n",
      "‚ö†Ô∏è Score: 0.0\n",
      "----------------------------------------\n",
      "=== Section: presupposition_cleft_uniqueness ‚Äî 2 failures ===\n",
      "Example({'premise': 'Is it Nancy that had driven to the college campuses?', 'hypothesis': 'Exactly one person had driven to the college campuses.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'})\n",
      "üéØ Ref label: entailment\n",
      "üí¨ Premise: Is it Nancy that had driven to the college campuses?\n",
      "üí¨ Hypothesis: Exactly one person had driven to the college campuses.\n",
      "üõë Raw output: Prediction(\n",
      "    \n",
      ")\n",
      "‚ö†Ô∏è Score: 0.0\n",
      "----------------------------------------\n",
      "Example({'premise': \"It isn't Cindy who was refusing to work with Chad.\", 'hypothesis': 'Exactly one person was refusing to work with Chad.', 'label': 'entailment'}) (input_keys={'premise', 'hypothesis'})\n",
      "üéØ Ref label: entailment\n",
      "üí¨ Premise: It isn't Cindy who was refusing to work with Chad.\n",
      "üí¨ Hypothesis: Exactly one person was refusing to work with Chad.\n",
      "üõë Raw output: Prediction(\n",
      "    \n",
      ")\n",
      "‚ö†Ô∏è Score: 0.0\n",
      "----------------------------------------\n",
      "=== Section: presupposition_only_presupposition ‚Äî 0 failures ===\n",
      "=== Section: presupposition_possessed_definites_existence ‚Äî 0 failures ===\n",
      "=== Section: presupposition_possessed_definites_uniqueness ‚Äî 0 failures ===\n",
      "=== Section: presupposition_question_presupposition ‚Äî 0 failures ===\n"
     ]
    }
   ],
   "source": [
    "df_np = pd.DataFrame(list(not_predicted.values())).set_index(\"section\")\n",
    "exploded = df_np[\"not_predicted\"].explode()\n",
    "df_details = (\n",
    "    exploded\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"section\", \"not_predicted\": \"detail\"})\n",
    "    .join(pd.json_normalize(exploded).add_prefix(\"detail.\"))\n",
    ")\n",
    "display(df_details)\n",
    "for sec, info in not_predicted.items():\n",
    "    print(f\"=== Section: {sec} ‚Äî {info['num_not_predicted']} failures ===\")\n",
    "    for ex, raw_out, score in info['not_predicted']:\n",
    "        print(ex)\n",
    "        premise, hypothesis, ref,= ex\n",
    "        print(f\"üéØ Ref label: {ex[ref]}\")\n",
    "        print(f\"üí¨ Premise: {ex[premise]}\")\n",
    "        print(f\"üí¨ Hypothesis: {ex[hypothesis]}\")\n",
    "        print(f\"üõë Raw output: {raw_out!r}\")\n",
    "        print(f\"‚ö†Ô∏è Score: {score}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20f5b67929c66534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:53.470522Z",
     "start_time": "2025-07-31T20:35:48.936298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for section: presupposition_all_n_presupposition\n",
      "Computing metrics for section: presupposition_both_presupposition\n",
      "Computing metrics for section: presupposition_change_of_state\n",
      "Computing metrics for section: presupposition_cleft_existence\n",
      "Computing metrics for section: presupposition_cleft_uniqueness\n",
      "Computing metrics for section: presupposition_only_presupposition\n",
      "Computing metrics for section: presupposition_possessed_definites_existence\n",
      "Computing metrics for section: presupposition_possessed_definites_uniqueness\n",
      "Computing metrics for section: presupposition_question_presupposition\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>presupposition_all_n_presupposition</th>\n",
       "      <td>0.962105</td>\n",
       "      <td>0.964902</td>\n",
       "      <td>0.962105</td>\n",
       "      <td>0.962229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_both_presupposition</th>\n",
       "      <td>0.965771</td>\n",
       "      <td>0.967112</td>\n",
       "      <td>0.965771</td>\n",
       "      <td>0.965764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_change_of_state</th>\n",
       "      <td>0.556842</td>\n",
       "      <td>0.654658</td>\n",
       "      <td>0.556842</td>\n",
       "      <td>0.491799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_cleft_existence</th>\n",
       "      <td>0.677199</td>\n",
       "      <td>0.811803</td>\n",
       "      <td>0.677199</td>\n",
       "      <td>0.658082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_cleft_uniqueness</th>\n",
       "      <td>0.474710</td>\n",
       "      <td>0.766148</td>\n",
       "      <td>0.474710</td>\n",
       "      <td>0.351054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_only_presupposition</th>\n",
       "      <td>0.674737</td>\n",
       "      <td>0.783490</td>\n",
       "      <td>0.674737</td>\n",
       "      <td>0.661324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_possessed_definites_existence</th>\n",
       "      <td>0.933158</td>\n",
       "      <td>0.937578</td>\n",
       "      <td>0.933158</td>\n",
       "      <td>0.933266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_possessed_definites_uniqueness</th>\n",
       "      <td>0.477895</td>\n",
       "      <td>0.603701</td>\n",
       "      <td>0.477895</td>\n",
       "      <td>0.357054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presupposition_question_presupposition</th>\n",
       "      <td>0.851053</td>\n",
       "      <td>0.871095</td>\n",
       "      <td>0.851053</td>\n",
       "      <td>0.848644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.730405</td>\n",
       "      <td>0.817392</td>\n",
       "      <td>0.730405</td>\n",
       "      <td>0.721824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               accuracy  precision    recall  \\\n",
       "section                                                                        \n",
       "presupposition_all_n_presupposition            0.962105   0.964902  0.962105   \n",
       "presupposition_both_presupposition             0.965771   0.967112  0.965771   \n",
       "presupposition_change_of_state                 0.556842   0.654658  0.556842   \n",
       "presupposition_cleft_existence                 0.677199   0.811803  0.677199   \n",
       "presupposition_cleft_uniqueness                0.474710   0.766148  0.474710   \n",
       "presupposition_only_presupposition             0.674737   0.783490  0.674737   \n",
       "presupposition_possessed_definites_existence   0.933158   0.937578  0.933158   \n",
       "presupposition_possessed_definites_uniqueness  0.477895   0.603701  0.477895   \n",
       "presupposition_question_presupposition         0.851053   0.871095  0.851053   \n",
       "all                                            0.730405   0.817392  0.730405   \n",
       "\n",
       "                                                     f1  \n",
       "section                                                  \n",
       "presupposition_all_n_presupposition            0.962229  \n",
       "presupposition_both_presupposition             0.965764  \n",
       "presupposition_change_of_state                 0.491799  \n",
       "presupposition_cleft_existence                 0.658082  \n",
       "presupposition_cleft_uniqueness                0.351054  \n",
       "presupposition_only_presupposition             0.661324  \n",
       "presupposition_possessed_definites_existence   0.933266  \n",
       "presupposition_possessed_definites_uniqueness  0.357054  \n",
       "presupposition_question_presupposition         0.848644  \n",
       "all                                            0.721824  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Prepare for metric calculation\n",
    "metric_prf = combine([\"precision\", \"recall\", \"f1\"])\n",
    "acc = load(\"accuracy\")\n",
    "rows = []\n",
    "all_preds, all_refs = [], []\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "for sec, data in results.items():\n",
    "    print(f\"Computing metrics for section: {sec}\")\n",
    "    preds = [label2id[label] for label in data[\"preds\"]]\n",
    "    refs  = [label2id[label] for label in data[\"refs\"]]\n",
    "    prf = metric_prf.compute(predictions=preds, references=refs, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "\n",
    "    rows.append({\"section\": sec, \"accuracy\": accuracy, **prf})\n",
    "    all_preds += preds\n",
    "    all_refs += refs\n",
    "\n",
    "# 3. Compute overall metrics\n",
    "overall_prf = metric_prf.compute(predictions=all_preds, references=all_refs, average=\"weighted\")\n",
    "overall_acc = acc.compute(predictions=all_preds, references=all_refs)[\"accuracy\"]\n",
    "rows.append({\"section\": \"all\", \"accuracy\": overall_acc, **overall_prf})\n",
    "\n",
    "# Create DataFrame and display\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "display(df_metrics.set_index(\"section\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fa69903b6adb1",
   "metadata": {},
   "source": [
    "In our experiment we got the following results:\n",
    "| section | accuracy | precision | recall | f1 |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| presupposition_all_n_presupposition | 0.962105 | 0.964902 | 0.962105 | 0.962229 |\n",
    "| presupposition_both_presupposition | 0.965771 | 0.967112 | 0.965771 | 0.965764 |\n",
    "| presupposition_change_of_state | 0.556842 | 0.654658 | 0.556842 | 0.491799 |\n",
    "| presupposition_cleft_existence | 0.677199 | 0.811803 | 0.677199 | 0.658082 |\n",
    "| presupposition_cleft_uniqueness | 0.474710 | 0.766148 | 0.474710 | 0.351054 |\n",
    "| presupposition_only_presupposition | 0.674737 | 0.783490 | 0.674737 | 0.661324 |\n",
    "| presupposition_possessed_definites_existence | 0.933158 | 0.937578 | 0.933158 | 0.933266 |\n",
    "| presupposition_possessed_definites_uniqueness | 0.477895 | 0.603701 | 0.477895 | 0.357054 |\n",
    "| presupposition_question_presupposition | 0.851053 | 0.871095 | 0.851053 | 0.848644 |\n",
    "| **all** | **0.730405** | **0.817392** | **0.730405** | **0.721824** |\n",
    "\n",
    "\n",
    "\n",
    "With a total F1 score of 0.730405 with grok-3-mini. Let's try to optimize the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8992aa33876994",
   "metadata": {},
   "source": [
    "## Optimizing the model\n",
    "We are going to try optimize the model in a couple ways.\n",
    "we will first create a dev\\test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed61f1096174010d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:53.505361Z",
     "start_time": "2025-07-31T20:35:53.488812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set size: 81\n",
      "Test set size: 17019\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "rng = random.default_rng(42)\n",
    "\n",
    "def stratified_split(df, n_per_section=10):\n",
    "    \"\"\"Split data keeping n examples per section for dev set.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        n_per_section: Number of examples to keep per section for dev set\n",
    "    \"\"\"\n",
    "    idx_dev = []\n",
    "    for (sec, lab), g in df.groupby([\"section\",\"gold_label\"]):\n",
    "        n = min(len(g), n_per_section // 3)  # Divide by 3 to account for label classes\n",
    "        idx = rng.permutation(g.index)\n",
    "        idx_dev.extend(idx[:n])\n",
    "    dev = df.loc[idx_dev]\n",
    "    test = df.drop(idx_dev)\n",
    "    return dev, test\n",
    "\n",
    "dev_df, test_df = stratified_split(combined_df)\n",
    "print(f\"Dev set size: {len(dev_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "777c40424bea1b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:53.563211Z",
     "start_time": "2025-07-31T20:35:53.554165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>All eight women that compel libraries to appre...</td>\n",
       "      <td>There are exactly eight women that compel libr...</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1795e</td>\n",
       "      <td>94</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Have the six guests that had badgered a lot of...</td>\n",
       "      <td>There are exactly six guests that had badgered...</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>500e</td>\n",
       "      <td>26</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>Do all eight women that compel libraries to ap...</td>\n",
       "      <td>There are exactly eight women that compel libr...</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1792e</td>\n",
       "      <td>94</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>Do all ten cashiers who weren't running around...</td>\n",
       "      <td>All ten cashiers who weren't running around th...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>567n</td>\n",
       "      <td>29</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>All four doors that open might have flung open.</td>\n",
       "      <td>There are exactly four mouths that open.</td>\n",
       "      <td>modal</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1113n</td>\n",
       "      <td>58</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16912</th>\n",
       "      <td>Marla finds out why these waitresses have reta...</td>\n",
       "      <td>A lot of teachers have retaliated.</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1712n</td>\n",
       "      <td>90</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15824</th>\n",
       "      <td>Had Mark figured out where Monet sells sweaters?</td>\n",
       "      <td>Mark has figured out where Monet sells sweaters.</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>624n</td>\n",
       "      <td>32</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15910</th>\n",
       "      <td>Does Ruth conceal why Derek figures out who mu...</td>\n",
       "      <td>Derek doesn't figure out who murmurs.</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>710c</td>\n",
       "      <td>37</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15707</th>\n",
       "      <td>If Tina does remember when Anne had bored Debr...</td>\n",
       "      <td>Anne hadn't bored Debra.</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>507c</td>\n",
       "      <td>26</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15462</th>\n",
       "      <td>Samuel doesn't forget about when those doctors...</td>\n",
       "      <td>Samuel does forget about when those doctors bake.</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>262c</td>\n",
       "      <td>13</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "1795   All eight women that compel libraries to appre...   \n",
       "500    Have the six guests that had badgered a lot of...   \n",
       "1792   Do all eight women that compel libraries to ap...   \n",
       "567    Do all ten cashiers who weren't running around...   \n",
       "1113     All four doors that open might have flung open.   \n",
       "...                                                  ...   \n",
       "16912  Marla finds out why these waitresses have reta...   \n",
       "15824   Had Mark figured out where Monet sells sweaters?   \n",
       "15910  Does Ruth conceal why Derek figures out who mu...   \n",
       "15707  If Tina does remember when Anne had bored Debr...   \n",
       "15462  Samuel doesn't forget about when those doctors...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "1795   There are exactly eight women that compel libr...           modal   \n",
       "500    There are exactly six guests that had badgered...   interrogative   \n",
       "1792   There are exactly eight women that compel libr...   interrogative   \n",
       "567    All ten cashiers who weren't running around th...  Not_In_Example   \n",
       "1113            There are exactly four mouths that open.           modal   \n",
       "...                                                  ...             ...   \n",
       "16912                 A lot of teachers have retaliated.      unembedded   \n",
       "15824   Mark has figured out where Monet sells sweaters.  Not_In_Example   \n",
       "15910              Derek doesn't figure out who murmurs.   interrogative   \n",
       "15707                           Anne hadn't bored Debra.     conditional   \n",
       "15462  Samuel does forget about when those doctors bake.  Not_In_Example   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "1795   Not_In_Example  Not_In_Example        positive           0   \n",
       "500    Not_In_Example  Not_In_Example        positive           0   \n",
       "1792   Not_In_Example  Not_In_Example        positive           0   \n",
       "567     interrogative      unembedded  Not_In_Example           1   \n",
       "1113   Not_In_Example  Not_In_Example         neutral           1   \n",
       "...               ...             ...             ...         ...   \n",
       "16912  Not_In_Example  Not_In_Example         neutral           1   \n",
       "15824   interrogative      unembedded  Not_In_Example           1   \n",
       "15910  Not_In_Example  Not_In_Example         negated           2   \n",
       "15707  Not_In_Example  Not_In_Example         negated           2   \n",
       "15462         negated      unembedded  Not_In_Example           2   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "1795      all_n_presupposition  1795e          94   \n",
       "500       all_n_presupposition   500e          26   \n",
       "1792      all_n_presupposition  1792e          94   \n",
       "567       all_n_presupposition   567n          29   \n",
       "1113      all_n_presupposition  1113n          58   \n",
       "...                        ...    ...         ...   \n",
       "16912  question_presupposition  1712n          90   \n",
       "15824  question_presupposition   624n          32   \n",
       "15910  question_presupposition   710c          37   \n",
       "15707  question_presupposition   507c          26   \n",
       "15462  question_presupposition   262c          13   \n",
       "\n",
       "                                      section  \n",
       "1795      presupposition_all_n_presupposition  \n",
       "500       presupposition_all_n_presupposition  \n",
       "1792      presupposition_all_n_presupposition  \n",
       "567       presupposition_all_n_presupposition  \n",
       "1113      presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "16912  presupposition_question_presupposition  \n",
       "15824  presupposition_question_presupposition  \n",
       "15910  presupposition_question_presupposition  \n",
       "15707  presupposition_question_presupposition  \n",
       "15462  presupposition_question_presupposition  \n",
       "\n",
       "[81 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>trigger</th>\n",
       "      <th>trigger1</th>\n",
       "      <th>trigger2</th>\n",
       "      <th>presupposition</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>0e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>1c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All ten guys that proved to boast were divorcing.</td>\n",
       "      <td>There are exactly ten senators that proved to ...</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>2n</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly ten guys that proved to boast.</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>3e</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All ten guys that proved to boast weren't divo...</td>\n",
       "      <td>There are exactly eleven guys that proved to b...</td>\n",
       "      <td>negated</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>2</td>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>4c</td>\n",
       "      <td>0</td>\n",
       "      <td>presupposition_all_n_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17095</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>Travel shocks Janet.</td>\n",
       "      <td>conditional</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1895n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17096</th>\n",
       "      <td>The actors didn't conceal where that mall shoc...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>negated</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>2</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1896c</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17097</th>\n",
       "      <td>Did the actors conceal where that mall shocks ...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>interrogative</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1897n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17098</th>\n",
       "      <td>The actors might have concealed where that mal...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>modal</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1898n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17099</th>\n",
       "      <td>If the actors do conceal where that mall shock...</td>\n",
       "      <td>The actors do conceal where that mall shocks J...</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>conditional</td>\n",
       "      <td>unembedded</td>\n",
       "      <td>Not_In_Example</td>\n",
       "      <td>1</td>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>1899n</td>\n",
       "      <td>99</td>\n",
       "      <td>presupposition_question_presupposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17019 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0      All ten guys that proved to boast were divorcing.   \n",
       "1      All ten guys that proved to boast were divorcing.   \n",
       "2      All ten guys that proved to boast were divorcing.   \n",
       "3      All ten guys that proved to boast weren't divo...   \n",
       "4      All ten guys that proved to boast weren't divo...   \n",
       "...                                                  ...   \n",
       "17095  If the actors do conceal where that mall shock...   \n",
       "17096  The actors didn't conceal where that mall shoc...   \n",
       "17097  Did the actors conceal where that mall shocks ...   \n",
       "17098  The actors might have concealed where that mal...   \n",
       "17099  If the actors do conceal where that mall shock...   \n",
       "\n",
       "                                              hypothesis         trigger  \\\n",
       "0       There are exactly ten guys that proved to boast.      unembedded   \n",
       "1      There are exactly eleven guys that proved to b...      unembedded   \n",
       "2      There are exactly ten senators that proved to ...      unembedded   \n",
       "3       There are exactly ten guys that proved to boast.         negated   \n",
       "4      There are exactly eleven guys that proved to b...         negated   \n",
       "...                                                  ...             ...   \n",
       "17095                               Travel shocks Janet.     conditional   \n",
       "17096  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17097  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17098  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "17099  The actors do conceal where that mall shocks J...  Not_In_Example   \n",
       "\n",
       "             trigger1        trigger2  presupposition  gold_label  \\\n",
       "0      Not_In_Example  Not_In_Example        positive           0   \n",
       "1      Not_In_Example  Not_In_Example         negated           2   \n",
       "2      Not_In_Example  Not_In_Example         neutral           1   \n",
       "3      Not_In_Example  Not_In_Example        positive           0   \n",
       "4      Not_In_Example  Not_In_Example         negated           2   \n",
       "...               ...             ...             ...         ...   \n",
       "17095  Not_In_Example  Not_In_Example         neutral           1   \n",
       "17096         negated      unembedded  Not_In_Example           2   \n",
       "17097   interrogative      unembedded  Not_In_Example           1   \n",
       "17098           modal      unembedded  Not_In_Example           1   \n",
       "17099     conditional      unembedded  Not_In_Example           1   \n",
       "\n",
       "                           UID pairID  paradigmID  \\\n",
       "0         all_n_presupposition     0e           0   \n",
       "1         all_n_presupposition     1c           0   \n",
       "2         all_n_presupposition     2n           0   \n",
       "3         all_n_presupposition     3e           0   \n",
       "4         all_n_presupposition     4c           0   \n",
       "...                        ...    ...         ...   \n",
       "17095  question_presupposition  1895n          99   \n",
       "17096  question_presupposition  1896c          99   \n",
       "17097  question_presupposition  1897n          99   \n",
       "17098  question_presupposition  1898n          99   \n",
       "17099  question_presupposition  1899n          99   \n",
       "\n",
       "                                      section  \n",
       "0         presupposition_all_n_presupposition  \n",
       "1         presupposition_all_n_presupposition  \n",
       "2         presupposition_all_n_presupposition  \n",
       "3         presupposition_all_n_presupposition  \n",
       "4         presupposition_all_n_presupposition  \n",
       "...                                       ...  \n",
       "17095  presupposition_question_presupposition  \n",
       "17096  presupposition_question_presupposition  \n",
       "17097  presupposition_question_presupposition  \n",
       "17098  presupposition_question_presupposition  \n",
       "17099  presupposition_question_presupposition  \n",
       "\n",
       "[17019 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(dev_df))\n",
    "display(pd.DataFrame(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72daff261fe243f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:54.186205Z",
     "start_time": "2025-07-31T20:35:53.623220Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_examples(df):\n",
    "    return [dspy.Example(\n",
    "        premise=r.premise, hypothesis=r.hypothesis,\n",
    "        label=label_names[r.gold_label]\n",
    "    ).with_inputs(\"premise\",\"hypothesis\") for r in df.itertuples()]\n",
    "dev_ex  = to_examples(dev_df)\n",
    "test_ex = to_examples(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77aea4441ccc9fc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:54.204836Z",
     "start_time": "2025-07-31T20:35:54.202644Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    results = Evaluate(\n",
    "        devset=test_ex[:120],  # Limit to 500 for faster evaluation\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=20,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    "    )(model)\n",
    "    score,results = results\n",
    "    print(f\"Score:\\t{score}\")\n",
    "    test_pred = [label2id[out[1].label] for out in results]\n",
    "    return score, results, test_pred\n",
    "\n",
    "def compute_matrices(test_pred):\n",
    "    prf = metric_prf.compute(predictions=test_pred, references=y_true[:120], average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=test_pred, references=y_true[:120])\n",
    "    return {**prf, **accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aec84683f7362d93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:35:59.168528Z",
     "start_time": "2025-07-31T20:35:54.258841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12434.00 / 17019 (73.1%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17019/17019 [01:11<00:00, 237.77it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 11:58:46 INFO dspy.evaluate.evaluate: Average Metric: 12434 / 17019 (73.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictor_test_predictions = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=50,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    ")(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37dfea0647d46d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:01.852911Z",
     "start_time": "2025-07-31T20:36:01.845253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 73.06\n"
     ]
    }
   ],
   "source": [
    "score,predictor_test_predictions_results = predictor_test_predictions\n",
    "print(f\"Score: {score}\")\n",
    "predictor_test_pred = [label2id[out[1].label] for out in predictor_test_predictions_results]\n",
    "y_true = [label2id[ex.label]  for ex in test_ex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24acf6b7d33c6e07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:01.965259Z",
     "start_time": "2025-07-31T20:36:01.915311Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_prf = metric_prf.compute(predictions=predictor_test_pred, references=y_true, average=\"weighted\")\n",
    "predictor_accuracy = acc.compute(predictions=predictor_test_pred, references=y_true)\n",
    "predictor_combined = {**predictor_prf, **predictor_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c81e5b6b99f149eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:01.989135Z",
     "start_time": "2025-07-31T20:36:01.984840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original predictor</th>\n",
       "      <td>0.81737</td>\n",
       "      <td>0.730595</td>\n",
       "      <td>0.722009</td>\n",
       "      <td>0.730595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    precision    recall        f1  accuracy\n",
       "Original predictor    0.81737  0.730595  0.722009  0.730595"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(predictor_combined, index=[\"Original predictor\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bc4a21287ce75",
   "metadata": {},
   "source": [
    "In our code we saw a F1 score of 0.722009 on the test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77973717f459d0b3",
   "metadata": {},
   "source": [
    "### Simply few-shot strategy over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b1318dbc67f041c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:08.222077Z",
     "start_time": "2025-07-31T20:36:02.046845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 23/81 [01:28<03:43,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 20 full traces after 23 examples for up to 1 rounds, amounting to 23 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "bs = BootstrapFewShot(metric=accuracy_metric, max_bootstrapped_demos=20, max_labeled_demos=16)\n",
    "overall_optimized = bs.compile(student=predictor, trainset=dev_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b6030edd5815279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:22.099046Z",
     "start_time": "2025-07-31T20:36:08.241009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12762.00 / 17019 (75.0%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17019/17019 [1:43:51<00:00,  2.73it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 13:47:06 INFO dspy.evaluate.evaluate: Average Metric: 12762 / 17019 (75.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Evaluate\n",
    "overall_report = Evaluate(\n",
    "        devset=test_ex,\n",
    "        metric=accuracy_metric,\n",
    "        return_outputs=True,\n",
    "        num_threads=10,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "        provide_traceback=False\n",
    ")(overall_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff158e4ed4f19e69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:22.126838Z",
     "start_time": "2025-07-31T20:36:22.119560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 74.99\n"
     ]
    }
   ],
   "source": [
    "# 2) Extract labels\n",
    "overall_score,overall_report_results = overall_report\n",
    "print(f\"Score: {overall_score}\")\n",
    "overall_test_pred = [label2id[out[1].label] for out in overall_report_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b49a734bbc008a1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:22.231660Z",
     "start_time": "2025-07-31T20:36:22.179344Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_prf = metric_prf.compute(predictions=overall_test_pred, references=y_true, average=\"weighted\")\n",
    "overall_accuracy = acc.compute(predictions=overall_test_pred, references=y_true)\n",
    "overall_combined = {**overall_prf, **overall_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7499fb9b43a28e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:36:22.255384Z",
     "start_time": "2025-07-31T20:36:22.251422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall_optimized predictor</th>\n",
       "      <td>0.826442</td>\n",
       "      <td>0.762768</td>\n",
       "      <td>0.758346</td>\n",
       "      <td>0.762768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             precision    recall        f1  accuracy\n",
       "Overall_optimized predictor   0.826442  0.762768  0.758346  0.762768"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(overall_combined, index=[\"Overall_optimized predictor\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3196a2632c1f64",
   "metadata": {},
   "source": [
    "When testing the overall model, we saw F1 Score of 0.758346, an improvement of 6.087%!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03933fa38856e3",
   "metadata": {},
   "source": [
    "### Adaptive few-shot strategy\n",
    "We will now try to optimize for each section and create a new model which will predicate by majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df6539e3ac2fee22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:48.644429Z",
     "start_time": "2025-07-31T13:55:48.592251Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_dev_ex = {sec: to_examples(group) for sec, group in dev_df.groupby(\"section\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1bfbe1173b8e658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:50.395721Z",
     "start_time": "2025-07-31T13:55:48.667649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for section: presupposition_all_n_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8/9 [00:37<00:04,  4.74s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n",
      "Optimizing for section: presupposition_both_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:43<00:00,  4.83s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_change_of_state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:49<00:00,  5.47s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_cleft_existence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:39<00:00,  4.36s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_cleft_uniqueness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:42<00:00,  4.72s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_only_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:44<00:00,  4.90s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_possessed_definites_existence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8/9 [00:28<00:03,  3.57s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n",
      "Optimizing for section: presupposition_possessed_definites_uniqueness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:41<00:00,  4.59s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 8 examples for up to 1 rounds, amounting to 9 attempts.\n",
      "Optimizing for section: presupposition_question_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8/9 [00:37<00:04,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_pipelines = {}\n",
    "\n",
    "for sec in sec_dev_ex:\n",
    "    print(f\"Optimizing for section: {sec}\")\n",
    "    # Flatten dev examples for prompt tuning\n",
    "    dev_set = sec_dev_ex[sec]\n",
    "\n",
    "    # Initialize optimizer\n",
    "    bs = BootstrapFewShot(\n",
    "        metric=accuracy_metric,\n",
    "        max_bootstrapped_demos=8,\n",
    "        max_labeled_demos=4\n",
    "    )\n",
    "\n",
    "    # Compile and tune using dev split\n",
    "    compiled = bs.compile(\n",
    "        student=predictor,\n",
    "        trainset=dev_set\n",
    "    )\n",
    "    optimized_pipelines[sec] = compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae6e76930f5c4558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:50.399127009Z",
     "start_time": "2025-07-23T16:05:04.050005Z"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_tp = dspy.Ensemble(reduce_fn=dspy.majority)\n",
    "adaptive_optimized = ensemble_tp.compile(list(optimized_pipelines.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b0d08aed1459bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:50.399739464Z",
     "start_time": "2025-07-23T16:05:04.112934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 114.00 / 120 (95.0%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120/120 [00:00<00:00, 1130.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:04:29 INFO dspy.evaluate.evaluate: Average Metric: 114 / 120 (95.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:\t95.0\n"
     ]
    }
   ],
   "source": [
    "adaptive_report = evaluate(adaptive_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0ca3387c359351f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:50.407770259Z",
     "start_time": "2025-07-23T17:44:38.474610Z"
    }
   },
   "outputs": [],
   "source": [
    "adaptive_score,adaptive_report_results,adaptive_test_pred = adaptive_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "383d148a01fdfb84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:55:50.409530486Z",
     "start_time": "2025-07-23T17:44:40.076006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adaptive_optimized predictor</th>\n",
       "      <td>0.955455</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.949911</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              precision  recall        f1  accuracy\n",
       "Adaptive_optimized predictor   0.955455    0.95  0.949911      0.95"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adaptive_combined = compute_matrices(adaptive_test_pred)\n",
    "display(pd.DataFrame(adaptive_combined, index=[\"Adaptive_optimized predictor\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a03cf39db51171",
   "metadata": {},
   "source": [
    "We got:\n",
    "0.821135,0.744834,0.738229,0.744834\n",
    "this shows around 0.02 improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e408e95c79e5bb1",
   "metadata": {},
   "source": [
    "### Few shots with Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8288e547ee09a9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:38:27.331641Z",
     "start_time": "2025-07-31T20:36:22.321419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 20 traces per predictor.\n",
      "Will attempt to bootstrap 1 candidate sets.\n",
      "Average Metric: 53.00 / 81 (65.4%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:05<00:00, 15.50it/s]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:05:34 INFO dspy.evaluate.evaluate: Average Metric: 53 / 81 (65.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 65.43 for seed -3\n",
      "Scores so far: [65.43]\n",
      "Best score so far: 65.43\n",
      "Average Metric: 61.00 / 81 (75.3%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:16<00:00,  4.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:05:53 INFO dspy.evaluate.evaluate: Average Metric: 61 / 81 (75.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 75.31 for seed -2\n",
      "Scores so far: [65.43, 75.31]\n",
      "Best score so far: 75.31\n",
      "  0%|          | 0/17019 [05:14<?, ?it/s]\n",
      "Average Metric: 121.00 / 127 (95.3%):  25%|‚ñà‚ñà‚ñå       | 127/500 [03:53<11:26,  1.84s/it]\n",
      "Average Metric: 121.00 / 127 (95.3%):  25%|‚ñà‚ñà‚ñå       | 127/500 [03:53<11:26,  1.84s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 23/81 [00:03<00:09,  6.31it/s]\n",
      " 28%|‚ñà‚ñà‚ñä       | 23/81 [00:03<00:09,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 20 full traces after 23 examples for up to 1 rounds, amounting to 23 attempts.\n",
      "Average Metric: 53.00 / 81 (65.4%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:12<00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:06:27 INFO dspy.evaluate.evaluate: Average Metric: 53 / 81 (65.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [65.43, 75.31, 65.43]\n",
      "Best score so far: 75.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñå       | 21/81 [01:16<03:38,  3.64s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 13 full traces after 21 examples for up to 1 rounds, amounting to 21 attempts.\n",
      "Average Metric: 55.00 / 81 (67.9%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:21<00:00,  3.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:08:19 INFO dspy.evaluate.evaluate: Average Metric: 55 / 81 (67.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [65.43, 75.31, 65.43, 67.9]\n",
      "Best score so far: 75.31\n",
      "4 candidate programs found.\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "bsrs = BootstrapFewShotWithRandomSearch(\n",
    "    metric=accuracy_metric,\n",
    "    max_bootstrapped_demos=20,\n",
    "    max_labeled_demos=9,\n",
    "    num_candidate_programs=1,\n",
    "    num_threads=30\n",
    ")\n",
    "opted_rs = bsrs.compile(student=predictor, trainset=dev_ex, valset=dev_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7aade7a8f2e5f2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:48:51.620718Z",
     "start_time": "2025-07-31T20:38:31.572843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 115.00 / 120 (95.8%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120/120 [00:28<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:08:48 INFO dspy.evaluate.evaluate: Average Metric: 115 / 120 (95.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:\t95.83\n"
     ]
    }
   ],
   "source": [
    "opted_rs_report = evaluate(opted_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "359c6cd63229607e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T20:48:51.672843Z",
     "start_time": "2025-07-31T20:48:51.663071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>opted_rs[_optimized predictor</th>\n",
       "      <td>0.962191</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958355</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               precision    recall        f1  accuracy\n",
       "opted_rs[_optimized predictor   0.962191  0.958333  0.958355  0.958333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opted_rs_score,opted_rs_report_results,opted_rs_test_pred= opted_rs_report\n",
    "opted_rs_combined = compute_matrices(opted_rs_test_pred)\n",
    "display(pd.DataFrame(opted_rs_combined, index=[\"opted_rs[_optimized predictor\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a46f92ab6043c",
   "metadata": {},
   "source": [
    "## Optimizing with MIPRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dffca042122d056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:39:04.597544Z",
     "start_time": "2025-07-31T20:59:53.434829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 14:16:19 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/08/02 14:16:19 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/08/02 14:16:19 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/08/02 14:16:19 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=12 sets of demonstrations...\n",
      "2025/08/02 14:16:19 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=12 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/12\n",
      "Bootstrapping set 2/12\n",
      "Bootstrapping set 2/12\n",
      "Bootstrapping set 3/12\n",
      "Bootstrapping set 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñé       | 4/17 [00:17<00:55,  4.29s/it]"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "random.seed(42)\n",
    "mipro = MIPROv2(\n",
    "    metric=accuracy_metric,\n",
    "    verbose=True,\n",
    "    auto=None,  # Disable auto mode to set custom params\n",
    "    num_candidates=12,  # Required when auto=None; controls candidates for few-shots/instructions\n",
    "    init_temperature=1.0\n",
    ")\n",
    "opted_mipro = mipro.compile(\n",
    "    predictor,\n",
    "    trainset=dev_ex,\n",
    "    num_trials=15,  # Number of optimization trials\n",
    "    max_bootstrapped_demos=8,  # Demos per few-shot set\n",
    "    max_labeled_demos=4,\n",
    "    minibatch=True,  # Enable minibatching for efficiency\n",
    "    minibatch_size=20,\n",
    "    minibatch_full_eval_steps=5,  # Full val eval every 5 minibatch steps\n",
    "    requires_permission_to_run=False  # Skip confirmation prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488952d6e2e2a5c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:39:09.810337Z",
     "start_time": "2025-07-31T21:39:04.740529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2181.00 / 2986 (73.0%):  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 2985/5130 [00:01<00:01, 1704.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mipro_report = evaluate(opted_mipro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3b4dc66a11dc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:39:09.940197Z",
     "start_time": "2025-07-31T21:39:09.829953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 72.48\n"
     ]
    }
   ],
   "source": [
    "mipro_score,mipro_report_results,mipro_test_pred = mipro_report\n",
    "mipro_combined = compute_matrices(mipro_test_pred)\n",
    "display(pd.DataFrame(mipro_combined, index=[\"mipro_combined predictor\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd022cac18fef56d",
   "metadata": {},
   "source": [
    "### ensembling mipro2 with BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb3f14d590a71b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:39:10.025844Z",
     "start_time": "2025-07-31T21:39:10.023206Z"
    }
   },
   "outputs": [],
   "source": [
    "from dspy.teleprompt import Ensemble\n",
    "ensemble = Ensemble(reduce_fn=dspy.majority)\n",
    "combined = ensemble.compile([opted_rs, opted_mipro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977c881515b38b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T21:39:45.542846Z",
     "start_time": "2025-07-31T21:39:10.124719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3523.00 / 4562 (77.2%):  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 4561/5130 [00:15<00:00, 1856.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3950.00 / 5130 (77.0%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5130/5130 [00:34<00:00, 149.08it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/01 00:39:45 INFO dspy.evaluate.evaluate: Average Metric: 3950 / 5130 (77.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:\t77.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>combined predictor</th>\n",
       "      <td>0.829834</td>\n",
       "      <td>0.769981</td>\n",
       "      <td>0.766227</td>\n",
       "      <td>0.769981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    precision    recall        f1  accuracy\n",
       "combined predictor   0.829834  0.769981  0.766227  0.769981"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_score, combined_report, combined_test_pred = evaluate(combined)\n",
    "combined_combined = compute_matrices(combined_test_pred)\n",
    "display(pd.DataFrame(combined_combined, index=[\"combined predictor\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5ae8dcfa60120",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "metric_prf = combine([\"precision\", \"recall\", \"f1\"])\n",
    "acc = load(\"accuracy\")\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc383dc3a22118",
   "metadata": {},
   "source": [
    "Let's examine the results:\n",
    "\n",
    "| Section                                          | Accuracy | Precision | Recall   | F1 Score |\n",
    "|--------------------------------------------------|----------|-----------|----------|----------|\n",
    "| presupposition_all_n_presupposition              | 0.991228 | 0.991400  | 0.991228 | 0.991237 |\n",
    "| presupposition_both_presupposition               | 0.984211 | 0.984519  | 0.984211 | 0.984191 |\n",
    "| presupposition_change_of_state                   | 0.557895 | 0.652114  | 0.557895 | 0.491531 |\n",
    "| presupposition_cleft_existence                   | 0.743860 | 0.835004  | 0.743860 | 0.736982 |\n",
    "| presupposition_cleft_uniqueness                  | 0.496491 | 0.769661  | 0.496491 | 0.385733 |\n",
    "| presupposition_only_presupposition               | 0.700000 | 0.813519  | 0.700000 | 0.685228 |\n",
    "| presupposition_possessed_definites_existence     | 0.964912 | 0.965735  | 0.964912 | 0.964956 |\n",
    "| presupposition_possessed_definites_uniqueness    | 0.463158 | 0.769068  | 0.463158 | 0.343951 |\n",
    "| presupposition_question_presupposition           | 0.863158 | 0.875327  | 0.863158 | 0.861277 |\n",
    "| all                                              | 0.751657 | 0.828935  | 0.751657 | 0.745117 |\n",
    "\n",
    "Total F1 score of 0.745, not that much of an improvement :(\n",
    "\n",
    "let's try to optimize it in another way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2cfb5e9f7637ee",
   "metadata": {},
   "source": [
    "### Prepare prediction variables for comparison\n",
    "\n",
    "Before comparing with DeBERTa, let's prepare all the prediction variables we need:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396b7e758643182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric predictions back to string labels for comparison\n",
    "id2label = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "\n",
    "# Zero-shot predictions (from the original predictor)\n",
    "zs_preds = [id2label[pred] for pred in predictor_test_pred]\n",
    "\n",
    "# Bootstrap few-shot predictions (from overall_optimized)\n",
    "bs_preds = [id2label[pred] for pred in overall_test_pred]\n",
    "\n",
    "# Random search predictions (from opted_rs)\n",
    "rs_report = Evaluate(\n",
    "    devset=test_ex,\n",
    "    metric=accuracy_metric,\n",
    "    return_outputs=True,\n",
    "    num_threads=50,\n",
    "    display_progress=True,\n",
    "    display_table=False,\n",
    "    provide_traceback=False\n",
    ")(opted_rs)\n",
    "rs_score, rs_results = rs_report\n",
    "rs_preds = [id2label[label2id[out[1].label]] for out in rs_results]\n",
    "\n",
    "# MIPROv2 predictions (from opted_mipro)\n",
    "mi_preds = [id2label[pred] for pred in mipro_test_pred]\n",
    "\n",
    "# Ensemble predictions (from combined model)\n",
    "ens_preds = [id2label[pred] for pred in combined_test_pred]\n",
    "\n",
    "# Add gold_label_str column to test_df\n",
    "test_df = test_df.copy()\n",
    "test_df['gold_label_str'] = test_df['gold_label'].map(id2label)\n",
    "\n",
    "# Define hf_metrics function\n",
    "def hf_metrics(preds, refs):\n",
    "    \"\"\"Compute HuggingFace metrics for predictions and references\"\"\"\n",
    "    pred_ids = [label2id[p] for p in preds]\n",
    "    ref_ids = [label2id[r] for r in refs]\n",
    "    \n",
    "    prf = metric_prf.compute(predictions=pred_ids, references=ref_ids, average=\"weighted\")\n",
    "    accuracy = acc.compute(predictions=pred_ids, references=ref_ids)\n",
    "    \n",
    "    return {**prf, **accuracy}\n",
    "\n",
    "print(\"All prediction variables prepared successfully!\")\n",
    "print(f\"zs_preds length: {len(zs_preds)}\")\n",
    "print(f\"bs_preds length: {len(bs_preds)}\")\n",
    "print(f\"rs_preds length: {len(rs_preds)}\")\n",
    "print(f\"mi_preds length: {len(mi_preds)}\")\n",
    "print(f\"ens_preds length: {len(ens_preds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdcf07f41b5409",
   "metadata": {},
   "source": [
    "### Comparing with DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a08c81b0aec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import exists\n",
    "\n",
    "# Load DeBERTa predictions if available\n",
    "if exists(\"deberta_item_preds.parquet\"):\n",
    "    deb = pd.read_parquet(\"deberta_item_preds.parquet\")\n",
    "    print(\"Loaded DeBERTa predictions from deberta_item_preds.parquet\")\n",
    "else:\n",
    "    print(\"Warning: deberta_item_preds.parquet not found!\")\n",
    "    print(\"This file should be generated by running imppres_baseline.ipynb first.\")\n",
    "    print(\"Skipping DeBERTa comparison section...\")\n",
    "    \n",
    "    # Create a summary of our LLM models without DeBERTa comparison\n",
    "    def pack_metrics_simple(name, preds):\n",
    "        return {\"model\": name, **hf_metrics(preds, test_df.gold_label_str.tolist())}\n",
    "\n",
    "    summary = [\n",
    "        pack_metrics_simple(\"ZeroShot\", zs_preds),\n",
    "        pack_metrics_simple(\"BootstrapFS\", bs_preds),\n",
    "        pack_metrics_simple(\"RandSearch\", rs_preds),\n",
    "        pack_metrics_simple(\"MIPROv2\", mi_preds),\n",
    "        pack_metrics_simple(\"Ensemble(RS+MI)\", ens_preds),\n",
    "    ]\n",
    "    summary_df = pd.DataFrame(summary).set_index(\"model\").sort_values(\"f1\", ascending=False)\n",
    "    print(\"\\nLLM Model Performance Summary:\")\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Exit early if no DeBERTa predictions\n",
    "    deb = None\n",
    "\n",
    "# Only run DeBERTa comparison if predictions are available\n",
    "if deb is not None:\n",
    "    # Build df for the current LLM model (example: zero-shot)\n",
    "    llm_df = test_df[[\"UID\",\"section\",\"gold_label_str\"]].copy()\n",
    "    llm_df[\"llm_pred\"] = zs_preds  # or bs_preds / rs_preds / ...\n",
    "\n",
    "    # Join\n",
    "    merged = llm_df.merge(deb[[\"UID\",\"deberta_pred\"]], on=\"UID\", how=\"inner\")\n",
    "\n",
    "    # Agreement counts\n",
    "    def agreement_counts(df, gold_col=\"gold_label_str\", p1=\"llm_pred\", p2=\"deberta_pred\"):\n",
    "        g = df[gold_col].values\n",
    "        a = df[p1].values\n",
    "        b = df[p2].values\n",
    "        both_correct  = ((a==g) & (b==g)).sum()\n",
    "        correct1_only = ((a==g) & (b!=g)).sum()\n",
    "        correct2_only = ((b==g) & (a!=g)).sum()\n",
    "        both_wrong    = ((a!=g) & (b!=g)).sum()\n",
    "        return both_correct, correct1_only, correct2_only, both_wrong\n",
    "\n",
    "    both, c1, c2, wrong = agreement_counts(merged)\n",
    "    agree_table = pd.DataFrame(\n",
    "        [[both, c1, c2, wrong]],\n",
    "        columns=[\"Correct (both)\", \"Correct1 (LLM only)\", \"Correct2 (DeBERTa only)\", \"Incorrect (both)\"],\n",
    "        index=[\"ZeroShot_vs_DeBERTa\"]\n",
    "    )\n",
    "    display(agree_table)\n",
    "\n",
    "    # Per-section agreement\n",
    "    def per_section_agreement(df):\n",
    "        rows = []\n",
    "        for sec, g in df.groupby(\"section\"):\n",
    "            b, c1, c2, w = agreement_counts(g)\n",
    "            rows.append([sec, b, c1, c2, w])\n",
    "        return pd.DataFrame(rows, columns=[\"section\",\"Correct\",\"Correct1\",\"Correct2\",\"Incorrect\"]).set_index(\"section\")\n",
    "\n",
    "    display(per_section_agreement(merged))\n",
    "\n",
    "    #%%\n",
    "    def compare_to_deberta(name, preds):\n",
    "        tmp = test_df[[\"UID\",\"section\",\"gold_label_str\"]].copy()\n",
    "        tmp[\"llm_pred\"] = preds\n",
    "        mer = tmp.merge(deb[[\"UID\",\"deberta_pred\"]], on=\"UID\")\n",
    "        b,c1,c2,w = agreement_counts(mer)\n",
    "        return pd.Series({\"model\":name,\"Correct\":b,\"Correct1\":c1,\"Correct2\":c2,\"Incorrect\":w})\n",
    "\n",
    "    rows = []\n",
    "    rows.append(compare_to_deberta(\"ZeroShot\", zs_preds))\n",
    "    rows.append(compare_to_deberta(\"BootstrapFS\", bs_preds))\n",
    "    rows.append(compare_to_deberta(\"RandSearch\", rs_preds))\n",
    "    rows.append(compare_to_deberta(\"MIPROv2\", mi_preds))\n",
    "    rows.append(compare_to_deberta(\"Ensemble(RS+MI)\", ens_preds))\n",
    "\n",
    "    agree_all_df = pd.DataFrame(rows).set_index(\"model\")\n",
    "    display(agree_all_df)\n",
    "    #%%\n",
    "    def pack_metrics(name, preds):\n",
    "        return {\"model\": name, **hf_metrics(preds, test_df.gold_label_str.tolist())}\n",
    "\n",
    "    summary = [\n",
    "        pack_metrics(\"ZeroShot\", zs_preds),\n",
    "        pack_metrics(\"BootstrapFS\", bs_preds),\n",
    "        pack_metrics(\"RandSearch\", rs_preds),\n",
    "        pack_metrics(\"MIPROv2\", mi_preds),\n",
    "        pack_metrics(\"Ensemble(RS+MI)\", ens_preds),\n",
    "        pack_metrics(\"DeBERTa\", deb.loc[deb.UID.isin(test_df.UID),\"deberta_pred\"].tolist()),\n",
    "    ]\n",
    "    summary_df = pd.DataFrame(summary).set_index(\"model\").sort_values(\"f1\", ascending=False)\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print(\"DeBERTa comparison section skipped due to missing predictions file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
