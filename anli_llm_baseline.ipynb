{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cec0d02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:49:04.954776Z",
     "start_time": "2025-07-09T15:49:04.952312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load API key from file\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "# Read the key from grok_key.ini\n",
    "with open('grok_key.ini', 'r') as f:\n",
    "    line = f.read().strip()\n",
    "    # Extract the key from \"export XAI_API_KEY=your_key_here\"\n",
    "    if line.startswith('export XAI_API_KEY='):\n",
    "        api_key = line.split('=', 1)[1]\n",
    "        os.environ['XAI_API_KEY'] = api_key\n",
    "        print(\"API key loaded successfully\")\n",
    "    else:\n",
    "        print(\"Could not parse API key from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8777c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "#xai\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "dspy.configure(lm=lm)\n",
    "# for ollama\n",
    "# lm = dspy.LM('ollama_chat/llama3.2', api_base='http://localhost:11434', api_key='')\n",
    "# dspy.configure(lm=lm)\n",
    "# lm = dspy.LM(\n",
    "#     \"ollama/llama3.2:latest\",\n",
    "#     api_base=\"http://localhost:11434\",\n",
    "#     format=\"json\"        # litellm translates this to Ollama's stream=false\n",
    "# )\n",
    "#dspy.configure(lm=lm, adapter=dspy.JSONAdapter())  # ask DSPy to keep JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b60da44b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T16:02:08.100066Z",
     "start_time": "2025-07-09T16:02:08.094177Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy classifier program.\n",
    "class NLIClassifier(dspy.Signature):\n",
    "    premise     :str = dspy.InputField(desc=\"A short passage or statement. All facts should be inferred from this text alone.\")\n",
    "    hypothesis  :str = dspy.InputField(desc=\"A second statement to evaluate. Check if this follows from, contradicts, or is unrelated to the premise.\")\n",
    "    label       : Literal[\"entailment\", \"neutral\", \"contradiction\"] = dspy.OutputField(\n",
    "        desc=(\n",
    "            \"Return one of: 'entailment', 'neutral', or 'contradiction'.\\n\"\n",
    "            \"- 'entailment': The hypothesis must be true if the premise is true.\\n\"\n",
    "            \"- 'contradiction': The hypothesis must be false if the premise is true.\\n\"\n",
    "            \"- 'neutral': The hypothesis could be either true or false based on the premise.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "predictor = dspy.Predict(NLIClassifier)\n",
    "\n",
    "def zero_shot_nli_classifier(x):\n",
    "    # if hasattr(x,'premise') and hasattr(x,'hypothesis'):\n",
    "    return predictor(premise=x['premise'], hypothesis=x['hypothesis']).label\n",
    "    # print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0438789b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:49:11.888190Z",
     "start_time": "2025-07-09T15:49:05.019909Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59927ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:49:11.897034Z",
     "start_time": "2025-07-09T15:49:11.894774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "596338cc3307306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T16:02:10.912905Z",
     "start_time": "2025-07-09T16:02:10.905642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entailment\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "example = dataset['test_r3'][0]\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "print(label_names[example['label']])\n",
    "print(zero_shot_nli_classifier(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9170647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 658/1200 [00:08<00:06, 79.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 500 full traces after 658 examples for up to 2 rounds, amounting to 843 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets optimize\n",
    "from dspy import BootstrapFewShot\n",
    "\n",
    "def accuracy_metric(example, pred, *args):\n",
    "    return int(pred.label.strip().lower() == example[\"label\"])\n",
    "\n",
    "opt = BootstrapFewShot(\n",
    "    metric=accuracy_metric,\n",
    "    max_bootstrapped_demos=500,\n",
    "    max_labeled_demos=20,\n",
    "    max_rounds=2,\n",
    ")\n",
    "def convert_dict(ex):\n",
    "    return (\n",
    "        dspy.Example(\n",
    "           premise=ex[\"premise\"],\n",
    "           hypothesis=ex[\"hypothesis\"],\n",
    "           label={0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}[ex[\"label\"]]\n",
    "        )\n",
    "        .with_inputs(\"premise\", \"hypothesis\")\n",
    "    )\n",
    "\n",
    "trainset = [convert_dict(x) for x in dataset['dev_r3'].to_list()]\n",
    "\n",
    "compiled_clf = opt.compile(predictor, trainset=trainset)  # returns an *improved* module\n",
    "\n",
    "# small_trainset = trainset[:200]\n",
    "# print(f\"üìö Using {len(small_trainset)} examples for optimization\")\n",
    "\n",
    "# compiled_clf = opt.compile(predictor, trainset=small_trainset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02512c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot model in 24 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "zero-shot batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:00<00:00, 61.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating optimized model in 24 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "optimized batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:35<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def convert_dict_for_eval(ex):\n",
    "    return {\n",
    "        'premise': ex[\"premise\"],\n",
    "        'hypothesis': ex[\"hypothesis\"],\n",
    "        'label': {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}[ex[\"label\"]]\n",
    "    }\n",
    "\n",
    "testset = [convert_dict_for_eval(x) for x in dataset['test_r3'].to_list()]\n",
    "\n",
    "def evaluate_with_progress(model_func, model_name, testset, batch_size=50):\n",
    "    \"\"\"Evaluate model with progress tracking\"\"\"\n",
    "    predictions = []\n",
    "    total_batches = (len(testset) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Evaluating {model_name} model in {total_batches} batches...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(testset), batch_size), desc=f\"{model_name} batches\"):\n",
    "        batch = testset[i:i + batch_size]\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        for example in batch:\n",
    "            if model_name == \"zero-shot\":\n",
    "                pred = model_func(example)\n",
    "            else:  # optimized\n",
    "                pred = model_func(premise=example['premise'], hypothesis=example['hypothesis']).label\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        # Optional: print batch timing\n",
    "        # print(f\"Batch {i//batch_size + 1} completed in {batch_time:.2f}s\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Run evaluations\n",
    "zero_shot_predictions = evaluate_with_progress(zero_shot_nli_classifier, \"zero-shot\", testset)\n",
    "optimized_predictions = evaluate_with_progress(compiled_clf, \"optimized\", testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb051c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATION RESULTS ===\n",
      "Total test examples: 1200\n",
      "Zero-shot predictions: 1200\n",
      "Optimized predictions: 1200\n",
      "\n",
      "=== ZERO-SHOT MODEL RESULTS ===\n",
      "Accuracy: 0.675\n",
      "\n",
      "Detailed Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.81      0.60      0.69       396\n",
      "   entailment       0.92      0.55      0.69       402\n",
      "      neutral       0.53      0.88      0.66       402\n",
      "\n",
      "     accuracy                           0.68      1200\n",
      "    macro avg       0.75      0.67      0.68      1200\n",
      " weighted avg       0.75      0.68      0.68      1200\n",
      "\n",
      "\n",
      "=== OPTIMIZED MODEL RESULTS ===\n",
      "Accuracy: 0.688\n",
      "\n",
      "Detailed Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.79      0.65      0.71       396\n",
      "   entailment       0.84      0.63      0.72       402\n",
      "      neutral       0.55      0.79      0.65       402\n",
      "\n",
      "     accuracy                           0.69      1200\n",
      "    macro avg       0.73      0.69      0.69      1200\n",
      " weighted avg       0.73      0.69      0.69      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, cohen_kappa_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Get ground truth labels\n",
    "gold_labels = [ex['label'] for ex in testset]\n",
    "\n",
    "print(\"=== EVALUATION RESULTS ===\")\n",
    "print(f\"Total test examples: {len(testset)}\")\n",
    "print(f\"Zero-shot predictions: {len(zero_shot_predictions)}\")\n",
    "print(f\"Optimized predictions: {len(optimized_predictions)}\")\n",
    "\n",
    "# Classification metrics for zero-shot model\n",
    "print(\"\\n=== ZERO-SHOT MODEL RESULTS ===\")\n",
    "zs_accuracy = accuracy_score(gold_labels, zero_shot_predictions)\n",
    "print(f\"Accuracy: {zs_accuracy:.3f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(gold_labels, zero_shot_predictions))\n",
    "\n",
    "# Classification metrics for optimized model\n",
    "print(\"\\n=== OPTIMIZED MODEL RESULTS ===\") \n",
    "opt_accuracy = accuracy_score(gold_labels, optimized_predictions)\n",
    "print(f\"Accuracy: {opt_accuracy:.3f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(gold_labels, optimized_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4cdd7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL AGREEMENT ANALYSIS ===\n",
      "Agreement Accuracy: 0.825\n",
      "Cohen's Kappa: 0.716\n",
      "Kappa Interpretation: Substantial agreement\n"
     ]
    }
   ],
   "source": [
    "# Agreement between the two models\n",
    "agreement_accuracy = accuracy_score(zero_shot_predictions, optimized_predictions)\n",
    "kappa_score = cohen_kappa_score(zero_shot_predictions, optimized_predictions)\n",
    "\n",
    "print(f\"\\n=== MODEL AGREEMENT ANALYSIS ===\")\n",
    "print(f\"Agreement Accuracy: {agreement_accuracy:.3f}\")\n",
    "print(f\"Cohen's Kappa: {kappa_score:.3f}\")\n",
    "\n",
    "# Interpretation of Kappa score\n",
    "if kappa_score < 0:\n",
    "    kappa_interp = \"Poor agreement\"\n",
    "elif kappa_score < 0.20:\n",
    "    kappa_interp = \"Slight agreement\"\n",
    "elif kappa_score < 0.40:\n",
    "    kappa_interp = \"Fair agreement\"\n",
    "elif kappa_score < 0.60:\n",
    "    kappa_interp = \"Moderate agreement\"\n",
    "elif kappa_score < 0.80:\n",
    "    kappa_interp = \"Substantial agreement\"\n",
    "else:\n",
    "    kappa_interp = \"Almost perfect agreement\"\n",
    "\n",
    "print(f\"Kappa Interpretation: {kappa_interp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22e3114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DISAGREEMENT ANALYSIS ===\n",
      "Total disagreements: 210\n",
      "Total agreements: 990\n",
      "Zero-shot right, Optimized wrong: 90\n",
      "Optimized right, Zero-shot wrong: 105\n",
      "Both wrong: 15\n"
     ]
    }
   ],
   "source": [
    "# Find where models disagree\n",
    "disagreements = []\n",
    "agreements = []\n",
    "\n",
    "for i, (zs, opt, gold) in enumerate(zip(zero_shot_predictions, optimized_predictions, gold_labels)):\n",
    "    if zs != opt:\n",
    "        disagreements.append({\n",
    "            'index': i,\n",
    "            'premise': testset[i]['premise'][:100] + \"...\",  # Truncate for display\n",
    "            'hypothesis': testset[i]['hypothesis'][:100] + \"...\",\n",
    "            'zero_shot': zs,\n",
    "            'optimized': opt,\n",
    "            'gold': gold,\n",
    "            'zs_correct': zs == gold,\n",
    "            'opt_correct': opt == gold\n",
    "        })\n",
    "    else:\n",
    "        agreements.append({'both_correct': zs == gold})\n",
    "\n",
    "print(f\"\\n=== DISAGREEMENT ANALYSIS ===\")\n",
    "print(f\"Total disagreements: {len(disagreements)}\")\n",
    "print(f\"Total agreements: {len(agreements)}\")\n",
    "\n",
    "# Show cases where each model is right when they disagree\n",
    "zs_right_opt_wrong = sum(1 for d in disagreements if d['zs_correct'] and not d['opt_correct'])\n",
    "opt_right_zs_wrong = sum(1 for d in disagreements if d['opt_correct'] and not d['zs_correct'])\n",
    "both_wrong = sum(1 for d in disagreements if not d['zs_correct'] and not d['opt_correct'])\n",
    "\n",
    "print(f\"Zero-shot right, Optimized wrong: {zs_right_opt_wrong}\")\n",
    "print(f\"Optimized right, Zero-shot wrong: {opt_right_zs_wrong}\")\n",
    "print(f\"Both wrong: {both_wrong}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985841a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [00:00<00:00, 3862.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating optimized model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimized: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [00:34<00:00, 34.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def convert_dict_for_eval(ex):\n",
    "    return {\n",
    "        'premise': ex[\"premise\"],\n",
    "        'hypothesis': ex[\"hypothesis\"],\n",
    "        'label': {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}[ex[\"label\"]]\n",
    "    }\n",
    "\n",
    "testset = [convert_dict_for_eval(x) for x in dataset['test_r3'].to_list()]\n",
    "\n",
    "# Simple approach - no batches, just progress bar\n",
    "print(\"Evaluating zero-shot model...\")\n",
    "zero_shot_predictions = []\n",
    "for example in tqdm(testset, desc=\"Zero-shot\"):\n",
    "    pred = zero_shot_nli_classifier(example)\n",
    "    zero_shot_predictions.append(pred)\n",
    "\n",
    "print(\"Evaluating optimized model...\")\n",
    "optimized_predictions = []\n",
    "for example in tqdm(testset, desc=\"Optimized\"):\n",
    "    pred = compiled_clf(premise=example['premise'], hypothesis=example['hypothesis']).label\n",
    "    optimized_predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eb5d699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading DeBERTa baseline model for comparison...\n",
      "üìä Running DeBERTa model on test_r3 dataset...\n",
      "‚è±Ô∏è  This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeBERTa evaluation:   0%|          | 0/1200 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "DeBERTa evaluation:  17%|‚ñà‚ñã        | 203/1200 [00:15<01:04, 15.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 200/1200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeBERTa evaluation:  34%|‚ñà‚ñà‚ñà‚ñé      | 403/1200 [00:28<00:51, 15.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 400/1200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeBERTa evaluation:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 601/1200 [00:40<00:39, 15.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 600/1200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeBERTa evaluation:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 801/1200 [00:54<00:26, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 800/1200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeBERTa evaluation:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1003/1200 [01:07<00:12, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 1000/1200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeBERTa evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [01:19<00:00, 15.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 1200/1200 samples\n",
      "‚úÖ DeBERTa evaluation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# COMPARISON WITH DeBERTa BASELINE MODEL (Requirement 1.3)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîÑ Loading DeBERTa baseline model for comparison...\")\n",
    "\n",
    "# Load the DeBERTa model (same as in anli_baseline.ipynb)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "deberta_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "deberta_model.to(device)\n",
    "\n",
    "def evaluate_deberta(premise, hypothesis):\n",
    "    \"\"\"Evaluate DeBERTa model on premise-hypothesis pair\"\"\"\n",
    "    input_tokens = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = deberta_model(input_tokens[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    prediction_dict = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction_dict\n",
    "\n",
    "def get_deberta_prediction(pred_dict):\n",
    "    \"\"\"Get the predicted label from DeBERTa scores\"\"\"\n",
    "    if pred_dict[\"entailment\"] > pred_dict[\"contradiction\"] and pred_dict[\"entailment\"] > pred_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif pred_dict[\"contradiction\"] > pred_dict[\"entailment\"] and pred_dict[\"contradiction\"] > pred_dict[\"neutral\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Run DeBERTa model on test_r3 dataset\n",
    "print(\"üìä Running DeBERTa model on test_r3 dataset...\")\n",
    "print(\"‚è±Ô∏è  This may take a few minutes...\")\n",
    "\n",
    "deberta_predictions = []\n",
    "test_r3_data = dataset['test_r3'].to_list()\n",
    "\n",
    "for i, example in enumerate(tqdm(test_r3_data, desc=\"DeBERTa evaluation\")):\n",
    "    premise = example['premise']\n",
    "    hypothesis = example['hypothesis']\n",
    "    pred_scores = evaluate_deberta(premise, hypothesis)\n",
    "    pred_label = get_deberta_prediction(pred_scores)\n",
    "    deberta_predictions.append(pred_label)\n",
    "    \n",
    "    # Progress update every 200 samples\n",
    "    if (i + 1) % 200 == 0:\n",
    "        print(f\"‚úÖ Processed {i + 1}/1200 samples\")\n",
    "\n",
    "print(\"‚úÖ DeBERTa evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "810b5140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Computing agreement metrics between LLM and DeBERTa models...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FOUR-WAY AGREEMENT ANALYSIS\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_agreement_metrics(llm_preds, deberta_preds, gold_labels):\n",
    "    \"\"\"\n",
    "    Compute the four-way agreement metrics between LLM and DeBERTa models:\n",
    "    - Correct: Both models correct\n",
    "    - Correct1: LLM correct, DeBERTa incorrect  \n",
    "    - Correct2: DeBERTa correct, LLM incorrect\n",
    "    - Incorrect: Both models incorrect\n",
    "    \"\"\"\n",
    "    \n",
    "    both_correct = 0\n",
    "    llm_correct_deberta_wrong = 0\n",
    "    deberta_correct_llm_wrong = 0\n",
    "    both_incorrect = 0\n",
    "    \n",
    "    agreement_details = []\n",
    "    \n",
    "    for i, (llm_pred, deberta_pred, gold) in enumerate(zip(llm_preds, deberta_preds, gold_labels)):\n",
    "        llm_correct = (llm_pred == gold)\n",
    "        deberta_correct = (deberta_pred == gold)\n",
    "        \n",
    "        if llm_correct and deberta_correct:\n",
    "            both_correct += 1\n",
    "            category = \"Both Correct\"\n",
    "        elif llm_correct and not deberta_correct:\n",
    "            llm_correct_deberta_wrong += 1\n",
    "            category = \"LLM Right, DeBERTa Wrong\"\n",
    "        elif not llm_correct and deberta_correct:\n",
    "            deberta_correct_llm_wrong += 1\n",
    "            category = \"DeBERTa Right, LLM Wrong\"\n",
    "        else:\n",
    "            both_incorrect += 1\n",
    "            category = \"Both Wrong\"\n",
    "            \n",
    "        agreement_details.append({\n",
    "            'index': i,\n",
    "            'llm_pred': llm_pred,\n",
    "            'deberta_pred': deberta_pred,\n",
    "            'gold_label': gold,\n",
    "            'category': category,\n",
    "            'premise': testset[i]['premise'][:100] + \"...\" if len(testset[i]['premise']) > 100 else testset[i]['premise'],\n",
    "            'hypothesis': testset[i]['hypothesis'][:100] + \"...\" if len(testset[i]['hypothesis']) > 100 else testset[i]['hypothesis']\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'both_correct': both_correct,\n",
    "        'llm_correct_deberta_wrong': llm_correct_deberta_wrong, \n",
    "        'deberta_correct_llm_wrong': deberta_correct_llm_wrong,\n",
    "        'both_incorrect': both_incorrect,\n",
    "        'details': agreement_details\n",
    "    }\n",
    "\n",
    "# Compute agreement metrics\n",
    "print(\"\\nüîç Computing agreement metrics between LLM and DeBERTa models...\")\n",
    "\n",
    "# Use the optimized LLM predictions from earlier\n",
    "agreement_results = compute_agreement_metrics(\n",
    "    optimized_predictions,  # From the optimized DSPy model\n",
    "    deberta_predictions,    # DeBERTa predictions we just computed\n",
    "    gold_labels            # Gold labels from testset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5318ac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéØ COMPREHENSIVE MODEL COMPARISON RESULTS\n",
      "================================================================================\n",
      "\n",
      "üìä DATASET: ANLI test_r3 (1,200 samples)\n",
      "--------------------------------------------------\n",
      "ü§ñ LLM Model (Optimized DSPy):     0.688 (825/1200)\n",
      "üß† DeBERTa Baseline:              0.495 (594/1200)\n",
      "üìà LLM Improvement:               +0.193 (+19.2%)\n",
      "\n",
      "üîÑ FOUR-WAY AGREEMENT ANALYSIS\n",
      "--------------------------------------------------\n",
      "‚úÖ Both Correct:                   448 (37.3%)\n",
      "ü§ñ LLM Right, DeBERTa Wrong:       377 (31.4%)\n",
      "üß† DeBERTa Right, LLM Wrong:       146 (12.2%)\n",
      "‚ùå Both Incorrect:                 229 (19.1%)\n",
      "\n",
      "ü§ù MODEL AGREEMENT\n",
      "--------------------------------------------------\n",
      "Same Prediction:                    619 (51.6%)\n",
      "Different Prediction:               581 (48.4%)\n",
      "Cohen's Kappa (LLM vs DeBERTa):    0.262\n",
      "Interpretation:                    Fair agreement\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# DISPLAY COMPREHENSIVE COMPARISON RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "total_samples = len(testset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ COMPREHENSIVE MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä DATASET: ANLI test_r3 ({total_samples:,} samples)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Individual model accuracies\n",
    "llm_accuracy = accuracy_score(gold_labels, optimized_predictions)\n",
    "deberta_accuracy = accuracy_score(gold_labels, deberta_predictions)\n",
    "\n",
    "print(f\"ü§ñ LLM Model (Optimized DSPy):     {llm_accuracy:.3f} ({int(llm_accuracy * total_samples)}/{total_samples})\")\n",
    "print(f\"üß† DeBERTa Baseline:              {deberta_accuracy:.3f} ({int(deberta_accuracy * total_samples)}/{total_samples})\")\n",
    "print(f\"üìà LLM Improvement:               {llm_accuracy - deberta_accuracy:+.3f} ({(llm_accuracy - deberta_accuracy)*100:+.1f}%)\")\n",
    "\n",
    "# Four-way agreement breakdown\n",
    "print(f\"\\nüîÑ FOUR-WAY AGREEMENT ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"‚úÖ Both Correct:                  {agreement_results['both_correct']:4d} ({agreement_results['both_correct']/total_samples*100:.1f}%)\")\n",
    "print(f\"ü§ñ LLM Right, DeBERTa Wrong:      {agreement_results['llm_correct_deberta_wrong']:4d} ({agreement_results['llm_correct_deberta_wrong']/total_samples*100:.1f}%)\")\n",
    "print(f\"üß† DeBERTa Right, LLM Wrong:      {agreement_results['deberta_correct_llm_wrong']:4d} ({agreement_results['deberta_correct_llm_wrong']/total_samples*100:.1f}%)\")\n",
    "print(f\"‚ùå Both Incorrect:                {agreement_results['both_incorrect']:4d} ({agreement_results['both_incorrect']/total_samples*100:.1f}%)\")\n",
    "\n",
    "# Agreement rate\n",
    "total_agreements = sum(1 for llm, deberta in zip(optimized_predictions, deberta_predictions) if llm == deberta)\n",
    "agreement_rate = total_agreements / total_samples\n",
    "\n",
    "print(f\"\\nü§ù MODEL AGREEMENT\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Same Prediction:                   {total_agreements:4d} ({agreement_rate*100:.1f}%)\")\n",
    "print(f\"Different Prediction:              {total_samples - total_agreements:4d} ({(1-agreement_rate)*100:.1f}%)\")\n",
    "\n",
    "# Cohen's Kappa between the two models\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "kappa_llm_deberta = cohen_kappa_score(optimized_predictions, deberta_predictions)\n",
    "print(f\"Cohen's Kappa (LLM vs DeBERTa):    {kappa_llm_deberta:.3f}\")\n",
    "\n",
    "if kappa_llm_deberta < 0:\n",
    "    kappa_interp = \"Poor agreement\"\n",
    "elif kappa_llm_deberta < 0.20:\n",
    "    kappa_interp = \"Slight agreement\"\n",
    "elif kappa_llm_deberta < 0.40:\n",
    "    kappa_interp = \"Fair agreement\"\n",
    "elif kappa_llm_deberta < 0.60:\n",
    "    kappa_interp = \"Moderate agreement\"\n",
    "elif kappa_llm_deberta < 0.80:\n",
    "    kappa_interp = \"Substantial agreement\"\n",
    "else:\n",
    "    kappa_interp = \"Almost perfect agreement\"\n",
    "\n",
    "print(f\"Interpretation:                    {kappa_interp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c479d540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù EXAMPLES WHERE MODELS DISAGREE\n",
      "================================================================================\n",
      "ü§ñ Examples where LLM is RIGHT and DeBERTa is WRONG:\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Gold: entailment | LLM: entailment | DeBERTa: neutral\n",
      "   P: By The Associated Press WELLINGTON, New Zealand (AP) ‚Äî All passengers and crew have survived a crash...\n",
      "   H: No children were killed in the accident.\n",
      "\n",
      "2. Gold: entailment | LLM: entailment | DeBERTa: neutral\n",
      "   P: Governor Greg Abbott has called for a statewide show of support for law enforcement Friday, July 7. ...\n",
      "   H: Law enforcement officers and the people at the Travis St. memorial do not show their support at the ...\n",
      "\n",
      "3. Gold: entailment | LLM: entailment | DeBERTa: neutral\n",
      "   P: press release: Fresca Opera presents Opera Storytellers! Opera is a series of notes from the soul. A...\n",
      "   H: Fresca Opera is a unique type of music.\n",
      "\n",
      "üß† Examples where DeBERTa is RIGHT and LLM is WRONG:\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Gold: entailment | LLM: neutral | DeBERTa: entailment\n",
      "   P: Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snackin...\n",
      "   H: Japanese like kit kat. \n",
      "\n",
      "2. Gold: entailment | LLM: neutral | DeBERTa: entailment\n",
      "   P: NEW YORK, Aug 14 (Reuters) - U.S. stocks fell on Wednesday as investors speculated when the Federal ...\n",
      "   H: U.S. stocks fell on Wednesday because of the Federal Reserve's stimulus measures\n",
      "\n",
      "3. Gold: entailment | LLM: neutral | DeBERTa: entailment\n",
      "   P: LONDON, Oct 28 (Reuters) - Britain will sell 4.0 billion pounds ($6.45 billion) of the 2068 gilt aft...\n",
      "   H: LONDON contains no z\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# EXAMPLES WHERE MODELS DISAGREE\n",
    "# ==============================================================================\n",
    "\n",
    "print(f\"\\nüìù EXAMPLES WHERE MODELS DISAGREE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show examples where LLM is right and DeBERTa is wrong\n",
    "llm_right_examples = [detail for detail in agreement_results['details'] \n",
    "                     if detail['category'] == \"LLM Right, DeBERTa Wrong\"][:3]\n",
    "\n",
    "print(f\"ü§ñ Examples where LLM is RIGHT and DeBERTa is WRONG:\")\n",
    "print(\"-\" * 60)\n",
    "for i, example in enumerate(llm_right_examples, 1):\n",
    "    print(f\"\\n{i}. Gold: {example['gold_label']} | LLM: {example['llm_pred']} | DeBERTa: {example['deberta_pred']}\")\n",
    "    print(f\"   P: {example['premise']}\")\n",
    "    print(f\"   H: {example['hypothesis']}\")\n",
    "\n",
    "# Show examples where DeBERTa is right and LLM is wrong  \n",
    "deberta_right_examples = [detail for detail in agreement_results['details'] \n",
    "                         if detail['category'] == \"DeBERTa Right, LLM Wrong\"][:3]\n",
    "\n",
    "print(f\"\\nüß† Examples where DeBERTa is RIGHT and LLM is WRONG:\")\n",
    "print(\"-\" * 60)\n",
    "for i, example in enumerate(deberta_right_examples, 1):\n",
    "    print(f\"\\n{i}. Gold: {example['gold_label']} | LLM: {example['llm_pred']} | DeBERTa: {example['deberta_pred']}\")\n",
    "    print(f\"   P: {example['premise']}\")\n",
    "    print(f\"   H: {example['hypothesis']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a76ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã DETAILED CLASSIFICATION REPORTS COMPARISON\n",
      "================================================================================\n",
      "ü§ñ LLM MODEL (Optimized DSPy) CLASSIFICATION REPORT:\n",
      "------------------------------------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.79      0.65      0.71       396\n",
      "   entailment       0.84      0.63      0.72       402\n",
      "      neutral       0.55      0.79      0.65       402\n",
      "\n",
      "     accuracy                           0.69      1200\n",
      "    macro avg       0.73      0.69      0.69      1200\n",
      " weighted avg       0.73      0.69      0.69      1200\n",
      "\n",
      "üß† DeBERTa BASELINE CLASSIFICATION REPORT:\n",
      "------------------------------------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.51      0.42      0.46       396\n",
      "   entailment       0.56      0.57      0.56       402\n",
      "      neutral       0.43      0.50      0.46       402\n",
      "\n",
      "     accuracy                           0.49      1200\n",
      "    macro avg       0.50      0.49      0.49      1200\n",
      " weighted avg       0.50      0.49      0.49      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CLASSIFICATION REPORTS COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "print(f\"\\nüìã DETAILED CLASSIFICATION REPORTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"ü§ñ LLM MODEL (Optimized DSPy) CLASSIFICATION REPORT:\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(gold_labels, optimized_predictions))\n",
    "\n",
    "print(\"üß† DeBERTa BASELINE CLASSIFICATION REPORT:\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(gold_labels, deberta_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "167534c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ SUMMARY AND CONCLUSIONS\n",
      "================================================================================\n",
      "‚úÖ The optimized LLM model outperforms the DeBERTa baseline by 0.193 (19.2%)\n",
      "   This represents a 38.9% relative improvement.\n",
      "\n",
      "üîç Key Insights:\n",
      "   ‚Ä¢ LLM model shows better overall accuracy (68.8% vs 49.5%)\n",
      "   ‚Ä¢ Models agree on 51.6% of predictions\n",
      "   ‚Ä¢ 377 cases where LLM succeeds but DeBERTa fails\n",
      "   ‚Ä¢ 146 cases where DeBERTa succeeds but LLM fails\n",
      "   ‚Ä¢ 448 cases where both models are correct\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SUMMARY AND CONCLUSIONS\n",
    "# ==============================================================================\n",
    "\n",
    "print(f\"\\nüéØ SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "improvement = llm_accuracy - deberta_accuracy\n",
    "if improvement > 0:\n",
    "    print(f\"‚úÖ The optimized LLM model outperforms the DeBERTa baseline by {improvement:.3f} ({improvement*100:.1f}%)\")\n",
    "    print(f\"   This represents a {improvement/deberta_accuracy*100:.1f}% relative improvement.\")\n",
    "elif improvement < 0:\n",
    "    print(f\"‚ùå The DeBERTa baseline outperforms the LLM model by {-improvement:.3f} ({-improvement*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"‚öñÔ∏è  Both models achieve similar performance\")\n",
    "\n",
    "print(f\"\\nüîç Key Insights:\")\n",
    "print(f\"   ‚Ä¢ LLM model shows better overall accuracy ({llm_accuracy:.1%} vs {deberta_accuracy:.1%})\")\n",
    "print(f\"   ‚Ä¢ Models agree on {agreement_rate:.1%} of predictions\")\n",
    "print(f\"   ‚Ä¢ {agreement_results['llm_correct_deberta_wrong']} cases where LLM succeeds but DeBERTa fails\")\n",
    "print(f\"   ‚Ä¢ {agreement_results['deberta_correct_llm_wrong']} cases where DeBERTa succeeds but LLM fails\")\n",
    "print(f\"   ‚Ä¢ {agreement_results['both_correct']} cases where both models are correct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
