{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2cec0d02",
   "metadata": {},
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "# lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama\n",
    "lm = dspy.LM('ollama_chat/llama3.1', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b60da44b",
   "metadata": {},
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy classifier program.\n",
    "class NLIClassifier(dspy.Signature):\n",
    "    premise     :str = dspy.InputField(desc = \"The premise\")\n",
    "    hypothesis  :str = dspy.InputField(desc=\"The hypothesis based on the premise\")\n",
    "    label       : Literal[\"entailment\", \"neutral\", \"contradiction\"] = dspy.OutputField(desc=\"Return exactly one of: \"\n",
    "                        \"'entailment', 'neutral', 'contradiction'\")\n",
    "\n",
    "predictor = dspy.Predict(NLIClassifier,llm=lm)\n",
    "\n",
    "def zero_shot_nli_classifier(x):\n",
    "    # if hasattr(x,'premise') and hasattr(x,'hypothesis'):\n",
    "    return predictor(premise=x['premise'], hypothesis=x['hypothesis']).label\n",
    "    # print(\"ERROR\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "0438789b",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e59927ea",
   "metadata": {},
   "source": [
    "dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T14:34:11.173521Z",
     "start_time": "2025-07-09T14:34:01.244427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = dataset['test_r3'][0]\n",
    "print(example)\n",
    "print(zero_shot_nli_classifier(example))"
   ],
   "id": "596338cc3307306",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uid': 'b0e63408-53af-4b46-b33d-bf5ba302949f', 'premise': \"It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.\", 'hypothesis': 'The day of the passage is usually when Christians praise the lord together', 'label': 0, 'reason': \"Sunday is considered Lord's Day\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/09 17:34:04 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.APIConnectionError: Ollama_chatException - Object of type LM is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:166\u001B[39m, in \u001B[36mBaseLLMHTTPHandler._make_common_sync_call\u001B[39m\u001B[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001B[39m\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    160\u001B[39m     response = sync_httpx_client.post(\n\u001B[32m    161\u001B[39m         url=api_base,\n\u001B[32m    162\u001B[39m         headers=headers,\n\u001B[32m    163\u001B[39m         data=(\n\u001B[32m    164\u001B[39m             signed_json_body\n\u001B[32m    165\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m signed_json_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m166\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mjson\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    167\u001B[39m         ),\n\u001B[32m    168\u001B[39m         timeout=timeout,\n\u001B[32m    169\u001B[39m         stream=stream,\n\u001B[32m    170\u001B[39m         logging_obj=logging_obj,\n\u001B[32m    171\u001B[39m     )\n\u001B[32m    172\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m httpx.HTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/json/__init__.py:231\u001B[39m, in \u001B[36mdumps\u001B[39m\u001B[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001B[39m\n\u001B[32m    227\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m skipkeys \u001B[38;5;129;01mand\u001B[39;00m ensure_ascii \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[32m    228\u001B[39m     check_circular \u001B[38;5;129;01mand\u001B[39;00m allow_nan \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[32m    229\u001B[39m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m separators \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[32m    230\u001B[39m     default \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sort_keys \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[32m--> \u001B[39m\u001B[32m231\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_encoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    232\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/json/encoder.py:200\u001B[39m, in \u001B[36mJSONEncoder.encode\u001B[39m\u001B[34m(self, o)\u001B[39m\n\u001B[32m    197\u001B[39m \u001B[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001B[39;00m\n\u001B[32m    198\u001B[39m \u001B[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001B[39;00m\n\u001B[32m    199\u001B[39m \u001B[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m200\u001B[39m chunks = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miterencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_one_shot\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    201\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunks, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/json/encoder.py:258\u001B[39m, in \u001B[36mJSONEncoder.iterencode\u001B[39m\u001B[34m(self, o, _one_shot)\u001B[39m\n\u001B[32m    254\u001B[39m     _iterencode = _make_iterencode(\n\u001B[32m    255\u001B[39m         markers, \u001B[38;5;28mself\u001B[39m.default, _encoder, \u001B[38;5;28mself\u001B[39m.indent, floatstr,\n\u001B[32m    256\u001B[39m         \u001B[38;5;28mself\u001B[39m.key_separator, \u001B[38;5;28mself\u001B[39m.item_separator, \u001B[38;5;28mself\u001B[39m.sort_keys,\n\u001B[32m    257\u001B[39m         \u001B[38;5;28mself\u001B[39m.skipkeys, _one_shot)\n\u001B[32m--> \u001B[39m\u001B[32m258\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_iterencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/json/encoder.py:180\u001B[39m, in \u001B[36mJSONEncoder.default\u001B[39m\u001B[34m(self, o)\u001B[39m\n\u001B[32m    162\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Implement this method in a subclass such that it returns\u001B[39;00m\n\u001B[32m    163\u001B[39m \u001B[33;03ma serializable object for ``o``, or calls the base implementation\u001B[39;00m\n\u001B[32m    164\u001B[39m \u001B[33;03m(to raise a ``TypeError``).\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    178\u001B[39m \n\u001B[32m    179\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m180\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mObject of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mo.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    181\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mis not JSON serializable\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[31mTypeError\u001B[39m: Object of type LM is not JSON serializable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mOllamaError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3025\u001B[39m, in \u001B[36mcompletion\u001B[39m\u001B[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001B[39m\n\u001B[32m   3018\u001B[39m     api_key = (\n\u001B[32m   3019\u001B[39m         api_key\n\u001B[32m   3020\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m litellm.ollama_key\n\u001B[32m   3021\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m os.environ.get(\u001B[33m\"\u001B[39m\u001B[33mOLLAMA_API_KEY\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   3022\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m litellm.api_key\n\u001B[32m   3023\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m3025\u001B[39m     response = \u001B[43mbase_llm_http_handler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompletion\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3026\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3027\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3028\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3029\u001B[39m \u001B[43m        \u001B[49m\u001B[43macompletion\u001B[49m\u001B[43m=\u001B[49m\u001B[43macompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3030\u001B[39m \u001B[43m        \u001B[49m\u001B[43mapi_base\u001B[49m\u001B[43m=\u001B[49m\u001B[43mapi_base\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3031\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_response\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_response\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3032\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptional_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptional_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3033\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlitellm_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlitellm_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3034\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mollama_chat\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   3035\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3036\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3037\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3038\u001B[39m \u001B[43m        \u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m=\u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3039\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogging_obj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlogging\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements\u001B[39;49;00m\n\u001B[32m   3040\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclient\u001B[49m\u001B[43m=\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3041\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3043\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m custom_llm_provider == \u001B[33m\"\u001B[39m\u001B[33mtriton\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:455\u001B[39m, in \u001B[36mBaseLLMHTTPHandler.completion\u001B[39m\u001B[34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config)\u001B[39m\n\u001B[32m    453\u001B[39m     sync_httpx_client = client\n\u001B[32m--> \u001B[39m\u001B[32m455\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_common_sync_call\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    456\u001B[39m \u001B[43m    \u001B[49m\u001B[43msync_httpx_client\u001B[49m\u001B[43m=\u001B[49m\u001B[43msync_httpx_client\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    457\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprovider_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprovider_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    458\u001B[39m \u001B[43m    \u001B[49m\u001B[43mapi_base\u001B[49m\u001B[43m=\u001B[49m\u001B[43mapi_base\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    459\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    460\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    461\u001B[39m \u001B[43m    \u001B[49m\u001B[43msigned_json_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43msigned_json_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    462\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    463\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlitellm_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlitellm_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    464\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogging_obj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlogging_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    465\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    466\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m provider_config.transform_response(\n\u001B[32m    467\u001B[39m     model=model,\n\u001B[32m    468\u001B[39m     raw_response=response,\n\u001B[32m   (...)\u001B[39m\u001B[32m    477\u001B[39m     json_mode=json_mode,\n\u001B[32m    478\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:187\u001B[39m, in \u001B[36mBaseLLMHTTPHandler._make_common_sync_call\u001B[39m\u001B[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001B[39m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m187\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_handle_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m=\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprovider_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprovider_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:2371\u001B[39m, in \u001B[36mBaseLLMHTTPHandler._handle_error\u001B[39m\u001B[34m(self, e, provider_config)\u001B[39m\n\u001B[32m   2369\u001B[39m     error_headers = {}\n\u001B[32m-> \u001B[39m\u001B[32m2371\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m provider_config.get_error_class(\n\u001B[32m   2372\u001B[39m     error_message=error_text,\n\u001B[32m   2373\u001B[39m     status_code=status_code,\n\u001B[32m   2374\u001B[39m     headers=error_headers,\n\u001B[32m   2375\u001B[39m )\n",
      "\u001B[31mOllamaError\u001B[39m: Object of type LM is not JSON serializable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mAPIConnectionError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1164\u001B[39m, in \u001B[36mclient.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1163\u001B[39m \u001B[38;5;66;03m# MODEL CALL\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1164\u001B[39m result = \u001B[43moriginal_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1165\u001B[39m end_time = datetime.datetime.now()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3305\u001B[39m, in \u001B[36mcompletion\u001B[39m\u001B[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001B[39m\n\u001B[32m   3303\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   3304\u001B[39m     \u001B[38;5;66;03m## Map to OpenAI Exception\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3305\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mexception_type\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3306\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3307\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3308\u001B[39m \u001B[43m        \u001B[49m\u001B[43moriginal_exception\u001B[49m\u001B[43m=\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3309\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcompletion_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3310\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3311\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2271\u001B[39m, in \u001B[36mexception_type\u001B[39m\u001B[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001B[39m\n\u001B[32m   2270\u001B[39m     \u001B[38;5;28msetattr\u001B[39m(e, \u001B[33m\"\u001B[39m\u001B[33mlitellm_response_headers\u001B[39m\u001B[33m\"\u001B[39m, litellm_response_headers)\n\u001B[32m-> \u001B[39m\u001B[32m2271\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m   2272\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2240\u001B[39m, in \u001B[36mexception_type\u001B[39m\u001B[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001B[39m\n\u001B[32m   2239\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(original_exception, \u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m2240\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(\n\u001B[32m   2241\u001B[39m         message=\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m - \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(exception_provider, error_str),\n\u001B[32m   2242\u001B[39m         llm_provider=custom_llm_provider,\n\u001B[32m   2243\u001B[39m         model=model,\n\u001B[32m   2244\u001B[39m         request=original_exception.request,\n\u001B[32m   2245\u001B[39m     )\n\u001B[32m   2246\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mAPIConnectionError\u001B[39m: litellm.APIConnectionError: Ollama_chatException - Object of type LM is not JSON serializable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mAPIConnectionError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/chat_adapter.py:42\u001B[39m, in \u001B[36mChatAdapter.__call__\u001B[39m\u001B[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msignature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdemos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     44\u001B[39m     \u001B[38;5;66;03m# fallback to JSONAdapter\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/base.py:119\u001B[39m, in \u001B[36mAdapter.__call__\u001B[39m\u001B[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001B[39m\n\u001B[32m    117\u001B[39m inputs = \u001B[38;5;28mself\u001B[39m.format(processed_signature, demos, inputs)\n\u001B[32m--> \u001B[39m\u001B[32m119\u001B[39m outputs = \u001B[43mlm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mlm_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    120\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_postprocess(signature, outputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001B[39m, in \u001B[36mwith_callbacks.<locals>.sync_wrapper\u001B[39m\u001B[34m(instance, *args, **kwargs)\u001B[39m\n\u001B[32m    325\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m callbacks:\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    328\u001B[39m call_id = uuid.uuid4().hex\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/base_lm.py:96\u001B[39m, in \u001B[36mBaseLM.__call__\u001B[39m\u001B[34m(self, prompt, messages, **kwargs)\u001B[39m\n\u001B[32m     94\u001B[39m \u001B[38;5;129m@with_callbacks\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, prompt=\u001B[38;5;28;01mNone\u001B[39;00m, messages=\u001B[38;5;28;01mNone\u001B[39;00m, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m96\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     97\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m._process_lm_response(response, prompt, messages, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:127\u001B[39m, in \u001B[36mLM.forward\u001B[39m\u001B[34m(self, prompt, messages, **kwargs)\u001B[39m\n\u001B[32m    125\u001B[39m completion, litellm_cache_args = \u001B[38;5;28mself\u001B[39m._get_cached_completion_fn(completion, cache, enable_memory_cache)\n\u001B[32m--> \u001B[39m\u001B[32m127\u001B[39m results = \u001B[43mcompletion\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_retries\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnum_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlitellm_cache_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(c.finish_reason == \u001B[33m\"\u001B[39m\u001B[33mlength\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m results[\u001B[33m\"\u001B[39m\u001B[33mchoices\u001B[39m\u001B[33m\"\u001B[39m]):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/cache.py:232\u001B[39m, in \u001B[36mrequest_cache.<locals>.decorator.<locals>.sync_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    231\u001B[39m \u001B[38;5;66;03m# Otherwise, compute and store the result\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m232\u001B[39m result = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    233\u001B[39m \u001B[38;5;66;03m# `enable_memory_cache` can be provided at call time to avoid indefinite growth.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:304\u001B[39m, in \u001B[36mlitellm_completion\u001B[39m\u001B[34m(request, num_retries, cache)\u001B[39m\n\u001B[32m    303\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stream_completion \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m304\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlitellm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompletion\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    305\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    306\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_retries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    307\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretry_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mexponential_backoff_retry\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    308\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    309\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    311\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m stream_completion()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1266\u001B[39m, in \u001B[36mclient.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1265\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mnum_retries\u001B[39m\u001B[33m\"\u001B[39m] = num_retries\n\u001B[32m-> \u001B[39m\u001B[32m1266\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlitellm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompletion_with_retries\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1267\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m (\n\u001B[32m   1268\u001B[39m     \u001B[38;5;28misinstance\u001B[39m(e, litellm.exceptions.ContextWindowExceededError)\n\u001B[32m   1269\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m context_window_fallback_dict\n\u001B[32m   1270\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m context_window_fallback_dict\n\u001B[32m   1271\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_litellm_router_call\n\u001B[32m   1272\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3343\u001B[39m, in \u001B[36mcompletion_with_retries\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   3340\u001B[39m     retryer = tenacity.Retrying(\n\u001B[32m   3341\u001B[39m         stop=tenacity.stop_after_attempt(num_retries), reraise=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   3342\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m3343\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mretryer\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:477\u001B[39m, in \u001B[36mRetrying.__call__\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    476\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m477\u001B[39m     do = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    478\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:378\u001B[39m, in \u001B[36mBaseRetrying.iter\u001B[39m\u001B[34m(self, retry_state)\u001B[39m\n\u001B[32m    377\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m action \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.iter_state.actions:\n\u001B[32m--> \u001B[39m\u001B[32m378\u001B[39m     result = \u001B[43maction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    379\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:420\u001B[39m, in \u001B[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001B[39m\u001B[34m(rs)\u001B[39m\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.reraise:\n\u001B[32m--> \u001B[39m\u001B[32m420\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mretry_exc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    421\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m retry_exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mfut\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexception\u001B[39;00m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:187\u001B[39m, in \u001B[36mRetryError.reraise\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.last_attempt.failed:\n\u001B[32m--> \u001B[39m\u001B[32m187\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlast_attempt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/concurrent/futures/_base.py:449\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    448\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m449\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    451\u001B[39m \u001B[38;5;28mself\u001B[39m._condition.wait(timeout)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/concurrent/futures/_base.py:401\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    400\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    402\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    403\u001B[39m     \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:480\u001B[39m, in \u001B[36mRetrying.__call__\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    479\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m480\u001B[39m     result = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    481\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1286\u001B[39m, in \u001B[36mclient.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1283\u001B[39m     logging_obj.failure_handler(\n\u001B[32m   1284\u001B[39m         e, traceback_exception, start_time, end_time\n\u001B[32m   1285\u001B[39m     )  \u001B[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1286\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1164\u001B[39m, in \u001B[36mclient.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1163\u001B[39m \u001B[38;5;66;03m# MODEL CALL\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1164\u001B[39m result = \u001B[43moriginal_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1165\u001B[39m end_time = datetime.datetime.now()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3305\u001B[39m, in \u001B[36mcompletion\u001B[39m\u001B[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001B[39m\n\u001B[32m   3303\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   3304\u001B[39m     \u001B[38;5;66;03m## Map to OpenAI Exception\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3305\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mexception_type\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3306\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3307\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3308\u001B[39m \u001B[43m        \u001B[49m\u001B[43moriginal_exception\u001B[49m\u001B[43m=\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3309\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcompletion_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3310\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3311\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2271\u001B[39m, in \u001B[36mexception_type\u001B[39m\u001B[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001B[39m\n\u001B[32m   2270\u001B[39m     \u001B[38;5;28msetattr\u001B[39m(e, \u001B[33m\"\u001B[39m\u001B[33mlitellm_response_headers\u001B[39m\u001B[33m\"\u001B[39m, litellm_response_headers)\n\u001B[32m-> \u001B[39m\u001B[32m2271\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m   2272\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2240\u001B[39m, in \u001B[36mexception_type\u001B[39m\u001B[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001B[39m\n\u001B[32m   2239\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(original_exception, \u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m2240\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(\n\u001B[32m   2241\u001B[39m         message=\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m - \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(exception_provider, error_str),\n\u001B[32m   2242\u001B[39m         llm_provider=custom_llm_provider,\n\u001B[32m   2243\u001B[39m         model=model,\n\u001B[32m   2244\u001B[39m         request=original_exception.request,\n\u001B[32m   2245\u001B[39m     )\n\u001B[32m   2246\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mAPIConnectionError\u001B[39m: litellm.APIConnectionError: Ollama_chatException - Object of type LM is not JSON serializable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/json_adapter.py:62\u001B[39m, in \u001B[36mJSONAdapter.__call__\u001B[39m\u001B[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001B[39m\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m62\u001B[39m     structured_output_model = \u001B[43m_get_structured_outputs_response_format\u001B[49m\u001B[43m(\u001B[49m\u001B[43msignature\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     63\u001B[39m     lm_kwargs[\u001B[33m\"\u001B[39m\u001B[33mresponse_format\u001B[39m\u001B[33m\"\u001B[39m] = structured_output_model\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/json_adapter.py:220\u001B[39m, in \u001B[36m_get_structured_outputs_response_format\u001B[39m\u001B[34m(signature)\u001B[39m\n\u001B[32m    219\u001B[39m \u001B[38;5;66;03m# Build the model with extra fields forbidden.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m220\u001B[39m pydantic_model = \u001B[43mpydantic\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    221\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mDSPyProgramOutputs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    222\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfields\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    223\u001B[39m \u001B[43m    \u001B[49m\u001B[43m__config__\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mConfig\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mextra\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mforbid\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    224\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[38;5;66;03m# Generate the initial schema.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/pydantic/main.py:1763\u001B[39m, in \u001B[36mcreate_model\u001B[39m\u001B[34m(model_name, __config__, __doc__, __base__, __module__, __validators__, __cls_kwargs__, **field_definitions)\u001B[39m\n\u001B[32m   1761\u001B[39m namespace.update(ns)\n\u001B[32m-> \u001B[39m\u001B[32m1763\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmeta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1764\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1765\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresolved_bases\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1766\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnamespace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1767\u001B[39m \u001B[43m    \u001B[49m\u001B[43m__pydantic_reset_parent_namespace__\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1768\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_create_model_module\u001B[49m\u001B[43m=\u001B[49m\u001B[34;43m__module__\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1769\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1770\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:110\u001B[39m, in \u001B[36mModelMetaclass.__new__\u001B[39m\u001B[34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001B[39m\n\u001B[32m    108\u001B[39m base_field_names, class_vars, base_private_attributes = mcs._collect_bases_data(bases)\n\u001B[32m--> \u001B[39m\u001B[32m110\u001B[39m config_wrapper = \u001B[43mConfigWrapper\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfor_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbases\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnamespace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    111\u001B[39m namespace[\u001B[33m'\u001B[39m\u001B[33mmodel_config\u001B[39m\u001B[33m'\u001B[39m] = config_wrapper.config_dict\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/pydantic/_internal/_config.py:138\u001B[39m, in \u001B[36mConfigWrapper.for_model\u001B[39m\u001B[34m(cls, bases, namespace, kwargs)\u001B[39m\n\u001B[32m    136\u001B[39m config_from_namespace = config_dict_from_namespace \u001B[38;5;129;01mor\u001B[39;00m prepare_config(config_class_from_namespace)\n\u001B[32m--> \u001B[39m\u001B[32m138\u001B[39m \u001B[43mconfig_new\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig_from_namespace\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    140\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(kwargs.keys()):\n",
      "\u001B[31mTypeError\u001B[39m: 'type' object is not iterable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:166\u001B[39m, in \u001B[36mBaseLLMHTTPHandler._make_common_sync_call\u001B[39m\u001B[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001B[39m\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    160\u001B[39m     response = sync_httpx_client.post(\n\u001B[32m    161\u001B[39m         url=api_base,\n\u001B[32m    162\u001B[39m         headers=headers,\n\u001B[32m    163\u001B[39m         data=(\n\u001B[32m    164\u001B[39m             signed_json_body\n\u001B[32m    165\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m signed_json_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m166\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mjson\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    167\u001B[39m         ),\n\u001B[32m    168\u001B[39m         timeout=timeout,\n\u001B[32m    169\u001B[39m         stream=stream,\n\u001B[32m    170\u001B[39m         logging_obj=logging_obj,\n\u001B[32m    171\u001B[39m     )\n\u001B[32m    172\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m httpx.HTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/json/__init__.py:231\u001B[39m, in \u001B[36mdumps\u001B[39m\u001B[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001B[39m\n\u001B[32m    227\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m skipkeys \u001B[38;5;129;01mand\u001B[39;00m ensure_ascii \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[32m    228\u001B[39m     check_circular \u001B[38;5;129;01mand\u001B[39;00m allow_nan \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[32m    229\u001B[39m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m separators \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[32m    230\u001B[39m     default \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sort_keys \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[32m--> \u001B[39m\u001B[32m231\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_encoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    232\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/json/encoder.py:200\u001B[39m, in \u001B[36mJSONEncoder.encode\u001B[39m\u001B[34m(self, o)\u001B[39m\n\u001B[32m    197\u001B[39m \u001B[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001B[39;00m\n\u001B[32m    198\u001B[39m \u001B[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001B[39;00m\n\u001B[32m    199\u001B[39m \u001B[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m200\u001B[39m chunks = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miterencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_one_shot\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    201\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunks, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/json/encoder.py:258\u001B[39m, in \u001B[36mJSONEncoder.iterencode\u001B[39m\u001B[34m(self, o, _one_shot)\u001B[39m\n\u001B[32m    254\u001B[39m     _iterencode = _make_iterencode(\n\u001B[32m    255\u001B[39m         markers, \u001B[38;5;28mself\u001B[39m.default, _encoder, \u001B[38;5;28mself\u001B[39m.indent, floatstr,\n\u001B[32m    256\u001B[39m         \u001B[38;5;28mself\u001B[39m.key_separator, \u001B[38;5;28mself\u001B[39m.item_separator, \u001B[38;5;28mself\u001B[39m.sort_keys,\n\u001B[32m    257\u001B[39m         \u001B[38;5;28mself\u001B[39m.skipkeys, _one_shot)\n\u001B[32m--> \u001B[39m\u001B[32m258\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_iterencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/json/encoder.py:180\u001B[39m, in \u001B[36mJSONEncoder.default\u001B[39m\u001B[34m(self, o)\u001B[39m\n\u001B[32m    162\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Implement this method in a subclass such that it returns\u001B[39;00m\n\u001B[32m    163\u001B[39m \u001B[33;03ma serializable object for ``o``, or calls the base implementation\u001B[39;00m\n\u001B[32m    164\u001B[39m \u001B[33;03m(to raise a ``TypeError``).\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    178\u001B[39m \n\u001B[32m    179\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m180\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mObject of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mo.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    181\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mis not JSON serializable\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[31mTypeError\u001B[39m: Object of type LM is not JSON serializable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mOllamaError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3025\u001B[39m, in \u001B[36mcompletion\u001B[39m\u001B[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001B[39m\n\u001B[32m   3018\u001B[39m     api_key = (\n\u001B[32m   3019\u001B[39m         api_key\n\u001B[32m   3020\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m litellm.ollama_key\n\u001B[32m   3021\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m os.environ.get(\u001B[33m\"\u001B[39m\u001B[33mOLLAMA_API_KEY\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   3022\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m litellm.api_key\n\u001B[32m   3023\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m3025\u001B[39m     response = \u001B[43mbase_llm_http_handler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompletion\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3026\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3027\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3028\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3029\u001B[39m \u001B[43m        \u001B[49m\u001B[43macompletion\u001B[49m\u001B[43m=\u001B[49m\u001B[43macompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3030\u001B[39m \u001B[43m        \u001B[49m\u001B[43mapi_base\u001B[49m\u001B[43m=\u001B[49m\u001B[43mapi_base\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3031\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_response\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_response\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3032\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptional_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptional_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3033\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlitellm_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlitellm_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3034\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mollama_chat\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   3035\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3036\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3037\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3038\u001B[39m \u001B[43m        \u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m=\u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3039\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogging_obj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlogging\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements\u001B[39;49;00m\n\u001B[32m   3040\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclient\u001B[49m\u001B[43m=\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3041\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3043\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m custom_llm_provider == \u001B[33m\"\u001B[39m\u001B[33mtriton\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:455\u001B[39m, in \u001B[36mBaseLLMHTTPHandler.completion\u001B[39m\u001B[34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config)\u001B[39m\n\u001B[32m    453\u001B[39m     sync_httpx_client = client\n\u001B[32m--> \u001B[39m\u001B[32m455\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_common_sync_call\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    456\u001B[39m \u001B[43m    \u001B[49m\u001B[43msync_httpx_client\u001B[49m\u001B[43m=\u001B[49m\u001B[43msync_httpx_client\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    457\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprovider_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprovider_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    458\u001B[39m \u001B[43m    \u001B[49m\u001B[43mapi_base\u001B[49m\u001B[43m=\u001B[49m\u001B[43mapi_base\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    459\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    460\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    461\u001B[39m \u001B[43m    \u001B[49m\u001B[43msigned_json_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43msigned_json_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    462\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    463\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlitellm_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlitellm_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    464\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogging_obj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlogging_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    465\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    466\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m provider_config.transform_response(\n\u001B[32m    467\u001B[39m     model=model,\n\u001B[32m    468\u001B[39m     raw_response=response,\n\u001B[32m   (...)\u001B[39m\u001B[32m    477\u001B[39m     json_mode=json_mode,\n\u001B[32m    478\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:187\u001B[39m, in \u001B[36mBaseLLMHTTPHandler._make_common_sync_call\u001B[39m\u001B[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001B[39m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m187\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_handle_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m=\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprovider_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprovider_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:2371\u001B[39m, in \u001B[36mBaseLLMHTTPHandler._handle_error\u001B[39m\u001B[34m(self, e, provider_config)\u001B[39m\n\u001B[32m   2369\u001B[39m     error_headers = {}\n\u001B[32m-> \u001B[39m\u001B[32m2371\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m provider_config.get_error_class(\n\u001B[32m   2372\u001B[39m     error_message=error_text,\n\u001B[32m   2373\u001B[39m     status_code=status_code,\n\u001B[32m   2374\u001B[39m     headers=error_headers,\n\u001B[32m   2375\u001B[39m )\n",
      "\u001B[31mOllamaError\u001B[39m: Object of type LM is not JSON serializable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mAPIConnectionError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1164\u001B[39m, in \u001B[36mclient.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1163\u001B[39m \u001B[38;5;66;03m# MODEL CALL\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1164\u001B[39m result = \u001B[43moriginal_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1165\u001B[39m end_time = datetime.datetime.now()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3305\u001B[39m, in \u001B[36mcompletion\u001B[39m\u001B[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001B[39m\n\u001B[32m   3303\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   3304\u001B[39m     \u001B[38;5;66;03m## Map to OpenAI Exception\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3305\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mexception_type\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3306\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3307\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3308\u001B[39m \u001B[43m        \u001B[49m\u001B[43moriginal_exception\u001B[49m\u001B[43m=\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3309\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcompletion_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3310\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3311\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2271\u001B[39m, in \u001B[36mexception_type\u001B[39m\u001B[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001B[39m\n\u001B[32m   2270\u001B[39m     \u001B[38;5;28msetattr\u001B[39m(e, \u001B[33m\"\u001B[39m\u001B[33mlitellm_response_headers\u001B[39m\u001B[33m\"\u001B[39m, litellm_response_headers)\n\u001B[32m-> \u001B[39m\u001B[32m2271\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m   2272\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2240\u001B[39m, in \u001B[36mexception_type\u001B[39m\u001B[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001B[39m\n\u001B[32m   2239\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(original_exception, \u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m2240\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(\n\u001B[32m   2241\u001B[39m         message=\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m - \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(exception_provider, error_str),\n\u001B[32m   2242\u001B[39m         llm_provider=custom_llm_provider,\n\u001B[32m   2243\u001B[39m         model=model,\n\u001B[32m   2244\u001B[39m         request=original_exception.request,\n\u001B[32m   2245\u001B[39m     )\n\u001B[32m   2246\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mAPIConnectionError\u001B[39m: litellm.APIConnectionError: Ollama_chatException - Object of type LM is not JSON serializable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mAPIConnectionError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/json_adapter.py:69\u001B[39m, in \u001B[36mJSONAdapter.__call__\u001B[39m\u001B[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001B[39m\n\u001B[32m     68\u001B[39m     lm_kwargs[\u001B[33m\"\u001B[39m\u001B[33mresponse_format\u001B[39m\u001B[33m\"\u001B[39m] = {\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mjson_object\u001B[39m\u001B[33m\"\u001B[39m}\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msignature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdemos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m AdapterParseError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     71\u001B[39m     \u001B[38;5;66;03m# On AdapterParseError, we raise the original error.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/chat_adapter.py:50\u001B[39m, in \u001B[36mChatAdapter.__call__\u001B[39m\u001B[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, ContextWindowExceededError) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, JSONAdapter):\n\u001B[32m     48\u001B[39m     \u001B[38;5;66;03m# On context window exceeded error or already using JSONAdapter, we don't want to retry with a different\u001B[39;00m\n\u001B[32m     49\u001B[39m     \u001B[38;5;66;03m# adapter.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m JSONAdapter()(lm, lm_kwargs, signature, demos, inputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/chat_adapter.py:42\u001B[39m, in \u001B[36mChatAdapter.__call__\u001B[39m\u001B[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msignature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdemos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     44\u001B[39m     \u001B[38;5;66;03m# fallback to JSONAdapter\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/base.py:119\u001B[39m, in \u001B[36mAdapter.__call__\u001B[39m\u001B[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001B[39m\n\u001B[32m    117\u001B[39m inputs = \u001B[38;5;28mself\u001B[39m.format(processed_signature, demos, inputs)\n\u001B[32m--> \u001B[39m\u001B[32m119\u001B[39m outputs = \u001B[43mlm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mlm_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    120\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_postprocess(signature, outputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001B[39m, in \u001B[36mwith_callbacks.<locals>.sync_wrapper\u001B[39m\u001B[34m(instance, *args, **kwargs)\u001B[39m\n\u001B[32m    325\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m callbacks:\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    328\u001B[39m call_id = uuid.uuid4().hex\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/base_lm.py:96\u001B[39m, in \u001B[36mBaseLM.__call__\u001B[39m\u001B[34m(self, prompt, messages, **kwargs)\u001B[39m\n\u001B[32m     94\u001B[39m \u001B[38;5;129m@with_callbacks\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, prompt=\u001B[38;5;28;01mNone\u001B[39;00m, messages=\u001B[38;5;28;01mNone\u001B[39;00m, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m96\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     97\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m._process_lm_response(response, prompt, messages, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:127\u001B[39m, in \u001B[36mLM.forward\u001B[39m\u001B[34m(self, prompt, messages, **kwargs)\u001B[39m\n\u001B[32m    125\u001B[39m completion, litellm_cache_args = \u001B[38;5;28mself\u001B[39m._get_cached_completion_fn(completion, cache, enable_memory_cache)\n\u001B[32m--> \u001B[39m\u001B[32m127\u001B[39m results = \u001B[43mcompletion\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_retries\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnum_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlitellm_cache_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(c.finish_reason == \u001B[33m\"\u001B[39m\u001B[33mlength\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m results[\u001B[33m\"\u001B[39m\u001B[33mchoices\u001B[39m\u001B[33m\"\u001B[39m]):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/cache.py:232\u001B[39m, in \u001B[36mrequest_cache.<locals>.decorator.<locals>.sync_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    231\u001B[39m \u001B[38;5;66;03m# Otherwise, compute and store the result\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m232\u001B[39m result = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    233\u001B[39m \u001B[38;5;66;03m# `enable_memory_cache` can be provided at call time to avoid indefinite growth.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:304\u001B[39m, in \u001B[36mlitellm_completion\u001B[39m\u001B[34m(request, num_retries, cache)\u001B[39m\n\u001B[32m    303\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stream_completion \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m304\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlitellm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompletion\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    305\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    306\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_retries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    307\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretry_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mexponential_backoff_retry\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    308\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    309\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    311\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m stream_completion()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1266\u001B[39m, in \u001B[36mclient.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1265\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mnum_retries\u001B[39m\u001B[33m\"\u001B[39m] = num_retries\n\u001B[32m-> \u001B[39m\u001B[32m1266\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlitellm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompletion_with_retries\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1267\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m (\n\u001B[32m   1268\u001B[39m     \u001B[38;5;28misinstance\u001B[39m(e, litellm.exceptions.ContextWindowExceededError)\n\u001B[32m   1269\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m context_window_fallback_dict\n\u001B[32m   1270\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m context_window_fallback_dict\n\u001B[32m   1271\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_litellm_router_call\n\u001B[32m   1272\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3343\u001B[39m, in \u001B[36mcompletion_with_retries\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   3340\u001B[39m     retryer = tenacity.Retrying(\n\u001B[32m   3341\u001B[39m         stop=tenacity.stop_after_attempt(num_retries), reraise=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   3342\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m3343\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mretryer\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:477\u001B[39m, in \u001B[36mRetrying.__call__\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    476\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m477\u001B[39m     do = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    478\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:378\u001B[39m, in \u001B[36mBaseRetrying.iter\u001B[39m\u001B[34m(self, retry_state)\u001B[39m\n\u001B[32m    377\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m action \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.iter_state.actions:\n\u001B[32m--> \u001B[39m\u001B[32m378\u001B[39m     result = \u001B[43maction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    379\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:420\u001B[39m, in \u001B[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001B[39m\u001B[34m(rs)\u001B[39m\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.reraise:\n\u001B[32m--> \u001B[39m\u001B[32m420\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mretry_exc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    421\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m retry_exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mfut\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexception\u001B[39;00m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:187\u001B[39m, in \u001B[36mRetryError.reraise\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.last_attempt.failed:\n\u001B[32m--> \u001B[39m\u001B[32m187\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlast_attempt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/concurrent/futures/_base.py:449\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    448\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m449\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    451\u001B[39m \u001B[38;5;28mself\u001B[39m._condition.wait(timeout)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.11/concurrent/futures/_base.py:401\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    400\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    402\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    403\u001B[39m     \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:480\u001B[39m, in \u001B[36mRetrying.__call__\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    479\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m480\u001B[39m     result = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    481\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1286\u001B[39m, in \u001B[36mclient.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1283\u001B[39m     logging_obj.failure_handler(\n\u001B[32m   1284\u001B[39m         e, traceback_exception, start_time, end_time\n\u001B[32m   1285\u001B[39m     )  \u001B[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1286\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1164\u001B[39m, in \u001B[36mclient.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1163\u001B[39m \u001B[38;5;66;03m# MODEL CALL\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1164\u001B[39m result = \u001B[43moriginal_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1165\u001B[39m end_time = datetime.datetime.now()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3305\u001B[39m, in \u001B[36mcompletion\u001B[39m\u001B[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001B[39m\n\u001B[32m   3303\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   3304\u001B[39m     \u001B[38;5;66;03m## Map to OpenAI Exception\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3305\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mexception_type\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3306\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3307\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3308\u001B[39m \u001B[43m        \u001B[49m\u001B[43moriginal_exception\u001B[49m\u001B[43m=\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3309\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcompletion_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3310\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3311\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2271\u001B[39m, in \u001B[36mexception_type\u001B[39m\u001B[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001B[39m\n\u001B[32m   2270\u001B[39m     \u001B[38;5;28msetattr\u001B[39m(e, \u001B[33m\"\u001B[39m\u001B[33mlitellm_response_headers\u001B[39m\u001B[33m\"\u001B[39m, litellm_response_headers)\n\u001B[32m-> \u001B[39m\u001B[32m2271\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m   2272\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2240\u001B[39m, in \u001B[36mexception_type\u001B[39m\u001B[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001B[39m\n\u001B[32m   2239\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(original_exception, \u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m2240\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(\n\u001B[32m   2241\u001B[39m         message=\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m - \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(exception_provider, error_str),\n\u001B[32m   2242\u001B[39m         llm_provider=custom_llm_provider,\n\u001B[32m   2243\u001B[39m         model=model,\n\u001B[32m   2244\u001B[39m         request=original_exception.request,\n\u001B[32m   2245\u001B[39m     )\n\u001B[32m   2246\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mAPIConnectionError\u001B[39m: litellm.APIConnectionError: Ollama_chatException - Object of type LM is not JSON serializable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m example = dataset[\u001B[33m'\u001B[39m\u001B[33mtest_r3\u001B[39m\u001B[33m'\u001B[39m][\u001B[32m0\u001B[39m]\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(example)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mzero_shot_nli_classifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 14\u001B[39m, in \u001B[36mzero_shot_nli_classifier\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mzero_shot_nli_classifier\u001B[39m(x):\n\u001B[32m     13\u001B[39m     \u001B[38;5;66;03m# if hasattr(x,'premise') and hasattr(x,'hypothesis'):\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpremise\u001B[49m\u001B[43m=\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mpremise\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhypothesis\u001B[49m\u001B[43m=\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhypothesis\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m.label\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:85\u001B[39m, in \u001B[36mPredict.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     82\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m args:\n\u001B[32m     83\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;28mself\u001B[39m._get_positional_args_error_message())\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001B[39m, in \u001B[36mwith_callbacks.<locals>.sync_wrapper\u001B[39m\u001B[34m(instance, *args, **kwargs)\u001B[39m\n\u001B[32m    324\u001B[39m callbacks = _get_active_callbacks(instance)\n\u001B[32m    325\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m callbacks:\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    328\u001B[39m call_id = uuid.uuid4().hex\n\u001B[32m    330\u001B[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/primitives/program.py:60\u001B[39m, in \u001B[36mModule.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     57\u001B[39m     output.set_lm_usage(usage_tracker.get_total_tokens())\n\u001B[32m     58\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:157\u001B[39m, in \u001B[36mPredict.forward\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m    155\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    156\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m settings.context(send_stream=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m157\u001B[39m         completions = \u001B[43madapter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msignature\u001B[49m\u001B[43m=\u001B[49m\u001B[43msignature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdemos\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdemos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_postprocess(completions, signature, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/chat_adapter.py:51\u001B[39m, in \u001B[36mChatAdapter.__call__\u001B[39m\u001B[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, ContextWindowExceededError) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, JSONAdapter):\n\u001B[32m     48\u001B[39m     \u001B[38;5;66;03m# On context window exceeded error or already using JSONAdapter, we don't want to retry with a different\u001B[39;00m\n\u001B[32m     49\u001B[39m     \u001B[38;5;66;03m# adapter.\u001B[39;00m\n\u001B[32m     50\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mJSONAdapter\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msignature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdemos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/json_adapter.py:75\u001B[39m, in \u001B[36mJSONAdapter.__call__\u001B[39m\u001B[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001B[39m\n\u001B[32m     72\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m     73\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     74\u001B[39m     \u001B[38;5;66;03m# On any other error, we raise a RuntimeError with the original error message.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m75\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m     76\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mBoth structured output format and JSON mode failed. Please choose a model that supports \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     77\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m`response_format` argument. Original error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     78\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.APIConnectionError: Ollama_chatException - Object of type LM is not JSON serializable"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0e2e9027",
   "metadata": {},
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ab24e1b",
   "metadata": {},
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d04f0c1",
   "metadata": {},
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
