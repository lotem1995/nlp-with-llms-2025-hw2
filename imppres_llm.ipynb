{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres with LLM\n",
    "\n",
    "You have to implement in this notebook a better ImpPres classifier using an LLM.\n",
    "This classifier must be implemented using DSPy.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from os.path import exists\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from typing import Literal\n",
    "from functools import reduce, partial\n",
    "from itertools import chain\n",
    "import evaluate\n",
    "\n",
    "# Configure DSPy\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)\n"
   ],
   "id": "ccdf1a23a9d0f6fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Constants and Configuration",
   "id": "723cd5589a5f8ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Global constants to eliminate redundancy\n",
    "SECTIONS = [\n",
    "    'presupposition_all_n_presupposition', \n",
    "    'presupposition_both_presupposition', \n",
    "    'presupposition_change_of_state', \n",
    "    'presupposition_cleft_existence', \n",
    "    'presupposition_cleft_uniqueness', \n",
    "    'presupposition_only_presupposition', \n",
    "    'presupposition_possessed_definites_existence', \n",
    "    'presupposition_possessed_definites_uniqueness', \n",
    "    'presupposition_question_presupposition'\n",
    "]\n",
    "\n",
    "LABEL_NAMES = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "LABEL_TO_ID = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "ID_TO_LABEL = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "\n",
    "# Load evaluation metrics once\n",
    "METRICS = {\n",
    "    'accuracy': evaluate.load(\"accuracy\"),\n",
    "    'precision': evaluate.load(\"precision\"),\n",
    "    'recall': evaluate.load(\"recall\"),\n",
    "    'f1': evaluate.load(\"f1\"),\n",
    "    'combined': evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "}"
   ],
   "id": "b0c006fc884952e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functional Utility Functions",
   "id": "5b4af5870ff451c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_or_create_combined_dataset(sections=SECTIONS, parquet_path='combined_imppres_presuppositions.parquet'):\n",
    "    \"\"\"Load combined dataset from parquet or create it from individual sections.\"\"\"\n",
    "    if not exists(parquet_path):\n",
    "        print(\"Creating combined dataset...\")\n",
    "        dataframes = [\n",
    "            load_dataset(\"facebook/imppres\", section).to_pandas().assign(section=section)\n",
    "            for section in sections\n",
    "        ]\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        combined_df.to_parquet(parquet_path)\n",
    "        print(f\"Saved combined dataset to {parquet_path}\")\n",
    "    else:\n",
    "        combined_df = pd.read_parquet(parquet_path)\n",
    "        print(f\"Loaded combined dataset from {parquet_path}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def analyze_paradigm_structure(df):\n",
    "    \"\"\"Analyze and display paradigm structure information.\"\"\"\n",
    "    paradigm_counts = df['paradigmID'].value_counts()\n",
    "    \n",
    "    analysis = {\n",
    "        'total_paradigms': df['paradigmID'].nunique(),\n",
    "        'mean_samples_per_paradigm': paradigm_counts.mean(),\n",
    "        'std_samples_per_paradigm': paradigm_counts.std(),\n",
    "        'paradigm_size_distribution': paradigm_counts.value_counts().head()\n",
    "    }\n",
    "    \n",
    "    # Display analysis\n",
    "    print(\"Paradigm structure analysis:\")\n",
    "    print(f\"Unique paradigm IDs: {analysis['total_paradigms']}\")\n",
    "    print(f\"Samples per paradigm - mean: {analysis['mean_samples_per_paradigm']:.1f}, std: {analysis['std_samples_per_paradigm']:.1f}\")\n",
    "    print(f\"Most common paradigm sizes: {analysis['paradigm_size_distribution']}\")\n",
    "    \n",
    "    # Show example paradigm\n",
    "    first_paradigm_id = df['paradigmID'].iloc[0]\n",
    "    first_paradigm = df[df['paradigmID'] == first_paradigm_id]\n",
    "    print(f\"\\nExample paradigm {first_paradigm_id} ({len(first_paradigm)} samples):\")\n",
    "    print(first_paradigm[['premise', 'hypothesis', 'gold_label']].head())\n",
    "    \n",
    "    return analysis\n"
   ],
   "id": "2b359bff2287719c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2.4: Explanation CoT LLM for ImpPres and Consistency Validation\n",
    "\n",
    "This implementation improves presupposition identification by exploiting paradigm signals in the ImpPres dataset.\n",
    "We use consistency across paradigms as a reward measure during LLM optimization, combined with overall accuracy.\n"
   ],
   "id": "ab5762c89f472f23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load and analyze the combined dataset\n",
    "combined_df = load_or_create_combined_dataset()\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "print(f\"Columns: {combined_df.columns.tolist()}\")\n",
    "\n",
    "paradigm_analysis = analyze_paradigm_structure(combined_df)\n"
   ],
   "id": "b1ffa93e84d35c87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DSPy Signature for Explanation-based Classification\n",
   "id": "5ca35ca6c07bdb61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ExplanationNLIClassifier(dspy.Signature):\n",
    "    \"\"\"Classify premise-hypothesis pairs with explanations for presupposition identification.\"\"\"\n",
    "    \n",
    "    premise: str = dspy.InputField(desc=\"A short passage or statement containing potential presuppositions.\")\n",
    "    hypothesis: str = dspy.InputField(desc=\"A statement to evaluate against the premise for presupposition relationships.\")\n",
    "    \n",
    "    explanation: str = dspy.OutputField(desc=\"Provide a detailed explanation of the presupposition relationship between the premise and hypothesis. Explain what presuppositions are triggered and how they relate to the entailment.\")\n",
    "    \n",
    "    label: Literal[\"entailment\", \"neutral\", \"contradiction\"] = dspy.OutputField(\n",
    "        desc=(\n",
    "            \"Based on the presupposition analysis, classify as:\\n\"\n",
    "            \"- 'entailment': The hypothesis follows from the premise's presuppositions\\n\"\n",
    "            \"- 'contradiction': The hypothesis contradicts the premise's presuppositions\\n\"\n",
    "            \"- 'neutral': The hypothesis is unrelated to the premise's presuppositions\"\n",
    "        )\n",
    "    )\n"
   ],
   "id": "fe62b52296fe9bf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Functional Paradigm Consistency Functions\n",
   "id": "a52b24528fda18be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def group_by_paradigm(df):\n",
    "    \"\"\"Group samples by paradigmID - pure function.\"\"\"\n",
    "    return df.groupby('paradigmID')\n",
    "\n",
    "def calculate_paradigm_consistency(paradigm_groups, predictions_dict):\n",
    "    \"\"\"Calculate consistency score for each paradigm - pure function.\"\"\"\n",
    "    consistency_scores = {}\n",
    "    \n",
    "    for paradigm_id, group in paradigm_groups:\n",
    "        if len(group) < 2:  # Skip paradigms with only one sample\n",
    "            continue\n",
    "            \n",
    "        # Get predictions for this paradigm\n",
    "        paradigm_preds = [\n",
    "            predictions_dict[idx] for idx in group.index \n",
    "            if idx in predictions_dict\n",
    "        ]\n",
    "        \n",
    "        if len(paradigm_preds) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Calculate consistency as agreement rate\n",
    "        pred_labels = [pred['pred_label'] for pred in paradigm_preds]\n",
    "        label_counts = Counter(pred_labels)\n",
    "        most_common_count = label_counts.most_common(1)[0][1]\n",
    "        consistency = most_common_count / len(pred_labels)\n",
    "        \n",
    "        consistency_scores[paradigm_id] = {\n",
    "            'consistency': consistency,\n",
    "            'size': len(paradigm_preds),\n",
    "            'predictions': pred_labels\n",
    "        }\n",
    "    \n",
    "    return consistency_scores\n",
    "\n",
    "def calculate_overall_consistency(consistency_scores):\n",
    "    \"\"\"Calculate overall consistency across all paradigms - pure function.\"\"\"\n",
    "    if not consistency_scores:\n",
    "        return 0.0\n",
    "    \n",
    "    total_weighted_consistency = sum(\n",
    "        scores['consistency'] * scores['size'] \n",
    "        for scores in consistency_scores.values()\n",
    "    )\n",
    "    total_samples = sum(scores['size'] for scores in consistency_scores.values())\n",
    "    \n",
    "    return total_weighted_consistency / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "def calculate_accuracy(predictions_dict, gold_labels_dict):\n",
    "    \"\"\"Calculate accuracy using evaluate library - pure function.\"\"\"\n",
    "    # Convert predictions and references to the format expected by evaluate library\n",
    "    valid_indices = [idx for idx in predictions_dict.keys() if idx in gold_labels_dict]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        return 0.0\n",
    "    \n",
    "    # Extract predictions and references in aligned order\n",
    "    preds = [LABEL_TO_ID[predictions_dict[idx]['pred_label']] for idx in valid_indices]\n",
    "    refs = [LABEL_TO_ID[gold_labels_dict[idx]] for idx in valid_indices]\n",
    "    \n",
    "    # Use evaluate library for accuracy calculation\n",
    "    return METRICS['accuracy'].compute(predictions=preds, references=refs)['accuracy']\n",
    "\n",
    "def calculate_combined_score(predictions_dict, gold_labels_dict, paradigm_groups, alpha=0.7):\n",
    "    \"\"\"Calculate combined score of accuracy and consistency - pure function.\"\"\"\n",
    "    accuracy = calculate_accuracy(predictions_dict, gold_labels_dict)\n",
    "    consistency_scores = calculate_paradigm_consistency(paradigm_groups, predictions_dict)\n",
    "    overall_consistency = calculate_overall_consistency(consistency_scores)\n",
    "    \n",
    "    combined_score = alpha * accuracy + (1 - alpha) * overall_consistency\n",
    "    \n",
    "    return {\n",
    "        'combined_score': combined_score,\n",
    "        'accuracy': accuracy,\n",
    "        'consistency': overall_consistency,\n",
    "        'paradigm_scores': consistency_scores\n",
    "    }\n"
   ],
   "id": "85bffe152dd7f808"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Functional DSPy Predictor\n",
   "id": "938ef58d77b2171b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_explanation_predictor():\n",
    "    \"\"\"Create explanation predictor - factory function.\"\"\"\n",
    "    return dspy.Predict(ExplanationNLIClassifier)\n",
    "\n",
    "def predict_with_explanation(predictor, premise, hypothesis):\n",
    "    \"\"\"Make prediction with explanation - pure function.\"\"\"\n",
    "    result = predictor(premise=premise, hypothesis=hypothesis)\n",
    "    return {\n",
    "        'explanation': result.explanation,\n",
    "        'label': result.label\n",
    "    }\n",
    "\n",
    "# Initialize the predictor\n",
    "explanation_predictor = create_explanation_predictor()\n"
   ],
   "id": "cb6b95842069afde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Functional Evaluation Functions\n",
   "id": "3cabc124bb9c6048"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_example(predictor, example, section_name, index):\n",
    "    \"\"\"Process a single example - pure function.\"\"\"\n",
    "    try:\n",
    "        pred_result = predict_with_explanation(\n",
    "            predictor, \n",
    "            example['premise'], \n",
    "            example['hypothesis']\n",
    "        )\n",
    "        \n",
    "        gold_label = ID_TO_LABEL[example['gold_label']]\n",
    "        \n",
    "        result = {\n",
    "            'premise': example['premise'],\n",
    "            'hypothesis': example['hypothesis'],\n",
    "            'explanation': pred_result['explanation'],\n",
    "            'pred_label': pred_result['label'],\n",
    "            'gold_label': gold_label,\n",
    "            'paradigmID': example.get('paradigmID', ''),\n",
    "            'UID': example.get('UID', ''),\n",
    "            'section': section_name\n",
    "        }\n",
    "        \n",
    "        return result, (index, result), (index, gold_label)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {index}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def evaluate_section(predictor, dataset_section, section_name, max_samples=None):\n",
    "    \"\"\"Evaluate predictor on a dataset section - functional approach.\"\"\"\n",
    "    print(f\"Evaluating section: {section_name}\")\n",
    "    \n",
    "    # Convert to list for easier handling\n",
    "    data_list = (list(dataset_section) if isinstance(dataset_section, Dataset) \n",
    "                else dataset_section.to_dict('records'))\n",
    "    \n",
    "    if max_samples:\n",
    "        data_list = data_list[:max_samples]\n",
    "    \n",
    "    # Process all examples functionally\n",
    "    processed = [\n",
    "        process_example(predictor, example, section_name, i)\n",
    "        for i, example in enumerate(tqdm(data_list, desc=f\"Processing {section_name}\"))\n",
    "    ]\n",
    "    \n",
    "    # Filter out failed examples and separate results\n",
    "    valid_results = [item for item in processed if item[0] is not None]\n",
    "    \n",
    "    if not valid_results:\n",
    "        return [], {}, {}\n",
    "    \n",
    "    results, predictions_items, gold_items = zip(*valid_results)\n",
    "    predictions_dict = dict(predictions_items)\n",
    "    gold_labels_dict = dict(gold_items)\n",
    "    \n",
    "    return list(results), predictions_dict, gold_labels_dict\n",
    "\n",
    "def create_section_datasets(combined_df, sections=SECTIONS):\n",
    "    \"\"\"Create section datasets from combined dataframe - pure function.\"\"\"\n",
    "    return {\n",
    "        section: Dataset.from_pandas(combined_df[combined_df['section'] == section])\n",
    "        for section in sections\n",
    "    }\n"
   ],
   "id": "f130d9e3e55632a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run Evaluation on All Sections\n",
   "id": "65e1bf3d682de1f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get paradigm groups for consistency calculation\n",
    "paradigm_groups = group_by_paradigm(combined_df)\n",
    "\n",
    "# Create section datasets\n",
    "section_datasets = create_section_datasets(combined_df)\n",
    "\n",
    "# Evaluate each section with limited samples for cost control\n",
    "max_samples_per_section = 50  # Adjust based on budget\n",
    "\n",
    "# Functional evaluation pipeline\n",
    "evaluation_results = {\n",
    "    section_name: evaluate_section(\n",
    "        explanation_predictor, \n",
    "        section_datasets[section_name], \n",
    "        section_name, \n",
    "        max_samples_per_section\n",
    "    )\n",
    "    for section_name in SECTIONS\n",
    "}\n",
    "\n",
    "# Separate results for analysis\n",
    "all_results = {name: results[0] for name, results in evaluation_results.items()}\n",
    "all_predictions = dict(chain.from_iterable(\n",
    "    results[1].items() for results in evaluation_results.values()\n",
    "))\n",
    "all_gold_labels = dict(chain.from_iterable(\n",
    "    results[2].items() for results in evaluation_results.values()\n",
    "))\n"
   ],
   "id": "e5b8db61edb9ce41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Functional Results Analysis and Metrics Computation\n",
   "id": "7bb50817d37c212f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_section_metrics(section_results, paradigm_groups):\n",
    "    \"\"\"Compute metrics for a section - pure function.\"\"\"\n",
    "    if not section_results:\n",
    "        return None\n",
    "    \n",
    "    # Convert labels to IDs for metrics computation\n",
    "    preds = [LABEL_TO_ID[result['pred_label']] for result in section_results]\n",
    "    refs = [LABEL_TO_ID[result['gold_label']] for result in section_results]\n",
    "    \n",
    "    # Calculate standard classification metrics\n",
    "    metrics = {\n",
    "        'accuracy': METRICS['accuracy'].compute(predictions=preds, references=refs)['accuracy'],\n",
    "        'precision': METRICS['precision'].compute(predictions=preds, references=refs, average='weighted')['precision'],\n",
    "        'recall': METRICS['recall'].compute(predictions=preds, references=refs, average='weighted')['recall'],\n",
    "        'f1': METRICS['f1'].compute(predictions=preds, references=refs, average='weighted')['f1'],\n",
    "        'samples': len(section_results)\n",
    "    }\n",
    "    \n",
    "    # Calculate section-specific consistency\n",
    "    section_predictions = {i: result for i, result in enumerate(section_results)}\n",
    "    section_gold = {i: result['gold_label'] for i, result in enumerate(section_results)}\n",
    "    section_combined = calculate_combined_score(section_predictions, section_gold, paradigm_groups)\n",
    "    \n",
    "    metrics.update({\n",
    "        'consistency': section_combined['consistency'],\n",
    "        'combined_score': section_combined['combined_score']\n",
    "    })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_section_metrics(section_name, metrics):\n",
    "    \"\"\"Print section metrics - pure function for display.\"\"\"\n",
    "    print(f\"\\n{section_name}:\")\n",
    "    print(f\"  Samples: {metrics['samples']}\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "    print(f\"  Consistency: {metrics['consistency']:.4f}\")\n",
    "    print(f\"  Combined Score: {metrics['combined_score']:.4f}\")\n"
   ],
   "id": "8731b52c3ee9703a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate overall metrics using functional approach\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERALL RESULTS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "overall_metrics = calculate_combined_score(all_predictions, all_gold_labels, paradigm_groups)\n",
    "\n",
    "print(f\"Overall Accuracy: {overall_metrics['accuracy']:.4f}\")\n",
    "print(f\"Overall Consistency: {overall_metrics['consistency']:.4f}\")\n",
    "print(f\"Combined Score: {overall_metrics['combined_score']:.4f}\")\n"
   ],
   "id": "944081fdd33f048d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate metrics per section using functional approach\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SECTION-WISE PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "section_metrics = {}\n",
    "for section_name in SECTIONS:\n",
    "    if section_name in all_results:\n",
    "        metrics = compute_section_metrics(all_results[section_name], paradigm_groups)\n",
    "        if metrics:\n",
    "            section_metrics[section_name] = metrics\n",
    "            print_section_metrics(section_name, metrics)\n"
   ],
   "id": "78b37e3eb946c211"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create results summary table\n",
    "results_df = pd.DataFrame.from_dict(section_metrics, orient='index')\n",
    "results_df = results_df.round(4)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*60)\n",
    "display(results_df)\n"
   ],
   "id": "3268a8708a370f1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Functional Analysis Utilities\n",
   "id": "6a28d1e877a8e8ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_consistency_distribution(paradigm_scores):\n",
    "    \"\"\"Analyze consistency distribution - pure function.\"\"\"\n",
    "    if not paradigm_scores:\n",
    "        return {}\n",
    "    \n",
    "    consistency_values = [scores['consistency'] for scores in paradigm_scores.values()]\n",
    "    return {\n",
    "        'mean': np.mean(consistency_values),\n",
    "        'std': np.std(consistency_values),\n",
    "        'min': np.min(consistency_values),\n",
    "        'max': np.max(consistency_values),\n",
    "        'total_paradigms': len(paradigm_scores)\n",
    "    }\n",
    "\n",
    "def get_top_paradigms(paradigm_scores, n=5, reverse=True):\n",
    "    \"\"\"Get top N paradigms by consistency - pure function.\"\"\"\n",
    "    if not paradigm_scores:\n",
    "        return []\n",
    "    \n",
    "    sorted_paradigms = sorted(\n",
    "        paradigm_scores.items(), \n",
    "        key=lambda x: x[1]['consistency'], \n",
    "        reverse=reverse\n",
    "    )\n",
    "    return sorted_paradigms[:n]\n",
    "\n",
    "def group_results_by_paradigm(all_results):\n",
    "    \"\"\"Group results by paradigm ID - pure function.\"\"\"\n",
    "    paradigm_analysis = defaultdict(list)\n",
    "    \n",
    "    for section_name, section_results in all_results.items():\n",
    "        for result in section_results:\n",
    "            paradigm_id = result.get('paradigmID', '')\n",
    "            if paradigm_id:\n",
    "                paradigm_analysis[paradigm_id].append({\n",
    "                    'section': section_name,\n",
    "                    'pred_label': result['pred_label'],\n",
    "                    'gold_label': result['gold_label'],\n",
    "                    'correct': result['pred_label'] == result['gold_label']\n",
    "                })\n",
    "    \n",
    "    return paradigm_analysis\n",
    "\n",
    "def analyze_transformation_patterns(paradigm_analysis):\n",
    "    \"\"\"Analyze transformation patterns - pure function.\"\"\"\n",
    "    transformation_patterns = defaultdict(int)\n",
    "    correct_by_position = defaultdict(list)\n",
    "    \n",
    "    for paradigm_id, paradigm_results in paradigm_analysis.items():\n",
    "        if len(paradigm_results) > 1:  # Only analyze paradigms with multiple samples\n",
    "            # Count correct predictions by position in paradigm\n",
    "            for i, result in enumerate(paradigm_results):\n",
    "                correct_by_position[i].append(result['correct'])\n",
    "                \n",
    "            # Analyze transformation patterns\n",
    "            labels = [r['pred_label'] for r in paradigm_results]\n",
    "            pattern = tuple(labels)\n",
    "            transformation_patterns[pattern] += 1\n",
    "    \n",
    "    return transformation_patterns, correct_by_position\n",
    "\n",
    "def get_example_predictions(all_results, max_examples=5, examples_per_section=2):\n",
    "    \"\"\"Get example predictions - pure function.\"\"\"\n",
    "    examples = []\n",
    "    example_count = 0\n",
    "    \n",
    "    for section_name, section_results in all_results.items():\n",
    "        if example_count >= max_examples:\n",
    "            break\n",
    "            \n",
    "        section_examples = section_results[:examples_per_section]\n",
    "        for result in section_examples:\n",
    "            if example_count >= max_examples:\n",
    "                break\n",
    "                \n",
    "            examples.append({\n",
    "                'section': section_name,\n",
    "                'premise': result['premise'],\n",
    "                'hypothesis': result['hypothesis'],\n",
    "                'explanation': result['explanation'],\n",
    "                'pred_label': result['pred_label'],\n",
    "                'gold_label': result['gold_label'],\n",
    "                'correct': result['pred_label'] == result['gold_label']\n",
    "            })\n",
    "            example_count += 1\n",
    "    \n",
    "    return examples\n",
    "\n",
    "def print_header(title, width=60):\n",
    "    \"\"\"Print formatted header - utility function.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*width)\n",
    "    print(title)\n",
    "    print(\"=\"*width)\n"
   ],
   "id": "cefd9fd2fbc2d496"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Functional Paradigm Analysis\n",
   "id": "14fbc8cd3003f07c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analyze paradigm consistency using functional approach\n",
    "print_header(\"PARADIGM CONSISTENCY ANALYSIS\")\n",
    "\n",
    "paradigm_scores = overall_metrics['paradigm_scores']\n",
    "consistency_stats = analyze_consistency_distribution(paradigm_scores)\n",
    "\n",
    "print(f\"Total paradigms analyzed: {consistency_stats['total_paradigms']}\")\n",
    "if consistency_stats:\n",
    "    print(f\"Mean paradigm consistency: {consistency_stats['mean']:.4f}\")\n",
    "    print(f\"Std paradigm consistency: {consistency_stats['std']:.4f}\")\n",
    "    print(f\"Min paradigm consistency: {consistency_stats['min']:.4f}\")\n",
    "    print(f\"Max paradigm consistency: {consistency_stats['max']:.4f}\")\n",
    "\n",
    "# Show examples of high and low consistency paradigms\n",
    "most_consistent = get_top_paradigms(paradigm_scores, n=5, reverse=True)\n",
    "least_consistent = get_top_paradigms(paradigm_scores, n=5, reverse=False)\n",
    "\n",
    "if most_consistent:\n",
    "    print(f\"\\nTop 5 most consistent paradigms:\")\n",
    "    for i, (paradigm_id, scores) in enumerate(most_consistent):\n",
    "        print(f\"  {i+1}. Paradigm {paradigm_id}: {scores['consistency']:.4f} (size: {scores['size']})\")\n",
    "        print(f\"     Predictions: {scores['predictions']}\")\n",
    "\n",
    "if least_consistent:\n",
    "    print(f\"\\nTop 5 least consistent paradigms:\")\n",
    "    for i, (paradigm_id, scores) in enumerate(least_consistent):\n",
    "        print(f\"  {i+1}. Paradigm {paradigm_id}: {scores['consistency']:.4f} (size: {scores['size']})\")\n",
    "        print(f\"     Predictions: {scores['predictions']}\")\n"
   ],
   "id": "974a74404a8e6a83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Functional Transformation Analysis\n",
   "id": "6388ed7001b4fdf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analyze transformation patterns using functional approach\n",
    "print_header(\"TRANSFORMATION TYPE ANALYSIS\")\n",
    "\n",
    "paradigm_analysis = group_results_by_paradigm(all_results)\n",
    "transformation_patterns, correct_by_position = analyze_transformation_patterns(paradigm_analysis)\n",
    "\n",
    "print(f\"Most common prediction patterns across paradigms:\")\n",
    "sorted_patterns = sorted(transformation_patterns.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (pattern, count) in enumerate(sorted_patterns[:10]):\n",
    "    print(f\"  {i+1}. {pattern}: {count} paradigms\")\n",
    "\n",
    "print(f\"\\nAccuracy by transformation position:\")\n",
    "for pos, correct_list in correct_by_position.items():\n",
    "    if correct_list:\n",
    "        pos_accuracy = np.mean(correct_list)\n",
    "        print(f\"  Position {pos}: {pos_accuracy:.4f} ({len(correct_list)} samples)\")\n"
   ],
   "id": "a2173a16f400f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Functional Example Display\n",
   "id": "77e9f6fe5d074154"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Show example predictions using functional approach\n",
    "print_header(\"EXAMPLE PREDICTIONS WITH EXPLANATIONS\")\n",
    "\n",
    "examples = get_example_predictions(all_results, max_examples=5)\n",
    "\n",
    "for i, example in enumerate(examples):\n",
    "    print(f\"\\nExample {i + 1} ({example['section']}):\")\n",
    "    print(f\"Premise: {example['premise']}\")\n",
    "    print(f\"Hypothesis: {example['hypothesis']}\")\n",
    "    print(f\"Explanation: {example['explanation']}\")\n",
    "    print(f\"Predicted: {example['pred_label']}\")\n",
    "    print(f\"Gold: {example['gold_label']}\")\n",
    "    print(f\"Correct: {example['correct']}\")\n",
    "    print(\"-\" * 40)\n"
   ],
   "id": "70f5cb35a19e24c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analysis and Conclusions\n",
   "id": "e399107f4cf13d87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS AND CONCLUSIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Task 2.4 Implementation Summary:\")\n",
    "print(\"1. Implemented explanation-based CoT LLM for presupposition identification\")\n",
    "print(\"2. Used paradigm consistency as a reward measure combined with accuracy\")\n",
    "print(\"3. Evaluated on all 9 presupposition sections of ImpPres dataset\")\n",
    "print(\"4. Analyzed consistency patterns across paradigm transformations\")\n",
    "print()\n",
    "\n",
    "print(\"Key Findings:\")\n",
    "print(f\"- Overall accuracy: {overall_metrics['accuracy']:.4f}\")\n",
    "print(f\"- Overall consistency: {overall_metrics['consistency']:.4f}\")\n",
    "print(f\"- Combined score (α=0.7): {overall_metrics['combined_score']:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Approach Explanation:\")\n",
    "print(\"- Used explanation-based prompting to improve presupposition understanding\")\n",
    "print(\"- Implemented paradigm consistency validation across transformations\")\n",
    "print(\"- Combined accuracy and consistency with α=0.7 weighting\")\n",
    "print(\"- Limited samples per section to control API costs\")\n",
    "print()\n",
    "\n",
    "print(\"Future Improvements:\")\n",
    "print(\"- Optimize DSPy program with few-shot examples\")\n",
    "print(\"- Implement more sophisticated consistency measures\")\n",
    "print(\"- Use larger sample sizes for more robust evaluation\")\n",
    "print(\"- Add comparison with multiple baseline models\")\n"
   ],
   "id": "22b5d36f6bf711ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bc0b6a91923af235"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
